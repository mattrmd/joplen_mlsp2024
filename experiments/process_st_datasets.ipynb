{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import yaml\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "DATA_PATH = Path(\"..\") / \"datasets\"\n",
    "\n",
    "RIBO_PATH = DATA_PATH / \"riboflavin\"\n",
    "PAC_PATH = DATA_PATH / \"pac_dimerization\"\n",
    "VIRUS_PATH = DATA_PATH / \"virus\"\n",
    "CRIME_PATH = DATA_PATH / \"crime\"\n",
    "\n",
    "round_digits = 3\n",
    "\n",
    "r = lambda x: round(x, round_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'shuffle': True, 'train_size': 0.9, 'test_size': 0.1}\n",
      "{'shuffle': True, 'train_size': 0.778, 'test_size': 0.222}\n"
     ]
    }
   ],
   "source": [
    "train_frac, val_frac, test_frac = 0.7, 0.2, 0.1\n",
    "\n",
    "sum_frac = round(train_frac + val_frac + test_frac, 10)\n",
    "assert sum_frac == 1.0, sum_frac\n",
    "\n",
    "random_state = np.random.RandomState(0)\n",
    "\n",
    "# Note that this random state is used for all splits, so it changes over time\n",
    "shared_params = {\n",
    "    \"shuffle\": True,\n",
    "}\n",
    "\n",
    "train_test_params = {\n",
    "    **shared_params,\n",
    "    \"train_size\": r(train_frac + val_frac),\n",
    "    \"test_size\": r(test_frac),\n",
    "}\n",
    "\n",
    "train_val_test_params = {\n",
    "    **shared_params,\n",
    "    \"train_size\": r(train_frac / (1 - test_frac)),\n",
    "    \"test_size\": r(val_frac / (1 - test_frac)),\n",
    "}\n",
    "\n",
    "print(train_test_params)\n",
    "print(train_val_test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(url, save_path, delimiter=\",\"):\n",
    "    # Convert save_path to a Path object\n",
    "    save_path = Path(save_path) / Path(url).name\n",
    "\n",
    "    # Define the custom User-Agent header\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36\",\n",
    "        \"Referrer\": url,\n",
    "    }\n",
    "\n",
    "    ext = save_path.suffix[1:]\n",
    "    compression = ext if ext in [\"zip\", \"bz2\"] else None\n",
    "\n",
    "    # Check if the file already exists at the save_path\n",
    "    if save_path.exists():\n",
    "        # Load the dataframe from the cache file\n",
    "        df = pd.read_csv(save_path, compression=compression, delimiter=delimiter)\n",
    "    else:\n",
    "        # Download the file as it doesn't exist in the cache\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            # Write the content to a file\n",
    "            save_path.write_bytes(response.content)\n",
    "            # Now, load the file into a pandas dataframe\n",
    "            df = pd.read_csv(save_path, compression=compression, delimiter=delimiter)\n",
    "        else:\n",
    "            raise Exception(\n",
    "                f\"Failed to retrieve the data. Status code: {response.status_code}\"\n",
    "            )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: These should be combined with process_pmlb.ipynb\n",
    "\n",
    "\n",
    "def get_state(random_state: np.random.RandomState) -> list:\n",
    "    state = random_state.get_state()\n",
    "\n",
    "    # cast the state to something that is yaml serializable\n",
    "    return [\n",
    "        str(state[0]),\n",
    "        list(map(int, state[1])),\n",
    "        int(state[2]),\n",
    "        int(state[3]),\n",
    "        float(state[4]),\n",
    "    ]\n",
    "\n",
    "\n",
    "def preprocess(\n",
    "    x: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    path: Path,\n",
    "    stratify: bool,\n",
    ") -> None:\n",
    "    tts_random_state = get_state(random_state)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x,\n",
    "        y,\n",
    "        **train_test_params,\n",
    "        stratify=y if stratify else None,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    tvs_random_state = get_state(random_state)\n",
    "    x_train, x_val, y_train, y_val = train_test_split(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        **train_val_test_params,\n",
    "        stratify=y_train if stratify else None,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    np.savetxt(path / \"x_train.csv\", x_train, delimiter=\",\")\n",
    "    np.savetxt(path / \"y_train.csv\", y_train, delimiter=\",\")\n",
    "    np.savetxt(path / \"x_val.csv\", x_val, delimiter=\",\")\n",
    "    np.savetxt(path / \"y_val.csv\", y_val, delimiter=\",\")\n",
    "    np.savetxt(path / \"x_test.csv\", x_test, delimiter=\",\")\n",
    "    np.savetxt(path / \"y_test.csv\", y_test, delimiter=\",\")\n",
    "\n",
    "    with open(path / \"metadata.yaml\", \"w\") as f:\n",
    "        yaml.dump(\n",
    "            {\n",
    "                \"train_split\": train_test_params,\n",
    "                \"val_split\": train_val_test_params,\n",
    "                \"scaler\": None,\n",
    "                \"stratify\": stratify,\n",
    "                \"n_train\": len(set(y_train)),\n",
    "                \"n_val\": len(set(y_val)),\n",
    "                \"n_test\": len(set(y_test)),\n",
    "                \"n_total\": x.shape[0],\n",
    "                \"n_features\": x.shape[1],\n",
    "                \"tts_random_state\": tts_random_state,\n",
    "                \"tvs_random_state\": tvs_random_state,\n",
    "            },\n",
    "            f,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure why riboflavin cannot be loaded using urllib\n",
    "ribo_url = \"https://www.annualreviews.org/doi/suppl/10.1146/annurev-statistics-022513-115545/suppl_file/riboflavin.csv\"\n",
    "\n",
    "# df = load_data(\n",
    "#     ribo_url,\n",
    "#     RIBO_PATH,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "riboflavin = pd.read_csv(\n",
    "    RIBO_PATH / \"raw\" / \"riboflavin.csv\", delimiter=\",\", index_col=0, header=0\n",
    ")\n",
    "riboflavin = riboflavin.T\n",
    "\n",
    "y = riboflavin.pop(\"q_RIBFLV\").to_numpy()\n",
    "x = riboflavin.to_numpy()\n",
    "\n",
    "preprocess(x, y, RIBO_PATH / \"processed\", stratify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = load_diabetes(return_X_y=True)\n",
    "preprocess(x, y, DATA_PATH / \"diabetes\" / \"processed\", stratify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This dataset is advised against by the SKLearn team. However, many\n",
    "# papers use it, so we run it anyway in case the reviewers specifically request\n",
    "# it. Otherwise, we will not include it in the paper.\n",
    "\n",
    "# SOURCE: SKLearn\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=r\"\\s+\", skiprows=22, header=None, engine=\"python\")\n",
    "x = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "y = raw_df.values[1::2, 2]\n",
    "preprocess(x, y, DATA_PATH / \"boston\" / \"processed\", stratify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pac = pd.read_csv(\n",
    "    PAC_PATH / \"raw\" / \"harmonic_average.csv\", delimiter=\",\", index_col=0, header=0\n",
    ")\n",
    "\n",
    "pac = pac.drop(columns=[\"cpsa_dpsa_1\"])\n",
    "y = pac.pop(\"FE1000\").to_numpy()\n",
    "x = pac.to_numpy()\n",
    "\n",
    "preprocess(x, y, PAC_PATH / \"processed\", stratify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "virus_files = list((VIRUS_PATH / \"raw\").glob(\"*.txt\"))\n",
    "virus_files.sort()\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "# Function to parse a line\n",
    "def parse_line(line):\n",
    "    elements = line.split()\n",
    "    label = float(elements[0])\n",
    "    indices_values = [tuple(map(int, elem.split(\":\"))) for elem in elements[1:]]\n",
    "    return label, indices_values\n",
    "\n",
    "\n",
    "def parse_matrix(lines):\n",
    "    labels = []\n",
    "    rows = []\n",
    "    cols = []\n",
    "    data = []\n",
    "\n",
    "    for line in lines:\n",
    "        label, indices_values = parse_line(line)\n",
    "        labels.append(label)\n",
    "        for col, value in indices_values:\n",
    "            rows.append(len(labels) - 1)  # Row index\n",
    "            cols.append(col)  # Column index\n",
    "            data.append(value)\n",
    "\n",
    "    # Create a sparse matrix\n",
    "    sparse_matrix = csr_matrix((data, (rows, cols)), dtype=int)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame.sparse.from_spmatrix(sparse_matrix)\n",
    "    df[\"label\"] = labels\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = parse_matrix(virus_files[0].read_text().splitlines())\n",
    "\n",
    "# should try to load the rest of the data later\n",
    "\n",
    "y = df.pop(\"label\").to_numpy()\n",
    "x = df.to_numpy()\n",
    "\n",
    "preprocess(x, y, VIRUS_PATH / \"processed\", stratify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime = pd.read_csv(\n",
    "    CRIME_PATH / \"raw\" / \"communities.data\",\n",
    "    delimiter=\",\",\n",
    "    na_values=\"?\",\n",
    ")\n",
    "\n",
    "with open(CRIME_PATH / \"raw\" / \"communities.names\") as f:\n",
    "    names = f.readlines()\n",
    "    names = [n for n in names if n.startswith(\"@attribute\")]\n",
    "    _, names, dtypes = zip(*[n.split() for n in names])\n",
    "\n",
    "crime.columns = names\n",
    "# drop string columns\n",
    "crime = crime.drop(columns=[\"communityname\"])\n",
    "# drop non-predictive columns\n",
    "crime = crime.drop(columns=[\"county\", \"community\", \"fold\", \"state\"])\n",
    "\n",
    "nan_columns = crime.columns[crime.isna().any()].tolist()\n",
    "crime = crime.drop(columns=nan_columns)\n",
    "\n",
    "y = crime.pop(\"ViolentCrimesPerPop\").to_numpy()\n",
    "x = crime.to_numpy()\n",
    "\n",
    "preprocess(x, y, CRIME_PATH / \"processed\", stratify=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
