{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import joplen as jp\n",
    "from enums import *\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from itertools import product\n",
    "from ax import optimize\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from data_tools import WhiteWineQuality\n",
    "from pathlib import Path\n",
    "from chem_data import *\n",
    "from copy import copy, deepcopy\n",
    "import yaml\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "CACHE_DIR = Path(\"cache\") / \"runs\"\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DS_PATH = Path(\".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for some reason, these are necessary for the case statement to work\n",
    "import lightgbm as lgbm\n",
    "import sklearn.ensemble as ske\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttv_split(x, y, seed, indent=False):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Split data into train validation and test sets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x,\n",
    "        y,\n",
    "        test_size=0.2,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    x_train, x_val, y_train, y_val = train_test_split(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        test_size=0.25,\n",
    "        shuffle=True,\n",
    "    )\n",
    "\n",
    "    res = {\n",
    "        \"fraction\": [\n",
    "            float(_x.shape[0] / x.shape[0]) for _x in [x_train, x_val, x_test]\n",
    "        ],\n",
    "        \"size\": [int(_x.shape[0]) for _x in [x_train, x_val, x_test]],\n",
    "        \"features\": int(x_train.shape[1]),\n",
    "    }\n",
    "\n",
    "    pprint(res)\n",
    "\n",
    "    return x_train, x_val, x_test, y_train, y_val, y_test, res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred, squared=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_regressor(x_train, x_val, x_test, y_train, y_val, y_test, indent=False):\n",
    "    dummy = DummyRegressor(strategy=\"mean\")\n",
    "    dummy.fit(x_train, y_train)\n",
    "    y_pred = dummy.predict(x_test)\n",
    "\n",
    "    res = {\n",
    "        \"model_name\": dummy.__class__.__name__,\n",
    "        \"rmse\": float(rmse(y_test, y_pred)),\n",
    "    }\n",
    "\n",
    "    pprint(res)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_params = [\n",
    "    {\n",
    "        \"name\": \"max_leaf_nodes\",\n",
    "        \"type\": \"range\",\n",
    "        \"bounds\": [2, 32],\n",
    "        \"value_type\": \"int\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"n_estimators\",\n",
    "        \"type\": \"range\",\n",
    "        \"bounds\": [10, 1000],\n",
    "        \"value_type\": \"int\",\n",
    "        \"log_scale\": True,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"min_samples_leaf\",\n",
    "        \"type\": \"range\",\n",
    "        \"bounds\": [1, 32],\n",
    "        \"value_type\": \"int\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"min_impurity_decrease\",\n",
    "        \"type\": \"range\",\n",
    "        \"bounds\": [0.0, 1.0],\n",
    "        \"value_type\": \"float\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"random_state\",\n",
    "        \"type\": \"fixed\",\n",
    "        \"value\": 0,\n",
    "        \"value_type\": \"int\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"criterion\",\n",
    "        \"type\": \"fixed\",\n",
    "        \"value\": \"squared_error\",\n",
    "        \"value_type\": \"str\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"max_features\",\n",
    "        \"type\": \"fixed\",\n",
    "        \"value\": 1,\n",
    "        \"value_type\": \"int\",\n",
    "    },\n",
    "]\n",
    "\n",
    "lgbm_params = [\n",
    "    {\n",
    "        \"name\": \"num_leaves\",\n",
    "        \"type\": \"range\",\n",
    "        \"bounds\": [2, 32],\n",
    "        \"value_type\": \"int\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"n_estimators\",\n",
    "        \"type\": \"range\",\n",
    "        \"bounds\": [10, 1000],\n",
    "        \"value_type\": \"int\",\n",
    "        \"log_scale\": True,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"learning_rate\",\n",
    "        \"type\": \"range\",\n",
    "        \"bounds\": [1e-5, 1e-1],\n",
    "        \"value_type\": \"float\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"min_split_gain\",\n",
    "        \"type\": \"range\",\n",
    "        \"bounds\": [0.0, 1.0],\n",
    "        \"value_type\": \"float\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"reg_alpha\",\n",
    "        \"type\": \"range\",\n",
    "        \"bounds\": [1e-5, 1e1],\n",
    "        \"value_type\": \"float\",\n",
    "        \"log_scale\": True,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"reg_lambda\",\n",
    "        \"type\": \"range\",\n",
    "        \"bounds\": [1e-5, 1e1],\n",
    "        \"value_type\": \"float\",\n",
    "        \"log_scale\": True,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"random_state\",\n",
    "        \"type\": \"fixed\",\n",
    "        \"value\": 0,\n",
    "        \"value_type\": \"int\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"verbose\",\n",
    "        \"type\": \"fixed\",\n",
    "        \"value\": -1,\n",
    "        \"value_type\": \"int\",\n",
    "    },\n",
    "]\n",
    "\n",
    "xgb_params = [\n",
    "    {\n",
    "        \"name\": \"max_leaves\",\n",
    "        \"type\": \"range\",\n",
    "        \"bounds\": [2, 32],\n",
    "        \"value_type\": \"int\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"n_estimators\",\n",
    "        \"type\": \"range\",\n",
    "        \"bounds\": [10, 1000],\n",
    "        \"value_type\": \"int\",\n",
    "        \"log_scale\": True,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"learning_rate\",\n",
    "        \"type\": \"range\",\n",
    "        \"bounds\": [1e-5, 1e-1],\n",
    "        \"value_type\": \"float\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"grow_policy\",\n",
    "        \"type\": \"choice\",\n",
    "        \"values\": [\"depthwise\", \"lossguide\"],\n",
    "        \"value_type\": \"str\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"booster\",\n",
    "        \"type\": \"choice\",\n",
    "        \"values\": [\"gbtree\", \"dart\"],\n",
    "        \"value_type\": \"str\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"gamma\",\n",
    "        \"type\": \"range\",\n",
    "        \"bounds\": [1e-5, 1e1],\n",
    "        \"value_type\": \"float\",\n",
    "        \"log_scale\": True,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"random_state\",\n",
    "        \"type\": \"fixed\",\n",
    "        \"value\": 0,\n",
    "        \"value_type\": \"int\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"tree_method\",\n",
    "        \"type\": \"fixed\",\n",
    "        \"value\": \"approx\",\n",
    "        \"value_type\": \"str\",\n",
    "    },\n",
    "]\n",
    "joplen_params = [\n",
    "    {\n",
    "        \"name\": \"n_cells\",\n",
    "        \"type\": \"range\",\n",
    "        \"bounds\": [2, 32],\n",
    "        \"value_type\": \"int\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"n_partitions\",\n",
    "        \"type\": \"range\",\n",
    "        \"bounds\": [10, 1000],\n",
    "        \"value_type\": \"int\",\n",
    "        \"log_scale\": True,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"lam\",\n",
    "        \"type\": \"range\",\n",
    "        \"bounds\": [0.0, 2.0],\n",
    "        \"value_type\": \"float\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"mu\",\n",
    "        \"type\": \"range\",\n",
    "        \"bounds\": [1e-5, 1e-1],\n",
    "        \"value_type\": \"float\",\n",
    "        \"log_scale\": True,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"alpha\",\n",
    "        \"type\": \"range\",\n",
    "        \"bounds\": [1e-5, 1e1],\n",
    "        \"value_type\": \"float\",\n",
    "        \"log_scale\": True,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"random_state\",\n",
    "        \"type\": \"fixed\",\n",
    "        \"value\": 0,\n",
    "        \"value_type\": \"int\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"partitioner\",\n",
    "        \"type\": \"fixed\",\n",
    "        \"value\": \"jp.VPartition\",\n",
    "        \"value_type\": \"str\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"cell_model\",\n",
    "        \"type\": \"fixed\",\n",
    "        \"value\": \"CellModel.linear\",\n",
    "        \"value_type\": \"str\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"max_iters\",\n",
    "        \"type\": \"fixed\",\n",
    "        \"value\": 10000,\n",
    "        \"value_type\": \"int\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"norm_type\",\n",
    "        \"type\": \"fixed\",\n",
    "        \"value\": \"NormType.L21\",\n",
    "        \"value_type\": \"str\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"verbose\",\n",
    "        \"type\": \"fixed\",\n",
    "        \"value\": False,\n",
    "        \"value_type\": \"bool\",\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training functions\n",
    "\n",
    "\n",
    "def timer_decorator(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        return (result, start_time, end_time, elapsed_time)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "@timer_decorator\n",
    "def train_er(\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "):\n",
    "    er = ExtraTreesRegressor(**params)\n",
    "    er.fit(x_train, y_train.flatten())\n",
    "\n",
    "    val_error = float(rmse(y_val.flatten(), er.predict(x_val)))\n",
    "\n",
    "    if x_test is not None and y_test is not None:\n",
    "        test_error = float(rmse(y_test.flatten(), er.predict(x_test)))\n",
    "        return val_error, test_error, er\n",
    "    else:\n",
    "        return val_error, er\n",
    "\n",
    "\n",
    "@timer_decorator\n",
    "def train_lgbm(params, x_train, y_train, x_val, y_val, x_test=None, y_test=None):\n",
    "    lgbm = LGBMRegressor(**params)\n",
    "    lgbm.fit(\n",
    "        x_train,\n",
    "        y_train.flatten(),\n",
    "        # TODO: Need to re-enable validation set\n",
    "        # eval_set=[(x_val, y_val.flatten())],\n",
    "        # verbose=-1,\n",
    "        # callbacks=[],\n",
    "    )\n",
    "\n",
    "    val_error = float(rmse(y_val.flatten(), lgbm.predict(x_val)))\n",
    "\n",
    "    if x_test is not None and y_test is not None:\n",
    "        test_error = float(rmse(y_test.flatten(), lgbm.predict(x_test)))\n",
    "        return val_error, test_error, lgbm\n",
    "    else:\n",
    "        return val_error, lgbm\n",
    "\n",
    "\n",
    "@timer_decorator\n",
    "def train_xgboost(params, x_train, y_train, x_val, y_val, x_test=None, y_test=None):\n",
    "    xgb = XGBRegressor(**params)\n",
    "    xgb.fit(\n",
    "        x_train,\n",
    "        y_train.flatten(),\n",
    "        # TODO: Need to re-enable validation set\n",
    "        # eval_set=[(x_val, y_val.flatten())],\n",
    "    )\n",
    "\n",
    "    val_error = float(rmse(y_val.flatten(), xgb.predict(x_val)))\n",
    "\n",
    "    if x_test is not None and y_test is not None:\n",
    "        test_error = float(rmse(y_test.flatten(), xgb.predict(x_test)))\n",
    "        return val_error, test_error, xgb\n",
    "    else:\n",
    "        return val_error, xgb\n",
    "\n",
    "\n",
    "@timer_decorator\n",
    "def train_joplen(\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "):\n",
    "    joplen = jp.JOPLEn(\n",
    "        partitioner=eval(params[\"partitioner\"]),\n",
    "        n_cells=params[\"n_cells\"],\n",
    "        n_partitions=params[\"n_partitions\"],\n",
    "        random_state=params[\"random_state\"],\n",
    "        cell_model=eval(params[\"cell_model\"]),\n",
    "    )\n",
    "    joplen.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        val_x=x_val,\n",
    "        val_y=y_val,\n",
    "        max_iters=params[\"max_iters\"],\n",
    "        norm_type=eval(params[\"norm_type\"]),\n",
    "        verbose=params[\"verbose\"],\n",
    "        mu=params[\"mu\"],\n",
    "        lam=params[\"lam\"],\n",
    "        alpha=params[\"alpha\"],\n",
    "    )\n",
    "\n",
    "    val_error = float(rmse(y_val, joplen.predict(x_val)))\n",
    "\n",
    "    if x_test is not None and y_test is not None:\n",
    "        test_error = float(rmse(y_test, joplen.predict(x_test)))\n",
    "        return val_error, test_error, joplen\n",
    "    else:\n",
    "        return val_error, joplen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ax.service.ax_client import AxClient, ObjectiveProperties\n",
    "\n",
    "\n",
    "def optimize_model(model_class, ds_class, n_trials, minimize, loss_type):\n",
    "    ds = ds_class(DS_PATH)\n",
    "    print(\"\\n\\n\" + ds.name)\n",
    "    x, y = ds.get_data()\n",
    "\n",
    "    x_train, x_val, x_test, y_train, y_val, y_test, ttv_info = ttv_split(\n",
    "        x, y, 0, indent=True\n",
    "    )\n",
    "\n",
    "    dummy_info = dummy_regressor(\n",
    "        x_train, x_val, x_test, y_train, y_val, y_test, indent=True\n",
    "    )\n",
    "\n",
    "    match model_class:\n",
    "        case jp.JOPLEn:\n",
    "            params = joplen_params\n",
    "            train_fn = train_joplen\n",
    "        case lgbm.LGBMRegressor:\n",
    "            params = lgbm_params\n",
    "            train_fn = train_lgbm\n",
    "        case ske.ExtraTreesRegressor:\n",
    "            params = et_params\n",
    "            train_fn = train_er\n",
    "        case xgb.XGBRegressor:\n",
    "            params = xgb_params\n",
    "            train_fn = train_xgboost\n",
    "        case _:\n",
    "            raise ValueError(\"Model not supported.\")\n",
    "\n",
    "    exp_name = model_class.__name__ + \"_\" + ds_class.__name__\n",
    "    exp_path = Path(\"ax_runs\") / f\"{exp_name}.json\"\n",
    "\n",
    "    ax_client = AxClient(random_seed=0)\n",
    "\n",
    "    ax_client.create_experiment(\n",
    "        name=exp_name,\n",
    "        parameters=params,\n",
    "        objectives={loss_type: ObjectiveProperties(minimize=minimize)},\n",
    "        overwrite_existing_experiment=True,\n",
    "    )\n",
    "\n",
    "    for _ in range(n_trials):\n",
    "        round_params, trial_index = ax_client.get_next_trial()\n",
    "        try:\n",
    "            val_error, _ = train_fn(\n",
    "                round_params, x_train=x_train, y_train=y_train, x_val=x_val, y_val=y_val\n",
    "            )[0]\n",
    "            ax_client.complete_trial(trial_index=trial_index, raw_data=float(val_error))\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "            ax_client.abandon_trial(\n",
    "                trial_index=trial_index,\n",
    "                reason=str(e),\n",
    "            )\n",
    "\n",
    "    best_parameters, values = ax_client.get_best_parameters()\n",
    "\n",
    "    (val_error, test_error, _), _, _, train_time = train_fn(\n",
    "        best_parameters,\n",
    "        x_train=x_train,\n",
    "        y_train=y_train,\n",
    "        x_val=x_val,\n",
    "        y_val=y_val,\n",
    "        x_test=x_test,\n",
    "        y_test=y_test,\n",
    "    )\n",
    "\n",
    "    exp_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    ax_client.save_to_json_file(\n",
    "        filepath=exp_path,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"model_name\": model_class.__name__,\n",
    "        \"val_score\": val_error,\n",
    "        \"test_score\": test_error,\n",
    "        \"train_time\": train_time,\n",
    "        \"params\": best_parameters,\n",
    "        \"dummy_loss\": dummy_info[\"rmse\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = optimize_model(jp.JOPLEn, NPLogP, 10, True, \"rmse\")\n",
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = optimize_model(ske.ExtraTreesRegressor, NPLogP, 10, True, \"rmse\")\n",
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 10-10 17:46:07] ax.service.ax_client: Starting optimization with verbose logging. To disable logging, set the `verbose_logging` argument to `False`. Note that float values in the logs are rounded to 6 decimal points.\n",
      "[WARNING 10-10 17:46:07] ax.service.ax_client: Random seed set to 0. Note that this setting only affects the Sobol quasi-random generator and BoTorch-powered Bayesian optimization models. For the latter models, setting random seed to the same number for two optimizations will make the generated trials similar, but not exactly the same, and over time the trials will diverge more.\n",
      "/home/matt/code/school/joplen/vboost/my_env_310/lib/python3.10/site-packages/ax/core/parameter.py:517: UserWarning: `is_ordered` is not specified for `ChoiceParameter` \"grow_policy\". Defaulting to `False` for parameters of `ParameterType` STRING. To override this behavior (or avoid this warning), specify `is_ordered` during `ChoiceParameter` construction.\n",
      "  warn(\n",
      "/home/matt/code/school/joplen/vboost/my_env_310/lib/python3.10/site-packages/ax/core/parameter.py:517: UserWarning: `sort_values` is not specified for `ChoiceParameter` \"grow_policy\". Defaulting to `False` for parameters of `ParameterType` STRING. To override this behavior (or avoid this warning), specify `sort_values` during `ChoiceParameter` construction.\n",
      "  warn(\n",
      "/home/matt/code/school/joplen/vboost/my_env_310/lib/python3.10/site-packages/ax/core/parameter.py:517: UserWarning: `is_ordered` is not specified for `ChoiceParameter` \"booster\". Defaulting to `False` for parameters of `ParameterType` STRING. To override this behavior (or avoid this warning), specify `is_ordered` during `ChoiceParameter` construction.\n",
      "  warn(\n",
      "/home/matt/code/school/joplen/vboost/my_env_310/lib/python3.10/site-packages/ax/core/parameter.py:517: UserWarning: `sort_values` is not specified for `ChoiceParameter` \"booster\". Defaulting to `False` for parameters of `ParameterType` STRING. To override this behavior (or avoid this warning), specify `sort_values` during `ChoiceParameter` construction.\n",
      "  warn(\n",
      "[INFO 10-10 17:46:07] ax.service.utils.instantiation: Created search space: SearchSpace(parameters=[RangeParameter(name='max_leaves', parameter_type=INT, range=[2, 32]), RangeParameter(name='n_estimators', parameter_type=INT, range=[10, 1000], log_scale=True), RangeParameter(name='learning_rate', parameter_type=FLOAT, range=[1e-05, 0.1]), ChoiceParameter(name='grow_policy', parameter_type=STRING, values=['depthwise', 'lossguide'], is_ordered=False, sort_values=False), ChoiceParameter(name='booster', parameter_type=STRING, values=['gbtree', 'dart'], is_ordered=False, sort_values=False), RangeParameter(name='gamma', parameter_type=FLOAT, range=[1e-05, 10.0], log_scale=True), FixedParameter(name='random_state', parameter_type=INT, value=0), FixedParameter(name='tree_method', parameter_type=STRING, value='approx')], parameter_constraints=[]).\n",
      "[INFO 10-10 17:46:07] ax.modelbridge.dispatch_utils: Using Models.GPEI since there are more ordered parameters than there are categories for the unordered categorical parameters.\n",
      "[INFO 10-10 17:46:07] ax.modelbridge.dispatch_utils: Calculating the number of remaining initialization trials based on num_initialization_trials=None max_initialization_trials=None num_tunable_parameters=6 num_trials=None use_batch_trials=False\n",
      "[INFO 10-10 17:46:07] ax.modelbridge.dispatch_utils: calculated num_initialization_trials=12\n",
      "[INFO 10-10 17:46:07] ax.modelbridge.dispatch_utils: num_completed_initialization_trials=0 num_remaining_initialization_trials=12\n",
      "[INFO 10-10 17:46:07] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+GPEI', steps=[Sobol for 12 trials, GPEI for subsequent trials]). Iterations after 12 will take longer to generate due to model-fitting.\n",
      "[INFO 10-10 17:46:07] ax.service.ax_client: Generated new trial 0 with parameters {'max_leaves': 16, 'n_estimators': 150, 'learning_rate': 0.049453, 'gamma': 3.2e-05, 'grow_policy': 'depthwise', 'booster': 'gbtree', 'random_state': 0, 'tree_method': 'approx'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NP LogP\n",
      "{'features': 1704,\n",
      " 'fraction': [0.5918367346938775, 0.20408163265306123, 0.20408163265306123],\n",
      " 'size': [87, 30, 30]}\n",
      "{'model_name': 'DummyRegressor', 'rmse': 1.7831159637044283}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 10-10 17:46:23] ax.service.ax_client: Completed trial 0 with data: {'rmse': (0.991298, None)}.\n",
      "[INFO 10-10 17:46:23] ax.service.ax_client: Generated new trial 1 with parameters {'max_leaves': 21, 'n_estimators': 80, 'learning_rate': 0.08649, 'gamma': 0.083746, 'grow_policy': 'lossguide', 'booster': 'dart', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-10 17:46:35] ax.service.ax_client: Completed trial 1 with data: {'rmse': (1.006066, None)}.\n",
      "[INFO 10-10 17:46:35] ax.service.ax_client: Generated new trial 2 with parameters {'max_leaves': 27, 'n_estimators': 574, 'learning_rate': 0.013206, 'gamma': 3.077612, 'grow_policy': 'depthwise', 'booster': 'dart', 'random_state': 0, 'tree_method': 'approx'}.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/matt/code/school/joplen/vboost/compare_models.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m res \u001b[39m=\u001b[39m optimize_model(XGBRegressor, NPLogP, \u001b[39m10\u001b[39;49m, \u001b[39mTrue\u001b[39;49;00m, \u001b[39m\"\u001b[39;49m\u001b[39mrmse\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m res\n",
      "\u001b[1;32m/home/matt/code/school/joplen/vboost/compare_models.ipynb Cell 12\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X15sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m round_params, trial_index \u001b[39m=\u001b[39m ax_client\u001b[39m.\u001b[39mget_next_trial()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X15sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X15sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     val_error, _ \u001b[39m=\u001b[39m train_fn(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X15sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m         round_params, x_train\u001b[39m=\u001b[39;49mx_train, y_train\u001b[39m=\u001b[39;49my_train, x_val\u001b[39m=\u001b[39;49mx_val, y_val\u001b[39m=\u001b[39;49my_val\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X15sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     )[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X15sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     ax_client\u001b[39m.\u001b[39mcomplete_trial(trial_index\u001b[39m=\u001b[39mtrial_index, raw_data\u001b[39m=\u001b[39m\u001b[39mfloat\u001b[39m(val_error))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X15sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;32m/home/matt/code/school/joplen/vboost/compare_models.ipynb Cell 12\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     elapsed_time \u001b[39m=\u001b[39m end_time \u001b[39m-\u001b[39m start_time\n",
      "\u001b[1;32m/home/matt/code/school/joplen/vboost/compare_models.ipynb Cell 12\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X15sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39m@timer_decorator\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X15sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_xgboost\u001b[39m(params, x_train, y_train, x_val, y_val, x_test\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, y_test\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X15sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     xgb \u001b[39m=\u001b[39m XGBRegressor(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X15sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     xgb\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X15sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m         x_train,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X15sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m         y_train\u001b[39m.\u001b[39;49mflatten(),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X15sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m         \u001b[39m# TODO: Need to re-enable validation set\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X15sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m         \u001b[39m# eval_set=[(x_val, y_val.flatten())],\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X15sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X15sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     val_error \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(rmse(y_val\u001b[39m.\u001b[39mflatten(), xgb\u001b[39m.\u001b[39mpredict(x_val)))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X15sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m     \u001b[39mif\u001b[39;00m x_test \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m y_test \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/code/school/joplen/vboost/my_env_310/lib/python3.10/site-packages/xgboost/core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    728\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 729\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/code/school/joplen/vboost/my_env_310/lib/python3.10/site-packages/xgboost/sklearn.py:1086\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1075\u001b[0m     obj \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m (\n\u001b[1;32m   1078\u001b[0m     model,\n\u001b[1;32m   1079\u001b[0m     metric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1084\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1085\u001b[0m )\n\u001b[0;32m-> 1086\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[1;32m   1087\u001b[0m     params,\n\u001b[1;32m   1088\u001b[0m     train_dmatrix,\n\u001b[1;32m   1089\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(),\n\u001b[1;32m   1090\u001b[0m     evals\u001b[39m=\u001b[39;49mevals,\n\u001b[1;32m   1091\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[1;32m   1092\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[1;32m   1093\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[1;32m   1094\u001b[0m     custom_metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[1;32m   1095\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   1096\u001b[0m     xgb_model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1097\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m   1098\u001b[0m )\n\u001b[1;32m   1100\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[1;32m   1101\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/code/school/joplen/vboost/my_env_310/lib/python3.10/site-packages/xgboost/core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    728\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 729\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/code/school/joplen/vboost/my_env_310/lib/python3.10/site-packages/xgboost/training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    180\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m bst\u001b[39m.\u001b[39;49mupdate(dtrain, i, obj)\n\u001b[1;32m    182\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/code/school/joplen/vboost/my_env_310/lib/python3.10/site-packages/xgboost/core.py:2050\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2048\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2049\u001b[0m     _check_call(\n\u001b[0;32m-> 2050\u001b[0m         _LIB\u001b[39m.\u001b[39;49mXGBoosterUpdateOneIter(\n\u001b[1;32m   2051\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle, ctypes\u001b[39m.\u001b[39;49mc_int(iteration), dtrain\u001b[39m.\u001b[39;49mhandle\n\u001b[1;32m   2052\u001b[0m         )\n\u001b[1;32m   2053\u001b[0m     )\n\u001b[1;32m   2054\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2055\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(dtrain, output_margin\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "res = optimize_model(XGBRegressor, NPLogP, 10, True, \"rmse\")\n",
    "res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 10-11 06:23:20] ax.service.ax_client: Starting optimization with verbose logging. To disable logging, set the `verbose_logging` argument to `False`. Note that float values in the logs are rounded to 6 decimal points.\n",
      "[WARNING 10-11 06:23:20] ax.service.ax_client: Random seed set to 0. Note that this setting only affects the Sobol quasi-random generator and BoTorch-powered Bayesian optimization models. For the latter models, setting random seed to the same number for two optimizations will make the generated trials similar, but not exactly the same, and over time the trials will diverge more.\n",
      "/home/matt/code/school/joplen/vboost/my_env_310/lib/python3.10/site-packages/ax/core/parameter.py:517: UserWarning: `is_ordered` is not specified for `ChoiceParameter` \"grow_policy\". Defaulting to `False` for parameters of `ParameterType` STRING. To override this behavior (or avoid this warning), specify `is_ordered` during `ChoiceParameter` construction.\n",
      "  warn(\n",
      "/home/matt/code/school/joplen/vboost/my_env_310/lib/python3.10/site-packages/ax/core/parameter.py:517: UserWarning: `sort_values` is not specified for `ChoiceParameter` \"grow_policy\". Defaulting to `False` for parameters of `ParameterType` STRING. To override this behavior (or avoid this warning), specify `sort_values` during `ChoiceParameter` construction.\n",
      "  warn(\n",
      "/home/matt/code/school/joplen/vboost/my_env_310/lib/python3.10/site-packages/ax/core/parameter.py:517: UserWarning: `is_ordered` is not specified for `ChoiceParameter` \"booster\". Defaulting to `False` for parameters of `ParameterType` STRING. To override this behavior (or avoid this warning), specify `is_ordered` during `ChoiceParameter` construction.\n",
      "  warn(\n",
      "/home/matt/code/school/joplen/vboost/my_env_310/lib/python3.10/site-packages/ax/core/parameter.py:517: UserWarning: `sort_values` is not specified for `ChoiceParameter` \"booster\". Defaulting to `False` for parameters of `ParameterType` STRING. To override this behavior (or avoid this warning), specify `sort_values` during `ChoiceParameter` construction.\n",
      "  warn(\n",
      "[INFO 10-11 06:23:20] ax.service.utils.instantiation: Created search space: SearchSpace(parameters=[RangeParameter(name='max_leaves', parameter_type=INT, range=[2, 32]), RangeParameter(name='n_estimators', parameter_type=INT, range=[10, 1000], log_scale=True), RangeParameter(name='learning_rate', parameter_type=FLOAT, range=[1e-05, 0.1]), ChoiceParameter(name='grow_policy', parameter_type=STRING, values=['depthwise', 'lossguide'], is_ordered=False, sort_values=False), ChoiceParameter(name='booster', parameter_type=STRING, values=['gbtree', 'dart'], is_ordered=False, sort_values=False), RangeParameter(name='gamma', parameter_type=FLOAT, range=[1e-05, 10.0], log_scale=True), FixedParameter(name='random_state', parameter_type=INT, value=0), FixedParameter(name='tree_method', parameter_type=STRING, value='approx')], parameter_constraints=[]).\n",
      "[INFO 10-11 06:23:20] ax.modelbridge.dispatch_utils: Using Models.GPEI since there are more ordered parameters than there are categories for the unordered categorical parameters.\n",
      "[INFO 10-11 06:23:20] ax.modelbridge.dispatch_utils: Calculating the number of remaining initialization trials based on num_initialization_trials=None max_initialization_trials=None num_tunable_parameters=6 num_trials=None use_batch_trials=False\n",
      "[INFO 10-11 06:23:20] ax.modelbridge.dispatch_utils: calculated num_initialization_trials=12\n",
      "[INFO 10-11 06:23:20] ax.modelbridge.dispatch_utils: num_completed_initialization_trials=0 num_remaining_initialization_trials=12\n",
      "[INFO 10-11 06:23:20] ax.modelbridge.dispatch_utils: Using Bayesian Optimization generation strategy: GenerationStrategy(name='Sobol+GPEI', steps=[Sobol for 12 trials, GPEI for subsequent trials]). Iterations after 12 will take longer to generate due to model-fitting.\n",
      "[INFO 10-11 06:23:20] ax.service.ax_client: Generated new trial 0 with parameters {'max_leaves': 16, 'n_estimators': 150, 'learning_rate': 0.049453, 'gamma': 3.2e-05, 'grow_policy': 'depthwise', 'booster': 'gbtree', 'random_state': 0, 'tree_method': 'approx'}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "NP LogP\n",
      "{'features': 1704,\n",
      " 'fraction': [0.5918367346938775, 0.20408163265306123, 0.20408163265306123],\n",
      " 'size': [87, 30, 30]}\n",
      "{'model_name': 'DummyRegressor', 'rmse': 1.7831159637044283}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 10-11 06:25:00] ax.service.ax_client: Completed trial 0 with data: {'rmse': (0.991298, None)}.\n",
      "[INFO 10-11 06:25:00] ax.service.ax_client: Generated new trial 1 with parameters {'max_leaves': 21, 'n_estimators': 80, 'learning_rate': 0.08649, 'gamma': 0.083746, 'grow_policy': 'lossguide', 'booster': 'dart', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 06:26:26] ax.service.ax_client: Completed trial 1 with data: {'rmse': (1.006066, None)}.\n",
      "[INFO 10-11 06:26:26] ax.service.ax_client: Generated new trial 2 with parameters {'max_leaves': 27, 'n_estimators': 574, 'learning_rate': 0.013206, 'gamma': 3.077612, 'grow_policy': 'depthwise', 'booster': 'dart', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 06:33:42] ax.service.ax_client: Completed trial 2 with data: {'rmse': (1.074275, None)}.\n",
      "[INFO 10-11 06:33:42] ax.service.ax_client: Generated new trial 3 with parameters {'max_leaves': 3, 'n_estimators': 10, 'learning_rate': 0.050871, 'gamma': 0.001162, 'grow_policy': 'lossguide', 'booster': 'gbtree', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 06:33:42] ax.service.ax_client: Completed trial 3 with data: {'rmse': (1.506007, None)}.\n",
      "[INFO 10-11 06:33:43] ax.service.ax_client: Generated new trial 4 with parameters {'max_leaves': 8, 'n_estimators': 511, 'learning_rate': 0.089878, 'gamma': 9e-05, 'grow_policy': 'lossguide', 'booster': 'gbtree', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 06:34:07] ax.service.ax_client: Completed trial 4 with data: {'rmse': (0.999461, None)}.\n",
      "[INFO 10-11 06:34:07] ax.service.ax_client: Generated new trial 5 with parameters {'max_leaves': 30, 'n_estimators': 27, 'learning_rate': 0.027316, 'gamma': 0.041874, 'grow_policy': 'depthwise', 'booster': 'dart', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 06:34:11] ax.service.ax_client: Completed trial 5 with data: {'rmse': (1.296986, None)}.\n",
      "[INFO 10-11 06:34:11] ax.service.ax_client: Generated new trial 6 with parameters {'max_leaves': 20, 'n_estimators': 224, 'learning_rate': 0.07238, 'gamma': 1.139763, 'grow_policy': 'lossguide', 'booster': 'dart', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 06:35:16] ax.service.ax_client: Completed trial 6 with data: {'rmse': (1.017516, None)}.\n",
      "[INFO 10-11 06:35:16] ax.service.ax_client: Generated new trial 7 with parameters {'max_leaves': 10, 'n_estimators': 40, 'learning_rate': 0.010446, 'gamma': 0.002452, 'grow_policy': 'depthwise', 'booster': 'gbtree', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 06:35:21] ax.service.ax_client: Completed trial 7 with data: {'rmse': (1.433102, None)}.\n",
      "[INFO 10-11 06:35:21] ax.service.ax_client: Generated new trial 8 with parameters {'max_leaves': 12, 'n_estimators': 906, 'learning_rate': 0.0639, 'gamma': 0.000531, 'grow_policy': 'depthwise', 'booster': 'dart', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 06:52:19] ax.service.ax_client: Completed trial 8 with data: {'rmse': (0.976723, None)}.\n",
      "[INFO 10-11 06:52:19] ax.service.ax_client: Generated new trial 9 with parameters {'max_leaves': 18, 'n_estimators': 15, 'learning_rate': 0.000175, 'gamma': 7.910765, 'grow_policy': 'lossguide', 'booster': 'gbtree', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 06:52:21] ax.service.ax_client: Completed trial 9 with data: {'rmse': (1.780588, None)}.\n",
      "[INFO 10-11 06:52:21] ax.service.ax_client: Generated new trial 10 with parameters {'max_leaves': 32, 'n_estimators': 122, 'learning_rate': 0.098752, 'gamma': 0.193281, 'grow_policy': 'depthwise', 'booster': 'gbtree', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 06:52:25] ax.service.ax_client: Completed trial 10 with data: {'rmse': (0.991331, None)}.\n",
      "[INFO 10-11 06:52:25] ax.service.ax_client: Generated new trial 11 with parameters {'max_leaves': 6, 'n_estimators': 71, 'learning_rate': 0.037194, 'gamma': 1.3e-05, 'grow_policy': 'lossguide', 'booster': 'dart', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 06:52:36] ax.service.ax_client: Completed trial 11 with data: {'rmse': (1.05291, None)}.\n",
      "[INFO 10-11 06:52:36] ax.service.ax_client: Generated new trial 12 with parameters {'max_leaves': 23, 'n_estimators': 224, 'learning_rate': 0.080203, 'gamma': 0.000476, 'grow_policy': 'depthwise', 'booster': 'gbtree', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 06:52:54] ax.service.ax_client: Completed trial 12 with data: {'rmse': (1.029612, None)}.\n",
      "[INFO 10-11 06:52:54] ax.service.ax_client: Generated new trial 13 with parameters {'max_leaves': 14, 'n_estimators': 390, 'learning_rate': 0.069533, 'gamma': 1.2e-05, 'grow_policy': 'depthwise', 'booster': 'gbtree', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 06:53:23] ax.service.ax_client: Completed trial 13 with data: {'rmse': (0.997037, None)}.\n",
      "[INFO 10-11 06:53:24] ax.service.ax_client: Generated new trial 14 with parameters {'max_leaves': 14, 'n_estimators': 405, 'learning_rate': 0.06885, 'gamma': 0.002567, 'grow_policy': 'lossguide', 'booster': 'dart', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 06:56:57] ax.service.ax_client: Completed trial 14 with data: {'rmse': (1.017324, None)}.\n",
      "[INFO 10-11 06:56:57] ax.service.ax_client: Generated new trial 15 with parameters {'max_leaves': 15, 'n_estimators': 434, 'learning_rate': 0.071037, 'gamma': 0.004258, 'grow_policy': 'depthwise', 'booster': 'dart', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 07:03:07] ax.service.ax_client: Completed trial 15 with data: {'rmse': (0.985822, None)}.\n",
      "[INFO 10-11 07:03:08] ax.service.ax_client: Generated new trial 16 with parameters {'max_leaves': 15, 'n_estimators': 549, 'learning_rate': 0.070073, 'gamma': 0.001605, 'grow_policy': 'depthwise', 'booster': 'dart', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 07:09:16] ax.service.ax_client: Completed trial 16 with data: {'rmse': (0.993118, None)}.\n",
      "[INFO 10-11 07:09:16] ax.service.ax_client: Generated new trial 17 with parameters {'max_leaves': 8, 'n_estimators': 657, 'learning_rate': 0.067111, 'gamma': 0.007383, 'grow_policy': 'depthwise', 'booster': 'dart', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 07:18:15] ax.service.ax_client: Completed trial 17 with data: {'rmse': (0.995842, None)}.\n",
      "[INFO 10-11 07:18:16] ax.service.ax_client: Generated new trial 18 with parameters {'max_leaves': 12, 'n_estimators': 639, 'learning_rate': 0.064235, 'gamma': 0.000198, 'grow_policy': 'depthwise', 'booster': 'dart', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 07:26:00] ax.service.ax_client: Completed trial 18 with data: {'rmse': (0.975494, None)}.\n",
      "[INFO 10-11 07:26:01] ax.service.ax_client: Generated new trial 19 with parameters {'max_leaves': 11, 'n_estimators': 590, 'learning_rate': 0.061642, 'gamma': 8.5e-05, 'grow_policy': 'depthwise', 'booster': 'dart', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 07:33:07] ax.service.ax_client: Completed trial 19 with data: {'rmse': (0.935118, None)}.\n",
      "[INFO 10-11 07:33:08] ax.service.ax_client: Generated new trial 20 with parameters {'max_leaves': 9, 'n_estimators': 450, 'learning_rate': 0.056653, 'gamma': 1e-05, 'grow_policy': 'depthwise', 'booster': 'dart', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 07:38:16] ax.service.ax_client: Completed trial 20 with data: {'rmse': (0.980947, None)}.\n",
      "[INFO 10-11 07:38:17] ax.service.ax_client: Generated new trial 21 with parameters {'max_leaves': 11, 'n_estimators': 525, 'learning_rate': 0.064096, 'gamma': 5.7e-05, 'grow_policy': 'depthwise', 'booster': 'dart', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 07:44:29] ax.service.ax_client: Completed trial 21 with data: {'rmse': (0.955627, None)}.\n",
      "[INFO 10-11 07:44:30] ax.service.ax_client: Generated new trial 22 with parameters {'max_leaves': 2, 'n_estimators': 10, 'learning_rate': 1e-05, 'gamma': 1e-05, 'grow_policy': 'lossguide', 'booster': 'dart', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 07:44:31] ax.service.ax_client: Completed trial 22 with data: {'rmse': (1.783273, None)}.\n",
      "[INFO 10-11 07:44:31] ax.service.ax_client: Generated new trial 23 with parameters {'max_leaves': 11, 'n_estimators': 460, 'learning_rate': 0.061741, 'gamma': 8.7e-05, 'grow_policy': 'depthwise', 'booster': 'dart', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 07:48:54] ax.service.ax_client: Completed trial 23 with data: {'rmse': (0.940952, None)}.\n",
      "[INFO 10-11 07:48:54] ax.service.ax_client: Generated new trial 24 with parameters {'max_leaves': 11, 'n_estimators': 390, 'learning_rate': 0.059304, 'gamma': 0.000169, 'grow_policy': 'depthwise', 'booster': 'dart', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 07:52:27] ax.service.ax_client: Completed trial 24 with data: {'rmse': (0.95015, None)}.\n",
      "[INFO 10-11 07:52:28] ax.service.ax_client: Generated new trial 25 with parameters {'max_leaves': 11, 'n_estimators': 394, 'learning_rate': 0.061377, 'gamma': 0.000117, 'grow_policy': 'depthwise', 'booster': 'dart', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 07:55:54] ax.service.ax_client: Completed trial 25 with data: {'rmse': (0.944356, None)}.\n",
      "[INFO 10-11 07:55:55] ax.service.ax_client: Generated new trial 26 with parameters {'max_leaves': 11, 'n_estimators': 384, 'learning_rate': 0.061001, 'gamma': 0.000128, 'grow_policy': 'depthwise', 'booster': 'dart', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 07:59:02] ax.service.ax_client: Completed trial 26 with data: {'rmse': (0.958279, None)}.\n",
      "[INFO 10-11 07:59:03] ax.service.ax_client: Generated new trial 27 with parameters {'max_leaves': 2, 'n_estimators': 10, 'learning_rate': 1e-05, 'gamma': 10.0, 'grow_policy': 'depthwise', 'booster': 'dart', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 07:59:03] ax.service.ax_client: Completed trial 27 with data: {'rmse': (1.783273, None)}.\n",
      "[INFO 10-11 07:59:04] ax.service.ax_client: Generated new trial 28 with parameters {'max_leaves': 11, 'n_estimators': 409, 'learning_rate': 0.059405, 'gamma': 0.00015, 'grow_policy': 'depthwise', 'booster': 'dart', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 08:02:20] ax.service.ax_client: Completed trial 28 with data: {'rmse': (0.93807, None)}.\n",
      "[INFO 10-11 08:02:21] ax.service.ax_client: Generated new trial 29 with parameters {'max_leaves': 11, 'n_estimators': 440, 'learning_rate': 0.057242, 'gamma': 0.000154, 'grow_policy': 'depthwise', 'booster': 'dart', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 08:06:32] ax.service.ax_client: Completed trial 29 with data: {'rmse': (0.954967, None)}.\n",
      "[INFO 10-11 08:06:33] ax.service.ax_client: Generated new trial 30 with parameters {'max_leaves': 11, 'n_estimators': 391, 'learning_rate': 0.060848, 'gamma': 0.000144, 'grow_policy': 'depthwise', 'booster': 'dart', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 08:10:09] ax.service.ax_client: Completed trial 30 with data: {'rmse': (0.957428, None)}.\n",
      "[INFO 10-11 08:10:10] ax.service.ax_client: Generated new trial 31 with parameters {'max_leaves': 11, 'n_estimators': 415, 'learning_rate': 0.060155, 'gamma': 0.000138, 'grow_policy': 'depthwise', 'booster': 'dart', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 08:14:06] ax.service.ax_client: Completed trial 31 with data: {'rmse': (0.931242, None)}.\n",
      "[INFO 10-11 08:14:07] ax.service.ax_client: Generated new trial 32 with parameters {'max_leaves': 11, 'n_estimators': 406, 'learning_rate': 0.059668, 'gamma': 0.000153, 'grow_policy': 'depthwise', 'booster': 'dart', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 08:17:39] ax.service.ax_client: Completed trial 32 with data: {'rmse': (0.936972, None)}.\n",
      "[INFO 10-11 08:17:40] ax.service.ax_client: Generated new trial 33 with parameters {'max_leaves': 11, 'n_estimators': 403, 'learning_rate': 0.05938, 'gamma': 0.000139, 'grow_policy': 'depthwise', 'booster': 'dart', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 08:21:04] ax.service.ax_client: Completed trial 33 with data: {'rmse': (0.939878, None)}.\n",
      "[INFO 10-11 08:21:05] ax.service.ax_client: Generated new trial 34 with parameters {'max_leaves': 11, 'n_estimators': 402, 'learning_rate': 0.059213, 'gamma': 0.000143, 'grow_policy': 'depthwise', 'booster': 'dart', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 08:25:05] ax.service.ax_client: Completed trial 34 with data: {'rmse': (0.951071, None)}.\n",
      "[INFO 10-11 08:25:06] ax.service.ax_client: Generated new trial 35 with parameters {'max_leaves': 10, 'n_estimators': 413, 'learning_rate': 0.059452, 'gamma': 0.000144, 'grow_policy': 'depthwise', 'booster': 'dart', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 08:29:08] ax.service.ax_client: Completed trial 35 with data: {'rmse': (0.98296, None)}.\n",
      "[INFO 10-11 08:29:09] ax.service.ax_client: Generated new trial 36 with parameters {'max_leaves': 14, 'n_estimators': 362, 'learning_rate': 0.057156, 'gamma': 4.2e-05, 'grow_policy': 'depthwise', 'booster': 'dart', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 08:31:36] ax.service.ax_client: Completed trial 36 with data: {'rmse': (1.003172, None)}.\n",
      "[INFO 10-11 08:31:40] ax.service.ax_client: Generated new trial 37 with parameters {'max_leaves': 11, 'n_estimators': 577, 'learning_rate': 0.050004, 'gamma': 1.2e-05, 'grow_policy': 'depthwise', 'booster': 'dart', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 08:38:52] ax.service.ax_client: Completed trial 37 with data: {'rmse': (0.961446, None)}.\n",
      "[INFO 10-11 08:38:56] ax.service.ax_client: Generated new trial 38 with parameters {'max_leaves': 29, 'n_estimators': 280, 'learning_rate': 0.1, 'gamma': 0.024221, 'grow_policy': 'depthwise', 'booster': 'gbtree', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 08:39:07] ax.service.ax_client: Completed trial 38 with data: {'rmse': (0.99783, None)}.\n",
      "[INFO 10-11 08:39:09] ax.service.ax_client: Generated new trial 39 with parameters {'max_leaves': 32, 'n_estimators': 451, 'learning_rate': 0.1, 'gamma': 0.014046, 'grow_policy': 'depthwise', 'booster': 'gbtree', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 08:39:23] ax.service.ax_client: Completed trial 39 with data: {'rmse': (1.005261, None)}.\n",
      "[INFO 10-11 08:39:26] ax.service.ax_client: Generated new trial 40 with parameters {'max_leaves': 11, 'n_estimators': 380, 'learning_rate': 0.060575, 'gamma': 7e-05, 'grow_policy': 'depthwise', 'booster': 'dart', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 08:42:57] ax.service.ax_client: Completed trial 40 with data: {'rmse': (0.947351, None)}.\n",
      "[INFO 10-11 08:43:02] ax.service.ax_client: Generated new trial 41 with parameters {'max_leaves': 30, 'n_estimators': 163, 'learning_rate': 0.1, 'gamma': 5.361105, 'grow_policy': 'depthwise', 'booster': 'gbtree', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 08:43:03] ax.service.ax_client: Completed trial 41 with data: {'rmse': (1.154216, None)}.\n",
      "[INFO 10-11 08:43:06] ax.service.ax_client: Generated new trial 42 with parameters {'max_leaves': 11, 'n_estimators': 530, 'learning_rate': 0.055511, 'gamma': 8.1e-05, 'grow_policy': 'depthwise', 'booster': 'dart', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 08:49:22] ax.service.ax_client: Completed trial 42 with data: {'rmse': (0.946087, None)}.\n",
      "[INFO 10-11 08:49:25] ax.service.ax_client: Generated new trial 43 with parameters {'max_leaves': 31, 'n_estimators': 41, 'learning_rate': 0.099147, 'gamma': 0.003417, 'grow_policy': 'depthwise', 'booster': 'gbtree', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 08:49:30] ax.service.ax_client: Completed trial 43 with data: {'rmse': (1.048698, None)}.\n",
      "[INFO 10-11 08:49:32] ax.service.ax_client: Generated new trial 44 with parameters {'max_leaves': 32, 'n_estimators': 170, 'learning_rate': 0.078284, 'gamma': 0.04308, 'grow_policy': 'depthwise', 'booster': 'gbtree', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 08:49:38] ax.service.ax_client: Completed trial 44 with data: {'rmse': (1.009156, None)}.\n",
      "[INFO 10-11 08:49:42] ax.service.ax_client: Generated new trial 45 with parameters {'max_leaves': 15, 'n_estimators': 247, 'learning_rate': 0.065224, 'gamma': 0.000329, 'grow_policy': 'depthwise', 'booster': 'gbtree', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 08:50:01] ax.service.ax_client: Completed trial 45 with data: {'rmse': (0.962926, None)}.\n",
      "[INFO 10-11 08:50:04] ax.service.ax_client: Generated new trial 46 with parameters {'max_leaves': 11, 'n_estimators': 693, 'learning_rate': 0.059517, 'gamma': 0.000169, 'grow_policy': 'depthwise', 'booster': 'dart', 'random_state': 0, 'tree_method': 'approx'}.\n",
      "[INFO 10-11 09:02:30] ax.service.ax_client: Completed trial 46 with data: {'rmse': (0.939434, None)}.\n",
      "[INFO 10-11 09:02:32] ax.service.ax_client: Generated new trial 47 with parameters {'max_leaves': 11, 'n_estimators': 766, 'learning_rate': 0.060046, 'gamma': 0.000179, 'grow_policy': 'depthwise', 'booster': 'dart', 'random_state': 0, 'tree_method': 'approx'}.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/matt/code/school/joplen/vboost/compare_models.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X14sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m k \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X14sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mfor\u001b[39;00m model_type, d \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(model_types, dicts):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X14sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     d[k] \u001b[39m=\u001b[39m optimize_model(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X14sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m         model_class\u001b[39m=\u001b[39;49mmodel_type,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X14sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m         ds_class\u001b[39m=\u001b[39;49mds,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X14sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m         n_trials\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X14sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m         minimize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X14sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m         loss_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mrmse\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X14sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     )\n",
      "\u001b[1;32m/home/matt/code/school/joplen/vboost/compare_models.ipynb Cell 12\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X14sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m round_params, trial_index \u001b[39m=\u001b[39m ax_client\u001b[39m.\u001b[39mget_next_trial()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X14sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X14sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     val_error, _ \u001b[39m=\u001b[39m train_fn(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X14sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m         round_params, x_train\u001b[39m=\u001b[39;49mx_train, y_train\u001b[39m=\u001b[39;49my_train, x_val\u001b[39m=\u001b[39;49mx_val, y_val\u001b[39m=\u001b[39;49my_val\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X14sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     )[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X14sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     ax_client\u001b[39m.\u001b[39mcomplete_trial(trial_index\u001b[39m=\u001b[39mtrial_index, raw_data\u001b[39m=\u001b[39m\u001b[39mfloat\u001b[39m(val_error))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X14sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;32m/home/matt/code/school/joplen/vboost/compare_models.ipynb Cell 12\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     elapsed_time \u001b[39m=\u001b[39m end_time \u001b[39m-\u001b[39m start_time\n",
      "\u001b[1;32m/home/matt/code/school/joplen/vboost/compare_models.ipynb Cell 12\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X14sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39m@timer_decorator\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X14sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_xgboost\u001b[39m(params, x_train, y_train, x_val, y_val, x_test\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, y_test\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X14sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m     xgb \u001b[39m=\u001b[39m XGBRegressor(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X14sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     xgb\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X14sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m         x_train,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X14sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m         y_train\u001b[39m.\u001b[39;49mflatten(),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X14sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m         \u001b[39m# TODO: Need to re-enable validation set\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X14sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m         \u001b[39m# eval_set=[(x_val, y_val.flatten())],\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X14sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X14sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     val_error \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(rmse(y_val\u001b[39m.\u001b[39mflatten(), xgb\u001b[39m.\u001b[39mpredict(x_val)))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/matt/code/school/joplen/vboost/compare_models.ipynb#X14sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m     \u001b[39mif\u001b[39;00m x_test \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m y_test \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/code/school/joplen/vboost/my_env_310/lib/python3.10/site-packages/xgboost/core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    728\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 729\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/code/school/joplen/vboost/my_env_310/lib/python3.10/site-packages/xgboost/sklearn.py:1086\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1075\u001b[0m     obj \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m (\n\u001b[1;32m   1078\u001b[0m     model,\n\u001b[1;32m   1079\u001b[0m     metric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1084\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1085\u001b[0m )\n\u001b[0;32m-> 1086\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[1;32m   1087\u001b[0m     params,\n\u001b[1;32m   1088\u001b[0m     train_dmatrix,\n\u001b[1;32m   1089\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(),\n\u001b[1;32m   1090\u001b[0m     evals\u001b[39m=\u001b[39;49mevals,\n\u001b[1;32m   1091\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[1;32m   1092\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[1;32m   1093\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[1;32m   1094\u001b[0m     custom_metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[1;32m   1095\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   1096\u001b[0m     xgb_model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1097\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m   1098\u001b[0m )\n\u001b[1;32m   1100\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[1;32m   1101\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/code/school/joplen/vboost/my_env_310/lib/python3.10/site-packages/xgboost/core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    728\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 729\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/code/school/joplen/vboost/my_env_310/lib/python3.10/site-packages/xgboost/training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    180\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m bst\u001b[39m.\u001b[39;49mupdate(dtrain, i, obj)\n\u001b[1;32m    182\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/code/school/joplen/vboost/my_env_310/lib/python3.10/site-packages/xgboost/core.py:2050\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2048\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2049\u001b[0m     _check_call(\n\u001b[0;32m-> 2050\u001b[0m         _LIB\u001b[39m.\u001b[39;49mXGBoosterUpdateOneIter(\n\u001b[1;32m   2051\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle, ctypes\u001b[39m.\u001b[39;49mc_int(iteration), dtrain\u001b[39m.\u001b[39;49mhandle\n\u001b[1;32m   2052\u001b[0m         )\n\u001b[1;32m   2053\u001b[0m     )\n\u001b[1;32m   2054\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2055\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(dtrain, output_margin\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "datasets = [\n",
    "    NPLogP,\n",
    "    NPZetaP,\n",
    "    ProtSol,\n",
    "    MolLogP,\n",
    "    MolHenry,\n",
    "    MolBoil,\n",
    "    MolMelt,\n",
    "]\n",
    "\n",
    "# joplen_res = {}\n",
    "# er_res = {}\n",
    "xgb_res = {}\n",
    "\n",
    "model_types = [\n",
    "    # jp.JOPLEn,\n",
    "    # ske.ExtraTreesRegressor,\n",
    "    xgb.XGBRegressor,\n",
    "]\n",
    "dicts = [\n",
    "    # joplen_res,\n",
    "    # er_res,\n",
    "    xgb_res,\n",
    "]\n",
    "\n",
    "for ds in datasets:\n",
    "    k = ds.__name__\n",
    "    for model_type, d in zip(model_types, dicts):\n",
    "        d[k] = optimize_model(\n",
    "            model_class=model_type,\n",
    "            ds_class=ds,\n",
    "            n_trials=100,\n",
    "            minimize=True,\n",
    "            loss_type=\"rmse\",\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NPLogP': 1.7831159637044283,\n",
       " 'NPZetaP': 34.064111323553945,\n",
       " 'ProtSol': 32.954534297678684,\n",
       " 'MolLogP': 1.841579159435372,\n",
       " 'MolHenry': 6.633168933233905,\n",
       " 'MolBoil': 83.22852562904664,\n",
       " 'MolMelt': 95.47062435805638}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v[\"dummy_loss\"] for k, v in joplen_res.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NPLogP': (0.9692163508285933, 0.8475834631371792),\n",
       " 'NPZetaP': (16.0458870062652, 23.03845015049339),\n",
       " 'ProtSol': (30.360103381391813, 31.555057577931034),\n",
       " 'MolLogP': (0.9351424854032594, 0.9479114131618289),\n",
       " 'MolHenry': (3.540368616878593, 3.7528611248193915),\n",
       " 'MolBoil': (62.961585482623335, 64.66564616385752),\n",
       " 'MolMelt': (59.98263321353336, 66.2503041016325)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: (v[\"val_score\"], v[\"test_score\"]) for k, v in joplen_res.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NPLogP': (1.4865816261209421, 1.43066223189318),\n",
       " 'NPZetaP': (20.430389546191787, 29.987465365315803),\n",
       " 'ProtSol': (32.83985954195217, 32.599932984909216),\n",
       " 'MolLogP': (1.7330388086119701, 1.736951246817223),\n",
       " 'MolHenry': (5.8139450056906155, 5.706304226866421),\n",
       " 'MolBoil': (78.39561956654113, 76.61976017902927),\n",
       " 'MolMelt': (89.44327967046853, 84.64096679642299)}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: (v[\"val_score\"], v[\"test_score\"]) for k, v in er_res.items()}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
