{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "from pmlb import classification_dataset_names, regression_dataset_names\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from JOPLEn.singletask import JOPLEn\n",
    "from JOPLEn.enums import *\n",
    "from sklearn.ensemble import (\n",
    "    ExtraTreesRegressor,\n",
    "    ExtraTreesClassifier,\n",
    "    RandomForestRegressor,\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingRegressor,\n",
    "    GradientBoostingClassifier,\n",
    "    AdaBoostRegressor,\n",
    "    AdaBoostClassifier,\n",
    ")\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from lineartree import (\n",
    "    LinearForestRegressor,\n",
    "    LinearForestClassifier,\n",
    "    LinearBoostRegressor,\n",
    "    LinearBoostClassifier,\n",
    ")\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from JOPLEn.ablation import Booster\n",
    "import lineartree as lt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.dummy import DummyRegressor, DummyClassifier\n",
    "from itertools import product\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from linear_operator.utils.warnings import NumericalWarning\n",
    "from sklearn.linear_model import Ridge, RidgeClassifier\n",
    "\n",
    "# Hide future warnings because ax uses deprecated functions from pandas\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "# Hide unfixable warning from ax (warns about default behavior but there isn't\n",
    "# a clear way to turn the warning off)\n",
    "warnings.simplefilter(action=\"ignore\", category=UserWarning)\n",
    "# Ax gives warning about non PSD matrix.\n",
    "# TODO: Should I fix this?\n",
    "warnings.simplefilter(action=\"ignore\", category=NumericalWarning)\n",
    "from ax import optimize\n",
    "from pathlib import Path\n",
    "from copy import copy, deepcopy\n",
    "import yaml\n",
    "import time\n",
    "from pprint import pprint\n",
    "from ax.service.ax_client import AxClient, ObjectiveProperties\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from ax.utils.common.logger import ROOT_STREAM_HANDLER\n",
    "from JOPLEn.partitioner import (\n",
    "    VPartition,\n",
    "    GBPartition,\n",
    "    RFPartition,\n",
    "    VarMaxForestPartition,\n",
    "    LinearForestPartition,\n",
    "    LinearBoostPartition,\n",
    "    CBPartition,\n",
    ")\n",
    "from JOPLEn.singletask import SquaredError, LogisticLoss\n",
    "from JOPLEn.enums import CellModel\n",
    "import matplotlib.pyplot as plt\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "import lightgbm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nn import NN\n",
    "from sklearn.metrics import log_loss, roc_auc_score, zero_one_loss\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier, Pool\n",
    "\n",
    "import sys\n",
    "\n",
    "fastel_path = Path().resolve().parent\n",
    "sys.path.append(str(fastel_path))\n",
    "\n",
    "from FASTEL.src.engine import MultiTaskTrees\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "ROOT_STREAM_HANDLER.setLevel(logging.ERROR)\n",
    "\n",
    "CACHE_DIR = Path(\"ax_runs\") / \"prediction\"\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DS_PATH = (Path(\"..\") / \"datasets\" / \"pmlb\" / \"processed\").resolve()\n",
    "PARAM_PATH = (Path(\".\") / \"parameters\").resolve()\n",
    "PLOT_PATH = (Path(\".\") / \"plots\").resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# too many samples, causes JOPLEn to crash\n",
    "EXCLUDE = [\n",
    "    # regression\n",
    "    \"1191_BNG_pbc\",\n",
    "    \"215_2dplanes\",\n",
    "    \"1201_BNG_breastTumor\",\n",
    "    \"1196_BNG_pharynx\",\n",
    "    \"1595_poker\",\n",
    "    \"1203_BNG_pwLinear\",\n",
    "    \"594_fri_c2_100_5\",\n",
    "    \"218_house_8L\",\n",
    "    \"1193_BNG_lowbwt\",\n",
    "    \"537_houses\",\n",
    "    \"564_fried\",\n",
    "    \"344_mv\",\n",
    "    \"574_house_16H\",\n",
    "    \"573_cpu_act\",\n",
    "    \"562_cpu_small\",\n",
    "    \"1199_BNG_echoMonths\",\n",
    "    \"294_satellite_image\",\n",
    "    \"197_cpu_act\",\n",
    "    \"201_pol\",\n",
    "    \"227_cpu_small\",\n",
    "    \"503_wind\",\n",
    "    # classification\n",
    "    # \"Hill_Valley_with_noise\",\n",
    "    # \"Hill_Valley_without_noise\",\n",
    "    # \"breast_cancer_wisconsin\",\n",
    "    # \"appendicitis\",\n",
    "    # \"prnn_synth\",\n",
    "    # \"sonar\",\n",
    "    # \"phoneme\",\n",
    "    # \"twonorm\",\n",
    "    # \"magic\",\n",
    "    # \"wdbc\",\n",
    "    \"adult\",\n",
    "    # crashing for some reason, fix later\n",
    "    \"Hill_Valley_without_noise\",\n",
    "    # crashes for joplen with catboost partitioner with early stopping\n",
    "    \"magic\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info = {\n",
    "    \"reg\": {},\n",
    "    \"class\": {},\n",
    "}\n",
    "\n",
    "for t in [\"reg\", \"class\"]:\n",
    "    for model in (PARAM_PATH / t).glob(\"*.yaml\"):\n",
    "        model_info[t][model.stem] = yaml.safe_load(open(model, \"r\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred, squared=False)\n",
    "\n",
    "\n",
    "def auc(y_true, y_pred):\n",
    "    if len(set(y_true)) == 2:\n",
    "        return float(roc_auc_score(y_true, y_pred))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def convert_to_ordinals(feature_type, x_train, x_val, x_test):\n",
    "    cat_idxs = np.array(\n",
    "        [i for i, t in enumerate(feature_type) if t in [\"categorical\", \"binary\"]]\n",
    "    )\n",
    "\n",
    "    if len(cat_idxs) > 0:\n",
    "        # lgbm doesn't like negative values for categorical values. Technically\n",
    "        # negative indicates that the value is actually quantized scalar, but\n",
    "        # PMLB doesn't distinguish these from regular categorical values.\n",
    "        x_train = x_train.copy()\n",
    "        x_val = x_val.copy()\n",
    "\n",
    "        enc = OrdinalEncoder().fit(x_train[:, cat_idxs])\n",
    "\n",
    "        x_train[:, cat_idxs] = enc.transform(x_train[:, cat_idxs])\n",
    "        x_val[:, cat_idxs] = enc.transform(x_val[:, cat_idxs])\n",
    "\n",
    "        if x_test is not None:\n",
    "            x_test = x_test.copy()\n",
    "            x_test[:, cat_idxs] = enc.transform(x_test[:, cat_idxs])\n",
    "\n",
    "    return cat_idxs, x_train, x_val, x_test\n",
    "\n",
    "\n",
    "def loss(\n",
    "    y_true: np.ndarray, y_pred: np.ndarray, loss_str: str\n",
    ") -> tuple[float, dict[str, float]]:\n",
    "    if loss_str in [\"mse\", \"rmse\", \"regression\", False, \"reg:squarederror\"]:\n",
    "        return float(rmse(y_true, y_pred)), {}\n",
    "    elif loss_str in [\"log_loss\", \"binary\", True, \"reg:logistic\"]:\n",
    "        y_class_pred = (y_pred > 0.5).astype(int)\n",
    "\n",
    "        return float(log_loss(y_true, y_pred)), {\n",
    "            \"auc\": float(roc_auc_score(y_true, y_pred)),\n",
    "            \"zo_loss\": float(zero_one_loss(y_true, y_class_pred)),\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown loss function: {loss_str}\")\n",
    "\n",
    "\n",
    "def timer_decorator(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        return (result, start_time, end_time, elapsed_time)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "@timer_decorator\n",
    "def train_lgbm(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    params = deepcopy(params)\n",
    "\n",
    "    early_stopping_rounds = params.pop(\"early_stopping_rounds\")\n",
    "    model = ModelClass(**params)\n",
    "\n",
    "    is_classifier = issubclass(ModelClass, ClassifierMixin)\n",
    "\n",
    "    early_stop = lightgbm.early_stopping(\n",
    "        stopping_rounds=early_stopping_rounds,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    cat_idxs, x_train, x_val, x_test = convert_to_ordinals(\n",
    "        feature_type, x_train, x_val, x_test\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train.flatten(),\n",
    "        eval_set=[(x_val, y_val.flatten())],\n",
    "        # verbose=-1,\n",
    "        callbacks=[early_stop],\n",
    "        categorical_feature=cat_idxs,\n",
    "    )\n",
    "\n",
    "    val_pred = (\n",
    "        model.predict_proba(x_val)[:, 1] if is_classifier else model.predict(x_val)\n",
    "    )\n",
    "\n",
    "    val_error = loss(y_val, val_pred, params[\"objective\"])\n",
    "\n",
    "    if x_test is not None and y_test is not None:\n",
    "        y_pred = (\n",
    "            model.predict_proba(x_test)[:, 1]\n",
    "            if is_classifier\n",
    "            else model.predict(x_test)\n",
    "        )\n",
    "\n",
    "        test_error = loss(y_test, y_pred, params[\"objective\"])\n",
    "\n",
    "        return (\n",
    "            val_error[0],\n",
    "            test_error[0],\n",
    "            model,\n",
    "            {\"val\": val_error[1], \"test\": test_error[1]},\n",
    "        )\n",
    "    else:\n",
    "        return val_error[0], model\n",
    "\n",
    "\n",
    "@timer_decorator\n",
    "def train_xgboost(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    params = deepcopy(params)\n",
    "    early_stopping_rounds = params.pop(\"early_stopping_rounds\")\n",
    "\n",
    "    model = ModelClass(**params)\n",
    "\n",
    "    is_classifier = issubclass(ModelClass, ClassifierMixin)\n",
    "\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        eval_set=[(x_val, y_val.flatten())],\n",
    "        early_stopping_rounds=early_stopping_rounds,\n",
    "        verbose=False,\n",
    "        eval_metric=\"logloss\" if is_classifier else \"rmse\",\n",
    "    )\n",
    "\n",
    "    val_pred = (\n",
    "        model.predict_proba(x_val)[:, 1] if is_classifier else model.predict(x_val)\n",
    "    )\n",
    "\n",
    "    val_error = loss(y_val, val_pred, params[\"objective\"])\n",
    "\n",
    "    if x_test is not None and y_test is not None:\n",
    "        y_pred = (\n",
    "            model.predict_proba(x_test)[:, 1]\n",
    "            if is_classifier\n",
    "            else model.predict(x_test)\n",
    "        )\n",
    "\n",
    "        test_error = loss(y_test, y_pred, params[\"objective\"])\n",
    "\n",
    "        return (\n",
    "            val_error[0],\n",
    "            test_error[0],\n",
    "            model,\n",
    "            {\"val\": val_error[1], \"test\": test_error[1]},\n",
    "        )\n",
    "    else:\n",
    "        return val_error[0], model\n",
    "\n",
    "\n",
    "@timer_decorator\n",
    "def train_catboost(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    feature_type = [\n",
    "        i for i, t in enumerate(feature_type) if t in [\"categorical\", \"binary\"]\n",
    "    ]\n",
    "\n",
    "    train_pool = Pool(x_train, y_train, cat_features=feature_type)\n",
    "    val_pool = Pool(x_val, y_val, cat_features=feature_type)\n",
    "\n",
    "    # https://catboost.ai/en/docs/concepts/python-reference_catboostregressor\n",
    "    model = ModelClass(**params)\n",
    "\n",
    "    # They don't subclass ClassifierMixin, so we have to check manually\n",
    "    is_classifier = ModelClass == CatBoostClassifier\n",
    "\n",
    "    model.fit(\n",
    "        train_pool,\n",
    "        eval_set=val_pool,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    val_pred = (\n",
    "        model.predict_proba(val_pool)[:, 1]\n",
    "        if is_classifier\n",
    "        else model.predict(val_pool)\n",
    "    )\n",
    "\n",
    "    loss_fn = \"log_loss\" if is_classifier else \"rmse\"\n",
    "\n",
    "    val_error = loss(y_val, val_pred, loss_fn)\n",
    "\n",
    "    if x_test is not None and y_test is not None:\n",
    "        test_pool = Pool(x_test, cat_features=feature_type)\n",
    "\n",
    "        y_pred = (\n",
    "            model.predict_proba(test_pool)[:, 1]\n",
    "            if is_classifier\n",
    "            else model.predict(test_pool)\n",
    "        )\n",
    "\n",
    "        test_error = loss(y_test, y_pred, loss_fn)\n",
    "\n",
    "        return (\n",
    "            val_error[0],\n",
    "            test_error[0],\n",
    "            model,\n",
    "            {\"val\": val_error[1], \"test\": test_error[1]},\n",
    "        )\n",
    "    else:\n",
    "        return val_error[0], model\n",
    "\n",
    "\n",
    "@timer_decorator\n",
    "def train_sklearn(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    rescale=False,\n",
    "    feature_type=[],\n",
    "):\n",
    "    if rescale:\n",
    "        model = Pipeline(\n",
    "            [(\"scaler\", StandardScaler()), (\"model\", ModelClass(**params))]\n",
    "        )\n",
    "    else:\n",
    "        model = ModelClass(**params)\n",
    "\n",
    "    model.fit(x_train, y_train.flatten())\n",
    "\n",
    "    is_classification = issubclass(ModelClass, ClassifierMixin)\n",
    "\n",
    "    if is_classification:\n",
    "        val_error = loss(y_val, model.predict_proba(x_val)[:, 1], True)\n",
    "    else:\n",
    "        val_error = loss(y_val, model.predict(x_val), False)\n",
    "\n",
    "    if x_test is not None and y_test is not None:\n",
    "        if is_classification:\n",
    "            y_pred = model.predict_proba(x_test)[:, 1]\n",
    "        else:\n",
    "            y_pred = model.predict(x_test)\n",
    "\n",
    "        test_error = loss(y_test, y_pred, is_classification)\n",
    "\n",
    "        return (\n",
    "            val_error[0],\n",
    "            test_error[0],\n",
    "            model,\n",
    "            {\"val\": val_error[1], \"test\": test_error[1]},\n",
    "        )\n",
    "    else:\n",
    "        return val_error[0], model\n",
    "\n",
    "\n",
    "def train_gbr(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    return train_sklearn(\n",
    "        ModelClass,\n",
    "        params,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_val,\n",
    "        y_val,\n",
    "        x_test,\n",
    "        y_test,\n",
    "        feature_type=[],\n",
    "    )\n",
    "\n",
    "\n",
    "def train_rfr(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    return train_sklearn(\n",
    "        ModelClass,\n",
    "        params,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_val,\n",
    "        y_val,\n",
    "        x_test,\n",
    "        y_test,\n",
    "        feature_type=[],\n",
    "    )\n",
    "\n",
    "\n",
    "def train_etr(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    return train_sklearn(\n",
    "        ModelClass,\n",
    "        params,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_val,\n",
    "        y_val,\n",
    "        x_test,\n",
    "        y_test,\n",
    "        feature_type=[],\n",
    "    )\n",
    "\n",
    "\n",
    "# TODO: compare JOPLEn to AdaBoost\n",
    "def train_abr(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    return train_sklearn(\n",
    "        ModelClass,\n",
    "        params,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_val,\n",
    "        y_val,\n",
    "        x_test,\n",
    "        y_test,\n",
    "        feature_type=[],\n",
    "    )\n",
    "\n",
    "\n",
    "def train_lf(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    return train_sklearn(\n",
    "        ModelClass,\n",
    "        {**params, \"base_estimator\": LinearRegression()},\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_val,\n",
    "        y_val,\n",
    "        x_test,\n",
    "        y_test,\n",
    "        feature_type=[],\n",
    "    )\n",
    "\n",
    "\n",
    "def train_ridge(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    return train_sklearn(\n",
    "        ModelClass,\n",
    "        params,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_val,\n",
    "        y_val,\n",
    "        x_test,\n",
    "        y_test,\n",
    "        rescale=True,\n",
    "        feature_type=[],\n",
    "    )\n",
    "\n",
    "\n",
    "@timer_decorator\n",
    "def train_pen(\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    params = deepcopy(params)\n",
    "    is_classification = eval(params.get(\"loss_fn\", \"SquaredError\")) == LogisticLoss\n",
    "\n",
    "    initial_params = {\n",
    "        \"partitioner\": eval(params.pop(\"partitioner\")),\n",
    "        \"n_cells\": params.pop(\"n_cells\"),\n",
    "        \"n_partitions\": params.pop(\"n_partitions\"),\n",
    "        \"random_state\": params.pop(\"random_state\"),\n",
    "    }\n",
    "\n",
    "    if \"cell_model\" in params:\n",
    "        initial_params[\"cell_model\"] = eval(params.pop(\"cell_model\"))\n",
    "\n",
    "    model = JOPLEn(\n",
    "        loss_fn=eval(params.pop(\"loss_fn\", \"SquaredError\")),\n",
    "        **initial_params,\n",
    "    )\n",
    "\n",
    "    if \"norm_type\" in params:\n",
    "        params[\"norm_type\"] = eval(params[\"norm_type\"])\n",
    "\n",
    "    history = model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        val_x=x_val,\n",
    "        val_y=y_val,\n",
    "        **params,\n",
    "    )\n",
    "\n",
    "    val_error = loss(y_val, model.predict(x_val), is_classification)\n",
    "\n",
    "    if x_test is not None and y_test is not None:\n",
    "        y_pred = model.predict(x_test)\n",
    "        test_error = loss(y_test, y_pred, is_classification)\n",
    "        return (\n",
    "            val_error[0],\n",
    "            test_error[0],\n",
    "            model,\n",
    "            {\n",
    "                \"n_epochs\": (\n",
    "                    len(history[\"objective\"]) if \"objective\" in history else None\n",
    "                ),\n",
    "                \"val\": val_error[1],\n",
    "                \"test\": test_error[1],\n",
    "            },\n",
    "        )\n",
    "    else:\n",
    "        return val_error[0], model\n",
    "\n",
    "\n",
    "def train_joplen(\n",
    "    _,  # ModelClass is not used\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    fn = train_cb_joplen if params[\"partitioner\"] == \"CBPartition\" else train_pen\n",
    "\n",
    "    return fn(\n",
    "        params,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_val,\n",
    "        y_val,\n",
    "        x_test,\n",
    "        y_test,\n",
    "        feature_type=[],\n",
    "    )\n",
    "\n",
    "\n",
    "@timer_decorator\n",
    "def train_cb_joplen(\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    params = deepcopy(params)\n",
    "    is_classification = eval(params.get(\"loss_fn\", \"SquaredError\")) == LogisticLoss\n",
    "\n",
    "    initial_params = {\n",
    "        \"partitioner\": eval(params.pop(\"partitioner\")),\n",
    "        \"n_cells\": 2 ** params.pop(\"max_depth\"),\n",
    "        # \"n_cells\": 2 ** params.pop(\"max_depth\"),\n",
    "        \"n_partitions\": params.pop(\"iterations\"),\n",
    "        \"random_state\": params.pop(\"random_state\"),\n",
    "    }\n",
    "\n",
    "    if \"cell_model\" in params:\n",
    "        initial_params[\"cell_model\"] = eval(params.pop(\"cell_model\"))\n",
    "\n",
    "    partitioner_keys = [\n",
    "        \"od_wait\",\n",
    "        \"learning_rate\",\n",
    "        \"l2_leaf_reg\",\n",
    "        \"od_type\",\n",
    "        \"subsample\",\n",
    "        \"grow_policy\",\n",
    "        \"allow_writing_files\",\n",
    "    ]\n",
    "    initial_params[\"part_kwargs\"] = {\n",
    "        k: params.pop(k) for k in partitioner_keys if k in params\n",
    "    }\n",
    "    initial_params[\"part_kwargs\"][\"cat_features\"] = feature_type\n",
    "\n",
    "    model = JOPLEn(\n",
    "        loss_fn=eval(params.pop(\"loss_fn\", \"SquaredError\")),\n",
    "        **initial_params,\n",
    "    )\n",
    "\n",
    "    if \"norm_type\" in params:\n",
    "        params[\"norm_type\"] = eval(params[\"norm_type\"])\n",
    "\n",
    "    history = model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        val_x=x_val,\n",
    "        val_y=y_val,\n",
    "        rescale=False,\n",
    "        **params,\n",
    "    )\n",
    "\n",
    "    val_error = loss(y_val, model.predict(x_val), is_classification)\n",
    "\n",
    "    if x_test is not None and y_test is not None:\n",
    "        y_pred = model.predict(x_test)\n",
    "        test_error = loss(y_test, y_pred, is_classification)\n",
    "        return (\n",
    "            val_error[0],\n",
    "            test_error[0],\n",
    "            model,\n",
    "            {\n",
    "                \"n_epochs\": (\n",
    "                    len(history[\"objective\"]) if \"objective\" in history else None\n",
    "                ),\n",
    "                \"val\": val_error[1],\n",
    "                \"test\": test_error[1],\n",
    "            },\n",
    "        )\n",
    "    else:\n",
    "        return val_error[0], model\n",
    "\n",
    "\n",
    "@timer_decorator\n",
    "def train_fastel(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    assert (\n",
    "        params.get(\"loss_criteria\", \"mse\") != \"log_loss\"\n",
    "    ), \"FASTEL does not support the logistic loss\"\n",
    "\n",
    "    xs = StandardScaler().fit(x_train)\n",
    "    x_train = xs.transform(x_train)\n",
    "    x_val = xs.transform(x_val)\n",
    "    x_test = xs.transform(x_test) if x_test is not None else None\n",
    "\n",
    "    ys = StandardScaler().fit(y_train.reshape(-1, 1))\n",
    "    y_train = ys.transform(y_train.reshape(-1, 1))\n",
    "    y_val = ys.transform(y_val.reshape(-1, 1))\n",
    "    y_test = ys.transform(y_test.reshape(-1, 1)) if y_test is not None else None\n",
    "\n",
    "    model = MultiTaskTrees(\n",
    "        input_shape=x_train.shape[1:],\n",
    "        **params,\n",
    "    )\n",
    "\n",
    "    model.train(\n",
    "        x_train,\n",
    "        y_train.reshape(-1, 1),\n",
    "        np.ones((y_train.shape[0], 1)),\n",
    "        x_val,\n",
    "        y_val.reshape(-1, 1),\n",
    "        np.ones((y_val.shape[0], 1)),\n",
    "    )\n",
    "\n",
    "    y_val_pred = ys.inverse_transform(model.predict(x_val)[:, None])\n",
    "    val_error = loss(y_val, y_val_pred.flatten(), params.get(\"loss_criteria\", \"mse\"))\n",
    "\n",
    "    if x_test is not None and y_test is not None:\n",
    "        y_test_pred = ys.inverse_transform(model.predict(x_test)[:, None])\n",
    "        test_error = loss(\n",
    "            y_test, y_test_pred.flatten(), params.get(\"loss_criteria\", \"mse\")\n",
    "        )\n",
    "        return (\n",
    "            val_error[0],\n",
    "            test_error[0],\n",
    "            model,\n",
    "            {\n",
    "                \"val\": val_error[1],\n",
    "                \"test\": test_error[1],\n",
    "            },\n",
    "        )\n",
    "    else:\n",
    "        return val_error[0], model\n",
    "\n",
    "\n",
    "@timer_decorator\n",
    "def train_nn(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    # TODO: should rescale the y values as well\n",
    "    xs = StandardScaler().fit(x_train)\n",
    "\n",
    "    loss_criteria = params.pop(\"loss_criteria\", \"mse\")\n",
    "\n",
    "    assert loss_criteria == \"mse\"\n",
    "\n",
    "    tmp_params = deepcopy(params)\n",
    "\n",
    "    model = NN(\n",
    "        hidden_layer_size=tmp_params.pop(\"hidden_layer_size\"),\n",
    "        n_hidden_layers=tmp_params.pop(\"n_hidden_layers\"),\n",
    "        activation=tmp_params.pop(\"activation\"),\n",
    "        sel_feat=False,\n",
    "    )\n",
    "    model.fit(\n",
    "        xs.transform(x_train),\n",
    "        y_train,\n",
    "        xs.transform(x_val),\n",
    "        y_val,\n",
    "        **tmp_params,\n",
    "    )\n",
    "\n",
    "    y_val_pred = model.predict(xs.transform(x_val))\n",
    "    val_error = loss(y_val, y_val_pred.flatten(), loss_criteria)\n",
    "\n",
    "    if x_test is not None and y_test is not None:\n",
    "        y_test_pred = model.predict(xs.transform(x_test))\n",
    "        test_error = loss(y_test, y_test_pred.flatten(), loss_criteria)\n",
    "        return (\n",
    "            val_error[0],\n",
    "            test_error[0],\n",
    "            model,\n",
    "            {\"val\": val_error[1], \"test\": test_error[1]},\n",
    "        )\n",
    "    else:\n",
    "        return val_error[0], model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_prediction(\n",
    "    x_train,\n",
    "    x_val,\n",
    "    x_test,\n",
    "    y_train,\n",
    "    y_val,\n",
    "    y_test,\n",
    "    is_classification,\n",
    "):\n",
    "    if is_classification:\n",
    "        dummy = DummyClassifier(strategy=\"most_frequent\")\n",
    "    else:\n",
    "        dummy = DummyRegressor(strategy=\"mean\")\n",
    "\n",
    "    dummy.fit(x_train, y_train)\n",
    "    y_pred = dummy.predict(x_test)\n",
    "\n",
    "    res = loss(y_test, y_pred, is_classification)\n",
    "\n",
    "    return {\n",
    "        \"model_name\": dummy.__class__.__name__,\n",
    "        \"loss\": res[0],\n",
    "        \"metadata\": res[1],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fn = {\n",
    "    LGBMRegressor.__name__: train_lgbm,\n",
    "    LGBMClassifier.__name__: train_lgbm,\n",
    "    CatBoostRegressor.__name__: train_catboost,\n",
    "    CatBoostClassifier.__name__: train_catboost,\n",
    "    XGBRegressor.__name__: train_xgboost,\n",
    "    XGBClassifier.__name__: train_xgboost,\n",
    "    GradientBoostingRegressor.__name__: train_gbr,\n",
    "    GradientBoostingClassifier.__name__: train_gbr,\n",
    "    RandomForestRegressor.__name__: train_rfr,\n",
    "    RandomForestClassifier.__name__: train_rfr,\n",
    "    ExtraTreesRegressor.__name__: train_etr,\n",
    "    ExtraTreesClassifier.__name__: train_etr,\n",
    "    JOPLEn.__name__: train_joplen,\n",
    "    LinearForestRegressor.__name__: train_lf,\n",
    "    LinearForestClassifier.__name__: train_lf,\n",
    "    Ridge.__name__: train_ridge,\n",
    "    MultiTaskTrees.__name__: train_fastel,\n",
    "    NN.__name__: train_nn,\n",
    "}\n",
    "\n",
    "\n",
    "def optimize_model(model_info, ds_path, n_trials, skip_categorical, ds_metadata):\n",
    "    ds_name = ds_path.name\n",
    "    params = model_info[\"parameters\"]\n",
    "\n",
    "    is_classification = ds_metadata[\"pmlb_metadata\"][\"target\"][\"type\"] == \"categorical\"\n",
    "\n",
    "    loss_type = \"log_loss\" if is_classification else \"rmse\"\n",
    "\n",
    "    dir_path = (\n",
    "        CACHE_DIR\n",
    "        / (\"class\" if is_classification else \"regr\")\n",
    "        / model_info[\"dir_name\"]\n",
    "        / ds_name\n",
    "    )\n",
    "    exp_path = dir_path / \"experiment.json\"\n",
    "    metadata_path = dir_path / \"metadata.yaml\"\n",
    "\n",
    "    if metadata_path.exists():\n",
    "        with open(metadata_path, \"r\") as f:\n",
    "            metadata = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "        return metadata\n",
    "\n",
    "    cont_mask = np.array([t == \"continuous\" for t in ds_metadata[\"feature_type\"]])\n",
    "    bl_categorical = np.any(~cont_mask)\n",
    "\n",
    "    if np.sum(cont_mask) == 0:\n",
    "        return None\n",
    "\n",
    "    x_train = np.loadtxt(ds_path / \"x_train.csv\", delimiter=\",\")[:, cont_mask]\n",
    "    x_val = np.loadtxt(ds_path / \"x_val.csv\", delimiter=\",\")[:, cont_mask]\n",
    "    x_test = np.loadtxt(ds_path / \"x_test.csv\", delimiter=\",\")[:, cont_mask]\n",
    "    y_train = np.loadtxt(ds_path / \"y_train.csv\", delimiter=\",\")\n",
    "    y_val = np.loadtxt(ds_path / \"y_val.csv\", delimiter=\",\")\n",
    "    y_test = np.loadtxt(ds_path / \"y_test.csv\", delimiter=\",\")\n",
    "\n",
    "    if is_classification:\n",
    "        enc = LabelEncoder()\n",
    "        y_train = enc.fit_transform(y_train)\n",
    "        y_val = enc.transform(y_val)\n",
    "        y_test = enc.transform(y_test)\n",
    "\n",
    "    dummy_info = dummy_prediction(\n",
    "        x_train,\n",
    "        x_val,\n",
    "        x_test,\n",
    "        y_train,\n",
    "        y_val,\n",
    "        y_test,\n",
    "        is_classification=is_classification,\n",
    "    )\n",
    "\n",
    "    if not exp_path.exists():\n",
    "        ax_client = AxClient(\n",
    "            random_seed=0,\n",
    "            verbose_logging=False,\n",
    "        )\n",
    "\n",
    "        ax_client.create_experiment(\n",
    "            name=f\"{model_info['model']}_{ds_name}\",\n",
    "            parameters=params,\n",
    "            objectives={loss_type: ObjectiveProperties(minimize=True)},\n",
    "            overwrite_existing_experiment=True,\n",
    "        )\n",
    "\n",
    "        for _ in trange(n_trials, leave=False, position=1):\n",
    "            round_params, trial_index = ax_client.get_next_trial()\n",
    "\n",
    "            try:\n",
    "                val_error, _ = train_fn[model_info[\"model\"]](\n",
    "                    eval(model_info[\"model\"]),\n",
    "                    round_params,\n",
    "                    x_train=x_train,\n",
    "                    y_train=y_train,\n",
    "                    x_val=x_val,\n",
    "                    y_val=y_val,\n",
    "                    feature_type=[],\n",
    "                )[0]\n",
    "                ax_client.complete_trial(\n",
    "                    trial_index=trial_index, raw_data=float(val_error)\n",
    "                )\n",
    "            except ValueError as e:\n",
    "                print(e)\n",
    "                ax_client.abandon_trial(\n",
    "                    trial_index=trial_index,\n",
    "                    reason=str(e),\n",
    "                )\n",
    "\n",
    "        exp_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        ax_client.save_to_json_file(\n",
    "            filepath=exp_path,\n",
    "        )\n",
    "    else:\n",
    "        ax_client = AxClient.load_from_json_file(filepath=exp_path)\n",
    "\n",
    "    best_parameters, values = ax_client.get_best_parameters()\n",
    "\n",
    "    (val_error, test_error, model, metadata), _, _, train_time = train_fn[\n",
    "        model_info[\"model\"]\n",
    "    ](\n",
    "        eval(model_info[\"model\"]),\n",
    "        best_parameters,\n",
    "        x_train=x_train,\n",
    "        y_train=y_train,\n",
    "        x_val=x_val,\n",
    "        y_val=y_val,\n",
    "        x_test=x_test,\n",
    "        y_test=y_test,\n",
    "        feature_type=[],\n",
    "    )\n",
    "\n",
    "    metadata = {\n",
    "        \"model_name\": model_info[\"model\"],\n",
    "        \"val_score\": float(val_error),\n",
    "        \"test_score\": float(test_error),\n",
    "        \"train_time\": float(train_time),\n",
    "        \"params\": best_parameters,\n",
    "        \"dummy_loss\": float(dummy_info[\"loss\"]),\n",
    "        \"contains_categorical\": \"postprocessed\" if bl_categorical else False,\n",
    "        \"metadata\": metadata,\n",
    "        \"dummy_metadata\": dummy_info[\"metadata\"],\n",
    "    }\n",
    "\n",
    "    with open(metadata_path, \"w\") as f:\n",
    "        yaml.dump(metadata, f)\n",
    "\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reg datasets\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f24f67d0d7404bac9279ecc8a0b8340f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running class datasets\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "512c65dc60064aaab31821f04089ee17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "531710cb91314fecb082ddbb97b92076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "Out of memory allocating 293,187,072 bytes (allocated so far: 1,256,591,360 bytes).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m model_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mds_path\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     55\u001b[0m itr\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_str\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m <50\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mds_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mds_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     reg_res[info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]][ds_path\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m res\n",
      "Cell \u001b[0;32mIn[27], line 92\u001b[0m, in \u001b[0;36moptimize_model\u001b[0;34m(model_info, ds_path, n_trials, skip_categorical, ds_metadata)\u001b[0m\n\u001b[1;32m     89\u001b[0m round_params, trial_index \u001b[38;5;241m=\u001b[39m ax_client\u001b[38;5;241m.\u001b[39mget_next_trial()\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m     val_error, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_fn\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mround_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    101\u001b[0m     ax_client\u001b[38;5;241m.\u001b[39mcomplete_trial(\n\u001b[1;32m    102\u001b[0m         trial_index\u001b[38;5;241m=\u001b[39mtrial_index, raw_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m(val_error)\n\u001b[1;32m    103\u001b[0m     )\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[25], line 507\u001b[0m, in \u001b[0;36mtrain_joplen\u001b[0;34m(_, params, x_train, y_train, x_val, y_val, x_test, y_test, feature_type)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_joplen\u001b[39m(\n\u001b[1;32m    495\u001b[0m     _,  \u001b[38;5;66;03m# ModelClass is not used\u001b[39;00m\n\u001b[1;32m    496\u001b[0m     params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    503\u001b[0m     feature_type\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m    504\u001b[0m ):\n\u001b[1;32m    505\u001b[0m     fn \u001b[38;5;241m=\u001b[39m train_cb_joplen \u001b[38;5;28;01mif\u001b[39;00m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartitioner\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCBPartition\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m train_pen\n\u001b[0;32m--> 507\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 55\u001b[0m, in \u001b[0;36mtimer_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     54\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 55\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     57\u001b[0m     elapsed_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "Cell \u001b[0;32mIn[25], line 566\u001b[0m, in \u001b[0;36mtrain_cb_joplen\u001b[0;34m(params, x_train, y_train, x_val, y_val, x_test, y_test, feature_type)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnorm_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m    564\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnorm_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnorm_type\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 566\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_x\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_y\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrescale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m val_error \u001b[38;5;241m=\u001b[39m loss(y_val, model\u001b[38;5;241m.\u001b[39mpredict(x_val), is_classification)\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x_test \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m y_test \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/code/school/joplen/code/joplen-tests/joplen/src/JOPLEn/singletask.py:527\u001b[0m, in \u001b[0;36mJOPLEn.fit\u001b[0;34m(self, x, y, lam, mu, max_iters, print_epochs, verbose, val_x, val_y, feat_thresh, alpha, gamma, norm_type, patience, stop_thresh, early_stop, rescale, mu_decr, use_nesterov)\u001b[0m\n\u001b[1;32m    525\u001b[0m best_val_w \u001b[38;5;241m=\u001b[39m w_prev\n\u001b[1;32m    526\u001b[0m best_val_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 527\u001b[0m obj_prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw_next\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m obj_prev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    529\u001b[0m     lam \u001b[38;5;241m*\u001b[39m norm(w_next)\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_partitions\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;241m+\u001b[39m alpha \u001b[38;5;241m*\u001b[39m sq_fnorm(w_next) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_partitions\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    531\u001b[0m )\n\u001b[1;32m    533\u001b[0m best_test_w \u001b[38;5;241m=\u001b[39m w_prev\n",
      "File \u001b[0;32m~/code/school/joplen/code/joplen-tests/joplen/src/JOPLEn/singletask.py:177\u001b[0m, in \u001b[0;36mLogisticLoss.__call__\u001b[0;34m(self, w, x, y, s)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28mself\u001b[39m: LogisticLoss,\n\u001b[1;32m    172\u001b[0m     w: cupy\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    175\u001b[0m     s: cupy\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[1;32m    176\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m--> 177\u001b[0m     raw_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode(y)\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(cupy\u001b[38;5;241m.\u001b[39mmean(cupy\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m cupy\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39my \u001b[38;5;241m*\u001b[39m raw_output))))\n",
      "File \u001b[0;32m~/code/school/joplen/code/joplen-tests/joplen/src/JOPLEn/singletask.py:106\u001b[0m, in \u001b[0;36mLoss._raw_output\u001b[0;34m(self, w, x, s)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raw_output\u001b[39m(\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28mself\u001b[39m: Loss,\n\u001b[1;32m    102\u001b[0m     w: cupy\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[1;32m    103\u001b[0m     x: cupy\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[1;32m    104\u001b[0m     s: cupy\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[1;32m    105\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m cupy\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cupy\u001b[38;5;241m.\u001b[39msum(\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_partitions\n",
      "File \u001b[0;32mcupy/_core/core.pyx:1281\u001b[0m, in \u001b[0;36mcupy._core.core._ndarray_base.__mul__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy/_core/_kernel.pyx:1347\u001b[0m, in \u001b[0;36mcupy._core._kernel.ufunc.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy/_core/_kernel.pyx:645\u001b[0m, in \u001b[0;36mcupy._core._kernel._get_out_args_from_optionals\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy/_core/core.pyx:2779\u001b[0m, in \u001b[0;36mcupy._core.core._ndarray_init\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy/_core/core.pyx:237\u001b[0m, in \u001b[0;36mcupy._core.core._ndarray_base._init_fast\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy/cuda/memory.pyx:740\u001b[0m, in \u001b[0;36mcupy.cuda.memory.alloc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy/cuda/memory.pyx:1426\u001b[0m, in \u001b[0;36mcupy.cuda.memory.MemoryPool.malloc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy/cuda/memory.pyx:1447\u001b[0m, in \u001b[0;36mcupy.cuda.memory.MemoryPool.malloc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy/cuda/memory.pyx:1118\u001b[0m, in \u001b[0;36mcupy.cuda.memory.SingleDeviceMemoryPool.malloc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy/cuda/memory.pyx:1139\u001b[0m, in \u001b[0;36mcupy.cuda.memory.SingleDeviceMemoryPool._malloc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy/cuda/memory.pyx:1384\u001b[0m, in \u001b[0;36mcupy.cuda.memory.SingleDeviceMemoryPool._try_malloc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy/cuda/memory.pyx:1387\u001b[0m, in \u001b[0;36mcupy.cuda.memory.SingleDeviceMemoryPool._try_malloc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: Out of memory allocating 293,187,072 bytes (allocated so far: 1,256,591,360 bytes)."
     ]
    }
   ],
   "source": [
    "ignored_models = [\n",
    "    \"adaboost\",\n",
    "    \"joplen_const_linforest_part\",\n",
    "    \"lf\",\n",
    "    \"et\",\n",
    "    \"joplen_const_rf_part\",\n",
    "    \"lgbm\",\n",
    "    \"fastel\",\n",
    "    \"joplen_const\",\n",
    "    \"nn\",\n",
    "    # \"gb\",  # normal GB\n",
    "    \"joplen_linear_gb_part\",\n",
    "    \"rf\",\n",
    "    \"joplen_const_gb_part_l1\",\n",
    "    \"joplen_linear_inf\",\n",
    "    \"ridge\",\n",
    "    # \"joplen_const_gb_part_l2\",  # fast joplen loss\n",
    "    \"joplen_linear_linforest_part\",\n",
    "    # \"xgboost\",  # gradient boosting with penalty term\n",
    "    \"joplen_const_gb_part_sl2\",\n",
    "    \"joplen_linear_rf_part\",\n",
    "    \"joplen_const_gb_part\",\n",
    "    \"joplen_linear\",\n",
    "    # \"jp_cb\",\n",
    "]\n",
    "\n",
    "reg_datasets = [d for d in (DS_PATH / \"reg\").iterdir() if d.is_dir()]\n",
    "class_datasets = [d for d in (DS_PATH / \"class\").iterdir() if d.is_dir()]\n",
    "\n",
    "metadata = {}\n",
    "\n",
    "for name in [\"reg\", \"class\"]:\n",
    "    metadata[name] = yaml.safe_load(open(DS_PATH.parent / f\"{name}_metadata.yaml\", \"r\"))\n",
    "\n",
    "\n",
    "reg_res = defaultdict(dict)\n",
    "\n",
    "for name, lst in zip([\"reg\", \"class\"], [reg_datasets, class_datasets]):\n",
    "    print(f\"Running {name} datasets\")\n",
    "\n",
    "    itr = tqdm(lst, position=0)\n",
    "\n",
    "    for ds_path in itr:\n",
    "        if ds_path.name in EXCLUDE:\n",
    "            continue\n",
    "\n",
    "        # print(ignored_models)\n",
    "        for file_name, info in model_info[name].items():\n",
    "            if file_name in ignored_models:\n",
    "                continue\n",
    "\n",
    "            # print(f\"Running {file_name} on {ds_path.name}\")\n",
    "\n",
    "            model_str = f\"{file_name} on {ds_path.name}\"\n",
    "            itr.set_description(f\"Running {model_str : <50}\")\n",
    "            res = optimize_model(\n",
    "                info,\n",
    "                ds_path,\n",
    "                50,\n",
    "                False,\n",
    "                metadata[name][ds_path.name],\n",
    "            )\n",
    "\n",
    "            if res is not None:\n",
    "                reg_res[info[\"name\"]][ds_path.name] = res\n",
    "\n",
    "reg_res = dict(reg_res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
