{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "from pmlb import classification_dataset_names, regression_dataset_names\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from JOPLEn.singletask import JOPLEn\n",
    "from JOPLEn.enums import *\n",
    "from sklearn.ensemble import (\n",
    "    ExtraTreesRegressor,\n",
    "    ExtraTreesClassifier,\n",
    "    RandomForestRegressor,\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingRegressor,\n",
    "    GradientBoostingClassifier,\n",
    "    AdaBoostRegressor,\n",
    "    AdaBoostClassifier,\n",
    ")\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from lineartree import (\n",
    "    LinearForestRegressor,\n",
    "    LinearForestClassifier,\n",
    "    LinearBoostRegressor,\n",
    "    LinearBoostClassifier,\n",
    ")\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from JOPLEn.ablation import Booster\n",
    "import lineartree as lt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.dummy import DummyRegressor, DummyClassifier\n",
    "from itertools import product\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from linear_operator.utils.warnings import NumericalWarning\n",
    "from sklearn.linear_model import Ridge, RidgeClassifier\n",
    "\n",
    "# Hide future warnings because ax uses deprecated functions from pandas\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "# Hide unfixable warning from ax (warns about default behavior but there isn't\n",
    "# a clear way to turn the warning off)\n",
    "warnings.simplefilter(action=\"ignore\", category=UserWarning)\n",
    "# Ax gives warning about non PSD matrix.\n",
    "# TODO: Should I fix this?\n",
    "warnings.simplefilter(action=\"ignore\", category=NumericalWarning)\n",
    "from ax import optimize\n",
    "from pathlib import Path\n",
    "from copy import copy, deepcopy\n",
    "import yaml\n",
    "import time\n",
    "from pprint import pprint\n",
    "from ax.service.ax_client import AxClient, ObjectiveProperties\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from ax.utils.common.logger import ROOT_STREAM_HANDLER\n",
    "from JOPLEn.partitioner import (\n",
    "    VPartition,\n",
    "    GBPartition,\n",
    "    RFPartition,\n",
    "    VarMaxForestPartition,\n",
    "    LinearForestPartition,\n",
    "    LinearBoostPartition,\n",
    ")\n",
    "from JOPLEn.singletask import SquaredError, LogisticLoss\n",
    "from JOPLEn.enums import CellModel\n",
    "import matplotlib.pyplot as plt\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "import lightgbm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nn import NN\n",
    "from sklearn.metrics import log_loss, roc_auc_score, zero_one_loss\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "import sys\n",
    "\n",
    "fastel_path = Path().resolve().parent\n",
    "sys.path.append(str(fastel_path))\n",
    "\n",
    "from FASTEL.src.engine import MultiTaskTrees\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "ROOT_STREAM_HANDLER.setLevel(logging.ERROR)\n",
    "\n",
    "CACHE_DIR = Path(\"ax_runs\") / \"prediction\"\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DS_PATH = (Path(\"..\") / \"datasets\" / \"pmlb\" / \"processed\").resolve()\n",
    "PARAM_PATH = (Path(\".\") / \"parameters\").resolve()\n",
    "PLOT_PATH = (Path(\".\") / \"plots\").resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# too many samples, causes JOPLEn to crash\n",
    "EXCLUDE = [\n",
    "    # regression\n",
    "    \"1191_BNG_pbc\",\n",
    "    \"215_2dplanes\",\n",
    "    \"1201_BNG_breastTumor\",\n",
    "    \"1196_BNG_pharynx\",\n",
    "    \"1595_poker\",\n",
    "    \"1203_BNG_pwLinear\",\n",
    "    \"594_fri_c2_100_5\",\n",
    "    \"218_house_8L\",\n",
    "    \"1193_BNG_lowbwt\",\n",
    "    \"537_houses\",\n",
    "    \"564_fried\",\n",
    "    \"344_mv\",\n",
    "    \"574_house_16H\",\n",
    "    \"573_cpu_act\",\n",
    "    \"562_cpu_small\",\n",
    "    \"1199_BNG_echoMonths\",\n",
    "    \"294_satellite_image\",\n",
    "    \"197_cpu_act\",\n",
    "    \"201_pol\",\n",
    "    \"227_cpu_small\",\n",
    "    \"503_wind\",\n",
    "    # classification\n",
    "    # \"Hill_Valley_with_noise\",\n",
    "    # \"Hill_Valley_without_noise\",\n",
    "    # \"breast_cancer_wisconsin\",\n",
    "    # \"appendicitis\",\n",
    "    # \"prnn_synth\",\n",
    "    # \"sonar\",\n",
    "    # \"phoneme\",\n",
    "    # \"twonorm\",\n",
    "    # \"magic\",\n",
    "    # \"wdbc\",\n",
    "    \"adult\",\n",
    "    # crashing for some reason, fix later\n",
    "    \"Hill_Valley_without_noise\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info = {\n",
    "    \"reg\": {},\n",
    "    \"class\": {},\n",
    "}\n",
    "\n",
    "for t in [\"reg\", \"class\"]:\n",
    "    for model in (PARAM_PATH / t).glob(\"*.yaml\"):\n",
    "        model_info[t][model.stem] = yaml.safe_load(open(model, \"r\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred, squared=False)\n",
    "\n",
    "\n",
    "def auc(y_true, y_pred):\n",
    "    if len(set(y_true)) == 2:\n",
    "        return float(roc_auc_score(y_true, y_pred))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def convert_to_ordinals(feature_type, x_train, x_val, x_test):\n",
    "    cat_idxs = np.array(\n",
    "        [i for i, t in enumerate(feature_type) if t in [\"categorical\", \"binary\"]]\n",
    "    )\n",
    "\n",
    "    if len(cat_idxs) > 0:\n",
    "        # lgbm doesn't like negative values for categorical values. Technically\n",
    "        # negative indicates that the value is actually quantized scalar, but\n",
    "        # PMLB doesn't distinguish these from regular categorical values.\n",
    "        x_train = x_train.copy()\n",
    "        x_val = x_val.copy()\n",
    "\n",
    "        enc = OrdinalEncoder().fit(x_train[:, cat_idxs])\n",
    "\n",
    "        x_train[:, cat_idxs] = enc.transform(x_train[:, cat_idxs])\n",
    "        x_val[:, cat_idxs] = enc.transform(x_val[:, cat_idxs])\n",
    "\n",
    "        if x_test is not None:\n",
    "            x_test = x_test.copy()\n",
    "            x_test[:, cat_idxs] = enc.transform(x_test[:, cat_idxs])\n",
    "\n",
    "    return cat_idxs, x_train, x_val, x_test\n",
    "\n",
    "\n",
    "def loss(\n",
    "    y_true: np.ndarray, y_pred: np.ndarray, loss_str: str\n",
    ") -> tuple[float, dict[str, float]]:\n",
    "    if loss_str in [\"mse\", \"rmse\", \"regression\", False, \"reg:squarederror\"]:\n",
    "        return float(rmse(y_true, y_pred)), {}\n",
    "    elif loss_str in [\"log_loss\", \"binary\", True, \"reg:logistic\"]:\n",
    "        y_class_pred = (y_pred > 0.5).astype(int)\n",
    "\n",
    "        return float(log_loss(y_true, y_pred)), {\n",
    "            \"auc\": float(roc_auc_score(y_true, y_pred)),\n",
    "            \"zo_loss\": float(zero_one_loss(y_true, y_class_pred)),\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown loss function: {loss_str}\")\n",
    "\n",
    "\n",
    "def timer_decorator(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        return (result, start_time, end_time, elapsed_time)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "@timer_decorator\n",
    "def train_lgbm(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    params = deepcopy(params)\n",
    "\n",
    "    early_stopping_rounds = params.pop(\"early_stopping_rounds\")\n",
    "    model = ModelClass(**params)\n",
    "\n",
    "    is_classifier = issubclass(ModelClass, ClassifierMixin)\n",
    "\n",
    "    early_stop = lightgbm.early_stopping(\n",
    "        stopping_rounds=early_stopping_rounds,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    cat_idxs, x_train, x_val, x_test = convert_to_ordinals(\n",
    "        feature_type, x_train, x_val, x_test\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train.flatten(),\n",
    "        # TODO: Need to re-enable validation set\n",
    "        eval_set=[(x_val, y_val.flatten())],\n",
    "        # verbose=-1,\n",
    "        callbacks=[early_stop],\n",
    "        categorical_feature=cat_idxs,\n",
    "    )\n",
    "\n",
    "    val_pred = (\n",
    "        model.predict_proba(x_val)[:, 1] if is_classifier else model.predict(x_val)\n",
    "    )\n",
    "\n",
    "    val_error = loss(y_val, val_pred, params[\"objective\"])\n",
    "\n",
    "    if x_test is not None and y_test is not None:\n",
    "        y_pred = (\n",
    "            model.predict_proba(x_test)[:, 1]\n",
    "            if is_classifier\n",
    "            else model.predict(x_test)\n",
    "        )\n",
    "\n",
    "        test_error = loss(y_test, y_pred, params[\"objective\"])\n",
    "\n",
    "        return (\n",
    "            val_error[0],\n",
    "            test_error[0],\n",
    "            model,\n",
    "            {\"val\": val_error[1], \"test\": test_error[1]},\n",
    "        )\n",
    "    else:\n",
    "        return val_error[0], model\n",
    "\n",
    "\n",
    "@timer_decorator\n",
    "def train_xgboost(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    params = deepcopy(params)\n",
    "    early_stopping_rounds = params.pop(\"early_stopping_rounds\")\n",
    "\n",
    "    model = ModelClass(**params)\n",
    "\n",
    "    is_classifier = issubclass(ModelClass, ClassifierMixin)\n",
    "\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train.flatten(),\n",
    "        # TODO: Need to re-enable validation set\n",
    "        eval_set=[(x_val, y_val.flatten())],\n",
    "        early_stopping_rounds=early_stopping_rounds,\n",
    "        verbose=False,\n",
    "        eval_metric=\"logloss\" if is_classifier else \"rmse\",\n",
    "    )\n",
    "\n",
    "    val_pred = (\n",
    "        model.predict_proba(x_val)[:, 1] if is_classifier else model.predict(x_val)\n",
    "    )\n",
    "\n",
    "    val_error = loss(y_val, val_pred, params[\"objective\"])\n",
    "\n",
    "    if x_test is not None and y_test is not None:\n",
    "        y_pred = (\n",
    "            model.predict_proba(x_test)[:, 1]\n",
    "            if is_classifier\n",
    "            else model.predict(x_test)\n",
    "        )\n",
    "\n",
    "        test_error = loss(y_test, y_pred, params[\"objective\"])\n",
    "\n",
    "        return (\n",
    "            val_error[0],\n",
    "            test_error[0],\n",
    "            model,\n",
    "            {\"val\": val_error[1], \"test\": test_error[1]},\n",
    "        )\n",
    "    else:\n",
    "        return val_error[0], model\n",
    "\n",
    "\n",
    "@timer_decorator\n",
    "def train_sklearn(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    rescale=False,\n",
    "    feature_type=[],\n",
    "):\n",
    "    if rescale:\n",
    "        model = Pipeline(\n",
    "            [(\"scaler\", StandardScaler()), (\"model\", ModelClass(**params))]\n",
    "        )\n",
    "    else:\n",
    "        model = ModelClass(**params)\n",
    "\n",
    "    model.fit(x_train, y_train.flatten())\n",
    "\n",
    "    is_classification = issubclass(ModelClass, ClassifierMixin)\n",
    "\n",
    "    if is_classification:\n",
    "        val_error = loss(y_val, model.predict_proba(x_val)[:, 1], True)\n",
    "    else:\n",
    "        val_error = loss(y_val, model.predict(x_val), False)\n",
    "\n",
    "    if x_test is not None and y_test is not None:\n",
    "        if is_classification:\n",
    "            y_pred = model.predict_proba(x_test)[:, 1]\n",
    "        else:\n",
    "            y_pred = model.predict(x_test)\n",
    "\n",
    "        test_error = loss(y_test, y_pred, is_classification)\n",
    "\n",
    "        return (\n",
    "            val_error[0],\n",
    "            test_error[0],\n",
    "            model,\n",
    "            {\"val\": val_error[1], \"test\": test_error[1]},\n",
    "        )\n",
    "    else:\n",
    "        return val_error[0], model\n",
    "\n",
    "\n",
    "def train_gbr(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    return train_sklearn(\n",
    "        ModelClass,\n",
    "        params,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_val,\n",
    "        y_val,\n",
    "        x_test,\n",
    "        y_test,\n",
    "        feature_type=[],\n",
    "    )\n",
    "\n",
    "\n",
    "def train_rfr(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    return train_sklearn(\n",
    "        ModelClass,\n",
    "        params,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_val,\n",
    "        y_val,\n",
    "        x_test,\n",
    "        y_test,\n",
    "        feature_type=[],\n",
    "    )\n",
    "\n",
    "\n",
    "def train_etr(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    return train_sklearn(\n",
    "        ModelClass,\n",
    "        params,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_val,\n",
    "        y_val,\n",
    "        x_test,\n",
    "        y_test,\n",
    "        feature_type=[],\n",
    "    )\n",
    "\n",
    "\n",
    "# TODO: compare JOPLEn to AdaBoost\n",
    "def train_abr(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    return train_sklearn(\n",
    "        ModelClass,\n",
    "        params,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_val,\n",
    "        y_val,\n",
    "        x_test,\n",
    "        y_test,\n",
    "        feature_type=[],\n",
    "    )\n",
    "\n",
    "\n",
    "def train_lf(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    return train_sklearn(\n",
    "        ModelClass,\n",
    "        {**params, \"base_estimator\": LinearRegression()},\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_val,\n",
    "        y_val,\n",
    "        x_test,\n",
    "        y_test,\n",
    "        feature_type=[],\n",
    "    )\n",
    "\n",
    "\n",
    "def train_ridge(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    return train_sklearn(\n",
    "        ModelClass,\n",
    "        params,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_val,\n",
    "        y_val,\n",
    "        x_test,\n",
    "        y_test,\n",
    "        rescale=True,\n",
    "        feature_type=[],\n",
    "    )\n",
    "\n",
    "\n",
    "@timer_decorator\n",
    "def train_pen(\n",
    "    ModelType,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    params = deepcopy(params)\n",
    "    is_classification = eval(params.get(\"loss_fn\", \"SquaredError\")) == LogisticLoss\n",
    "\n",
    "    initial_params = {\n",
    "        \"partitioner\": eval(params.pop(\"partitioner\")),\n",
    "        \"n_cells\": params.pop(\"n_cells\"),\n",
    "        \"n_partitions\": params.pop(\"n_partitions\"),\n",
    "        \"random_state\": params.pop(\"random_state\"),\n",
    "    }\n",
    "\n",
    "    if \"cell_model\" in params:\n",
    "        initial_params[\"cell_model\"] = eval(params.pop(\"cell_model\"))\n",
    "\n",
    "    model = ModelType(\n",
    "        loss_fn=eval(params.pop(\"loss_fn\", \"SquaredError\")),\n",
    "        **initial_params,\n",
    "    )\n",
    "\n",
    "    if \"norm_type\" in params:\n",
    "        params[\"norm_type\"] = eval(params[\"norm_type\"])\n",
    "\n",
    "    history = model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        val_x=x_val,\n",
    "        val_y=y_val,\n",
    "        **params,\n",
    "    )\n",
    "\n",
    "    val_error = loss(y_val, model.predict(x_val), is_classification)\n",
    "\n",
    "    if x_test is not None and y_test is not None:\n",
    "        y_pred = model.predict(x_test)\n",
    "        test_error = loss(y_test, y_pred, is_classification)\n",
    "        return (\n",
    "            val_error[0],\n",
    "            test_error[0],\n",
    "            model,\n",
    "            {\n",
    "                \"n_epochs\": (\n",
    "                    len(history[\"objective\"]) if \"objective\" in history else None\n",
    "                ),\n",
    "                \"val\": val_error[1],\n",
    "                \"test\": test_error[1],\n",
    "            },\n",
    "        )\n",
    "    else:\n",
    "        return val_error[0], model\n",
    "\n",
    "\n",
    "def train_joplen(\n",
    "    _,  # ModelClass is not used\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    return train_pen(\n",
    "        JOPLEn,\n",
    "        params,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_val,\n",
    "        y_val,\n",
    "        x_test,\n",
    "        y_test,\n",
    "        feature_type=[],\n",
    "    )\n",
    "\n",
    "\n",
    "@timer_decorator\n",
    "def train_fastel(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    assert (\n",
    "        params.get(\"loss_criteria\", \"mse\") != \"log_loss\"\n",
    "    ), \"FASTEL does not support the logistic loss\"\n",
    "\n",
    "    xs = StandardScaler().fit(x_train)\n",
    "    x_train = xs.transform(x_train)\n",
    "    x_val = xs.transform(x_val)\n",
    "    x_test = xs.transform(x_test) if x_test is not None else None\n",
    "\n",
    "    ys = StandardScaler().fit(y_train.reshape(-1, 1))\n",
    "    y_train = ys.transform(y_train.reshape(-1, 1))\n",
    "    y_val = ys.transform(y_val.reshape(-1, 1))\n",
    "    y_test = ys.transform(y_test.reshape(-1, 1)) if y_test is not None else None\n",
    "\n",
    "    model = MultiTaskTrees(\n",
    "        input_shape=x_train.shape[1:],\n",
    "        **params,\n",
    "    )\n",
    "\n",
    "    model.train(\n",
    "        x_train,\n",
    "        y_train.reshape(-1, 1),\n",
    "        np.ones((y_train.shape[0], 1)),\n",
    "        x_val,\n",
    "        y_val.reshape(-1, 1),\n",
    "        np.ones((y_val.shape[0], 1)),\n",
    "    )\n",
    "\n",
    "    y_val_pred = ys.inverse_transform(model.predict(x_val)[:, None])\n",
    "    val_error = loss(y_val, y_val_pred.flatten(), params.get(\"loss_criteria\", \"mse\"))\n",
    "\n",
    "    if x_test is not None and y_test is not None:\n",
    "        y_test_pred = ys.inverse_transform(model.predict(x_test)[:, None])\n",
    "        test_error = loss(\n",
    "            y_test, y_test_pred.flatten(), params.get(\"loss_criteria\", \"mse\")\n",
    "        )\n",
    "        return (\n",
    "            val_error[0],\n",
    "            test_error[0],\n",
    "            model,\n",
    "            {\n",
    "                \"val\": val_error[1],\n",
    "                \"test\": test_error[1],\n",
    "            },\n",
    "        )\n",
    "    else:\n",
    "        return val_error[0], model\n",
    "\n",
    "\n",
    "@timer_decorator\n",
    "def train_nn(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    # TODO: should rescale the y values as well\n",
    "    xs = StandardScaler().fit(x_train)\n",
    "\n",
    "    loss_criteria = params.pop(\"loss_criteria\", \"mse\")\n",
    "\n",
    "    assert loss_criteria == \"mse\"\n",
    "\n",
    "    tmp_params = deepcopy(params)\n",
    "\n",
    "    model = NN(\n",
    "        hidden_layer_size=tmp_params.pop(\"hidden_layer_size\"),\n",
    "        n_hidden_layers=tmp_params.pop(\"n_hidden_layers\"),\n",
    "        activation=tmp_params.pop(\"activation\"),\n",
    "        sel_feat=False,\n",
    "    )\n",
    "    model.fit(\n",
    "        xs.transform(x_train),\n",
    "        y_train,\n",
    "        xs.transform(x_val),\n",
    "        y_val,\n",
    "        **tmp_params,\n",
    "    )\n",
    "\n",
    "    y_val_pred = model.predict(xs.transform(x_val))\n",
    "    val_error = loss(y_val, y_val_pred.flatten(), loss_criteria)\n",
    "\n",
    "    if x_test is not None and y_test is not None:\n",
    "        y_test_pred = model.predict(xs.transform(x_test))\n",
    "        test_error = loss(y_test, y_test_pred.flatten(), loss_criteria)\n",
    "        return (\n",
    "            val_error[0],\n",
    "            test_error[0],\n",
    "            model,\n",
    "            {\"val\": val_error[1], \"test\": test_error[1]},\n",
    "        )\n",
    "    else:\n",
    "        return val_error[0], model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_prediction(\n",
    "    x_train,\n",
    "    x_val,\n",
    "    x_test,\n",
    "    y_train,\n",
    "    y_val,\n",
    "    y_test,\n",
    "    is_classification,\n",
    "):\n",
    "    if is_classification:\n",
    "        dummy = DummyClassifier(strategy=\"most_frequent\")\n",
    "    else:\n",
    "        dummy = DummyRegressor(strategy=\"mean\")\n",
    "\n",
    "    dummy.fit(x_train, y_train)\n",
    "    y_pred = dummy.predict(x_test)\n",
    "\n",
    "    res = loss(y_test, y_pred, is_classification)\n",
    "\n",
    "    return {\n",
    "        \"model_name\": dummy.__class__.__name__,\n",
    "        \"loss\": res[0],\n",
    "        \"metadata\": res[1],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def contains_categorical(arr: np.ndarray) -> bool:\n",
    "#     \"\"\"Checks if an array contains categorical, ordinal, binary, or nominal features\"\"\"\n",
    "#     return bool(np.any(np.all(np.mod(arr, 1) == 0, axis=0)))\n",
    "\n",
    "\n",
    "train_fn = {\n",
    "    LGBMRegressor.__name__: train_lgbm,\n",
    "    LGBMClassifier.__name__: train_lgbm,\n",
    "    XGBRegressor.__name__: train_xgboost,\n",
    "    XGBClassifier.__name__: train_xgboost,\n",
    "    GradientBoostingRegressor.__name__: train_gbr,\n",
    "    GradientBoostingClassifier.__name__: train_gbr,\n",
    "    RandomForestRegressor.__name__: train_rfr,\n",
    "    RandomForestClassifier.__name__: train_rfr,\n",
    "    ExtraTreesRegressor.__name__: train_etr,\n",
    "    ExtraTreesClassifier.__name__: train_etr,\n",
    "    JOPLEn.__name__: train_joplen,\n",
    "    LinearForestRegressor.__name__: train_lf,\n",
    "    LinearForestClassifier.__name__: train_lf,\n",
    "    Ridge.__name__: train_ridge,\n",
    "    MultiTaskTrees.__name__: train_fastel,\n",
    "    NN.__name__: train_nn,\n",
    "}\n",
    "\n",
    "\n",
    "def optimize_model(model_info, ds_path, n_trials, skip_categorical, ds_metadata):\n",
    "    ds_name = ds_path.name\n",
    "    params = model_info[\"parameters\"]\n",
    "\n",
    "    is_classification = ds_metadata[\"pmlb_metadata\"][\"target\"][\"type\"] == \"categorical\"\n",
    "    bl_categorical = \"categorical\" in ds_metadata[\"feature_type\"]\n",
    "\n",
    "    loss_type = \"log_loss\" if is_classification else \"rmse\"\n",
    "\n",
    "    dir_path = (\n",
    "        CACHE_DIR\n",
    "        / (\"class\" if is_classification else \"regr\")\n",
    "        / model_info[\"dir_name\"]\n",
    "        / ds_name\n",
    "    )\n",
    "    exp_path = dir_path / \"experiment.json\"\n",
    "    metadata_path = dir_path / \"metadata.yaml\"\n",
    "\n",
    "    if metadata_path.exists():\n",
    "        with open(metadata_path, \"r\") as f:\n",
    "            metadata = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "        return metadata\n",
    "\n",
    "    x_train = np.loadtxt(ds_path / \"x_train.csv\", delimiter=\",\")\n",
    "    x_val = np.loadtxt(ds_path / \"x_val.csv\", delimiter=\",\")\n",
    "    x_test = np.loadtxt(ds_path / \"x_test.csv\", delimiter=\",\")\n",
    "    y_train = np.loadtxt(ds_path / \"y_train.csv\", delimiter=\",\")\n",
    "    y_val = np.loadtxt(ds_path / \"y_val.csv\", delimiter=\",\")\n",
    "    y_test = np.loadtxt(ds_path / \"y_test.csv\", delimiter=\",\")\n",
    "\n",
    "    if is_classification:\n",
    "        enc = LabelEncoder()\n",
    "        y_train = enc.fit_transform(y_train)\n",
    "        y_val = enc.transform(y_val)\n",
    "        y_test = enc.transform(y_test)\n",
    "\n",
    "    if bl_categorical and skip_categorical:\n",
    "        return None\n",
    "\n",
    "    if bl_categorical and not model_info[\"handles_categorical\"]:\n",
    "        return None\n",
    "\n",
    "    dummy_info = dummy_prediction(\n",
    "        x_train,\n",
    "        x_val,\n",
    "        x_test,\n",
    "        y_train,\n",
    "        y_val,\n",
    "        y_test,\n",
    "        is_classification=is_classification,\n",
    "    )\n",
    "\n",
    "    if not exp_path.exists():\n",
    "        ax_client = AxClient(\n",
    "            random_seed=0,\n",
    "            verbose_logging=False,\n",
    "        )\n",
    "\n",
    "        ax_client.create_experiment(\n",
    "            name=f\"{model_info['model']}_{ds_name}\",\n",
    "            parameters=params,\n",
    "            objectives={loss_type: ObjectiveProperties(minimize=True)},\n",
    "            overwrite_existing_experiment=True,\n",
    "        )\n",
    "\n",
    "        for _ in trange(n_trials, leave=False, position=1):\n",
    "            round_params, trial_index = ax_client.get_next_trial()\n",
    "\n",
    "            try:\n",
    "                val_error, _ = train_fn[model_info[\"model\"]](\n",
    "                    eval(model_info[\"model\"]),\n",
    "                    round_params,\n",
    "                    x_train=x_train,\n",
    "                    y_train=y_train,\n",
    "                    x_val=x_val,\n",
    "                    y_val=y_val,\n",
    "                    feature_type=ds_metadata[\"feature_type\"],\n",
    "                )[0]\n",
    "                ax_client.complete_trial(\n",
    "                    trial_index=trial_index, raw_data=float(val_error)\n",
    "                )\n",
    "            except ValueError as e:\n",
    "                print(e)\n",
    "                ax_client.abandon_trial(\n",
    "                    trial_index=trial_index,\n",
    "                    reason=str(e),\n",
    "                )\n",
    "\n",
    "        exp_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        ax_client.save_to_json_file(\n",
    "            filepath=exp_path,\n",
    "        )\n",
    "    else:\n",
    "        ax_client = AxClient.load_from_json_file(filepath=exp_path)\n",
    "\n",
    "    best_parameters, values = ax_client.get_best_parameters()\n",
    "\n",
    "    (val_error, test_error, model, metadata), _, _, train_time = train_fn[\n",
    "        model_info[\"model\"]\n",
    "    ](\n",
    "        eval(model_info[\"model\"]),\n",
    "        best_parameters,\n",
    "        x_train=x_train,\n",
    "        y_train=y_train,\n",
    "        x_val=x_val,\n",
    "        y_val=y_val,\n",
    "        x_test=x_test,\n",
    "        y_test=y_test,\n",
    "        feature_type=ds_metadata[\"feature_type\"],\n",
    "    )\n",
    "\n",
    "    metadata = {\n",
    "        \"model_name\": model_info[\"model\"],\n",
    "        \"val_score\": float(val_error),\n",
    "        \"test_score\": float(test_error),\n",
    "        \"train_time\": float(train_time),\n",
    "        \"params\": best_parameters,\n",
    "        \"dummy_loss\": float(dummy_info[\"loss\"]),\n",
    "        \"contains_categorical\": bl_categorical,\n",
    "        \"metadata\": metadata,\n",
    "        \"dummy_metadata\": dummy_info[\"metadata\"],\n",
    "    }\n",
    "\n",
    "    with open(metadata_path, \"w\") as f:\n",
    "        yaml.dump(metadata, f)\n",
    "\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reg datasets\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c4b88ab43d412ea058e3332e76771c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6507f09ee0474778b74444240aacaa7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ignored_models = [\n",
    "    \"adaboost\",\n",
    "    \"joplen_const_linforest_part\",\n",
    "    \"lf\",\n",
    "    \"et\",\n",
    "    \"joplen_const_rf_part\",\n",
    "    # \"lgbm\",\n",
    "    \"fastel\",\n",
    "    \"joplen_const\",\n",
    "    \"nn\",\n",
    "    \"gb\",\n",
    "    \"joplen_linear_gb_part\",\n",
    "    \"rf\",\n",
    "    \"joplen_const_gb_part_l1\",\n",
    "    \"joplen_linear_inf\",\n",
    "    \"ridge\",\n",
    "    \"joplen_const_gb_part_l2\",\n",
    "    \"joplen_linear_linforest_part\",\n",
    "    \"xgboost\",\n",
    "    \"joplen_const_gb_part_sl2\",\n",
    "    \"joplen_linear_rf_part\",\n",
    "    \"joplen_const_gb_part\",\n",
    "    \"joplen_linear\",\n",
    "]\n",
    "\n",
    "reg_datasets = [d for d in (DS_PATH / \"reg\").iterdir() if d.is_dir()]\n",
    "class_datasets = [d for d in (DS_PATH / \"class\").iterdir() if d.is_dir()]\n",
    "\n",
    "metadata = {}\n",
    "\n",
    "for name in [\"reg\", \"class\"]:\n",
    "    metadata[name] = yaml.safe_load(open(DS_PATH.parent / f\"{name}_metadata.yaml\", \"r\"))\n",
    "\n",
    "\n",
    "reg_res = defaultdict(dict)\n",
    "\n",
    "for name, lst in zip([\"reg\", \"class\"], [reg_datasets, class_datasets]):\n",
    "    print(f\"Running {name} datasets\")\n",
    "\n",
    "    itr = tqdm(lst, position=0)\n",
    "\n",
    "    for ds_path in itr:\n",
    "        if ds_path.name in EXCLUDE:\n",
    "            continue\n",
    "\n",
    "        # print(ignored_models)\n",
    "        for file_name, info in model_info[name].items():\n",
    "            if file_name in ignored_models:\n",
    "                continue\n",
    "\n",
    "            model_str = f\"{file_name} on {ds_path.name}\"\n",
    "            itr.set_description(f\"Running {model_str : <50}\")\n",
    "            res = optimize_model(\n",
    "                info,\n",
    "                ds_path,\n",
    "                50,\n",
    "                False,\n",
    "                metadata[name][ds_path.name],\n",
    "            )\n",
    "\n",
    "            if res is not None:\n",
    "                reg_res[info[\"name\"]][ds_path.name] = res\n",
    "\n",
    "reg_res = dict(reg_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 83\u001b[0m\n\u001b[1;32m     80\u001b[0m ONLY_CLASSIFICATION \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     81\u001b[0m ONLY_CONTINUOUS \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m ax, means \u001b[38;5;241m=\u001b[39m \u001b[43mplot_strip\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreg_res\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscatter_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43malpha\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43monly_classification\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mONLY_CLASSIFICATION\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43monly_continuous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mONLY_CONTINUOUS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlim(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ONLY_CLASSIFICATION \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1.5\u001b[39m)\n\u001b[1;32m     92\u001b[0m plt\u001b[38;5;241m.\u001b[39msavefig(PLOT_PATH \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreg_strip.png\u001b[39m\u001b[38;5;124m\"\u001b[39m, dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m)\n",
      "Cell \u001b[0;32mIn[8], line 75\u001b[0m, in \u001b[0;36mplot_strip\u001b[0;34m(reg_res, jitter, random_state, plot_kwargs, scatter_kwargs, only_classification, only_continuous)\u001b[0m\n\u001b[1;32m     71\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_ylim(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mlen\u001b[39m(y_labels) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     73\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNormalized Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtight_layout\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ax, means\n",
      "File \u001b[0;32m~/code/school/joplen/code/joplen-tests/my_env311/lib/python3.11/site-packages/matplotlib/pyplot.py:2587\u001b[0m, in \u001b[0;36mtight_layout\u001b[0;34m(pad, h_pad, w_pad, rect)\u001b[0m\n\u001b[1;32m   2579\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Figure\u001b[38;5;241m.\u001b[39mtight_layout)\n\u001b[1;32m   2580\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtight_layout\u001b[39m(\n\u001b[1;32m   2581\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2585\u001b[0m     rect: \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   2586\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2587\u001b[0m     \u001b[43mgcf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtight_layout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_pad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mh_pad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_pad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mw_pad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrect\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/school/joplen/code/joplen-tests/my_env311/lib/python3.11/site-packages/matplotlib/figure.py:3540\u001b[0m, in \u001b[0;36mFigure.tight_layout\u001b[0;34m(self, pad, h_pad, w_pad, rect)\u001b[0m\n\u001b[1;32m   3538\u001b[0m previous_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_layout_engine()\n\u001b[1;32m   3539\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_layout_engine(engine)\n\u001b[0;32m-> 3540\u001b[0m \u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m previous_engine \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   3542\u001b[0m     previous_engine, (TightLayoutEngine, PlaceHolderLayoutEngine)\n\u001b[1;32m   3543\u001b[0m ):\n\u001b[1;32m   3544\u001b[0m     _api\u001b[38;5;241m.\u001b[39mwarn_external(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe figure layout has changed to tight\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/code/school/joplen/code/joplen-tests/my_env311/lib/python3.11/site-packages/matplotlib/layout_engine.py:183\u001b[0m, in \u001b[0;36mTightLayoutEngine.execute\u001b[0;34m(self, fig)\u001b[0m\n\u001b[1;32m    181\u001b[0m renderer \u001b[38;5;241m=\u001b[39m fig\u001b[38;5;241m.\u001b[39m_get_renderer()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(renderer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_draw_disabled\u001b[39m\u001b[38;5;124m\"\u001b[39m, nullcontext)():\n\u001b[0;32m--> 183\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mget_tight_layout_figure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_subplotspec_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_pad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mh_pad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_pad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw_pad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrect\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[1;32m    188\u001b[0m     fig\u001b[38;5;241m.\u001b[39msubplots_adjust(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/code/school/joplen/code/joplen-tests/my_env311/lib/python3.11/site-packages/matplotlib/_tight_layout.py:266\u001b[0m, in \u001b[0;36mget_tight_layout_figure\u001b[0;34m(fig, axes_list, subplotspec_list, renderer, pad, h_pad, w_pad, rect)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {}\n\u001b[1;32m    262\u001b[0m     span_pairs\u001b[38;5;241m.\u001b[39mappend((\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;28mslice\u001b[39m(ss\u001b[38;5;241m.\u001b[39mrowspan\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m*\u001b[39m div_row, ss\u001b[38;5;241m.\u001b[39mrowspan\u001b[38;5;241m.\u001b[39mstop \u001b[38;5;241m*\u001b[39m div_row),\n\u001b[1;32m    264\u001b[0m         \u001b[38;5;28mslice\u001b[39m(ss\u001b[38;5;241m.\u001b[39mcolspan\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m*\u001b[39m div_col, ss\u001b[38;5;241m.\u001b[39mcolspan\u001b[38;5;241m.\u001b[39mstop \u001b[38;5;241m*\u001b[39m div_col)))\n\u001b[0;32m--> 266\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[43m_auto_adjust_subplotpars\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmax_nrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ncols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mspan_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspan_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43msubplot_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubplot_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43max_bbox_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43max_bbox_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mpad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_pad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mh_pad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_pad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mw_pad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# kwargs can be none if tight_layout fails...\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rect \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;66;03m# if rect is given, the whole subplots area (including\u001b[39;00m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;66;03m# labels) will fit into the rect instead of the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;66;03m# auto_adjust_subplotpars twice, where the second run\u001b[39;00m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;66;03m# with adjusted rect parameters.\u001b[39;00m\n",
      "File \u001b[0;32m~/code/school/joplen/code/joplen-tests/my_env311/lib/python3.11/site-packages/matplotlib/_tight_layout.py:85\u001b[0m, in \u001b[0;36m_auto_adjust_subplotpars\u001b[0;34m(fig, renderer, shape, span_pairs, subplot_list, ax_bbox_list, pad, h_pad, w_pad, rect)\u001b[0m\n\u001b[1;32m     82\u001b[0m         bb \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [martist\u001b[38;5;241m.\u001b[39m_get_tightbbox_for_layout_only(ax, renderer)]\n\u001b[1;32m     84\u001b[0m tight_bbox_raw \u001b[38;5;241m=\u001b[39m Bbox\u001b[38;5;241m.\u001b[39munion(bb)\n\u001b[0;32m---> 85\u001b[0m tight_bbox \u001b[38;5;241m=\u001b[39m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransFigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform_bbox(tight_bbox_raw)\n\u001b[1;32m     87\u001b[0m hspaces[rowspan, colspan\u001b[38;5;241m.\u001b[39mstart] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ax_bbox\u001b[38;5;241m.\u001b[39mxmin \u001b[38;5;241m-\u001b[39m tight_bbox\u001b[38;5;241m.\u001b[39mxmin  \u001b[38;5;66;03m# l\u001b[39;00m\n\u001b[1;32m     88\u001b[0m hspaces[rowspan, colspan\u001b[38;5;241m.\u001b[39mstop] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tight_bbox\u001b[38;5;241m.\u001b[39mxmax \u001b[38;5;241m-\u001b[39m ax_bbox\u001b[38;5;241m.\u001b[39mxmax  \u001b[38;5;66;03m# r\u001b[39;00m\n",
      "File \u001b[0;32m~/code/school/joplen/code/joplen-tests/my_env311/lib/python3.11/site-packages/matplotlib/transforms.py:1879\u001b[0m, in \u001b[0;36mAffine2DBase.inverted\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1877\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shorthand_name:\n\u001b[1;32m   1878\u001b[0m         shorthand_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)-1\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shorthand_name\n\u001b[0;32m-> 1879\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inverted \u001b[38;5;241m=\u001b[39m Affine2D(\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmtx\u001b[49m\u001b[43m)\u001b[49m, shorthand_name\u001b[38;5;241m=\u001b[39mshorthand_name)\n\u001b[1;32m   1880\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invalid \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1881\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inverted\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36minv\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/code/school/joplen/code/joplen-tests/my_env311/lib/python3.11/site-packages/numpy/linalg/linalg.py:538\u001b[0m, in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    536\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD->D\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md->d\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    537\u001b[0m extobj \u001b[38;5;241m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n\u001b[0;32m--> 538\u001b[0m ainv \u001b[38;5;241m=\u001b[39m \u001b[43m_umath_linalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap(ainv\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[0;32m~/code/school/joplen/code/joplen-tests/my_env311/lib/python3.11/site-packages/numpy/linalg/linalg.py:89\u001b[0m, in \u001b[0;36m_raise_linalgerror_singular\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_linalgerror_singular\u001b[39m(err, flag):\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSingular matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matt/code/school/joplen/code/joplen-tests/my_env311/lib/python3.11/site-packages/matplotlib/_tight_bbox.py:67: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  fig.patch.set_bounds(x0 / w1, y0 / h1,\n",
      "/home/matt/code/school/joplen/code/joplen-tests/my_env311/lib/python3.11/site-packages/matplotlib/_tight_bbox.py:68: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  fig.bbox.width / w1, fig.bbox.height / h1)\n",
      "/home/matt/code/school/joplen/code/joplen-tests/my_env311/lib/python3.11/site-packages/matplotlib/patches.py:743: RuntimeWarning: invalid value encountered in scalar add\n",
      "  y1 = self.convert_yunits(self._y0 + self._height)\n",
      "/home/matt/code/school/joplen/code/joplen-tests/my_env311/lib/python3.11/site-packages/matplotlib/transforms.py:2039: RuntimeWarning: invalid value encountered in scalar add\n",
      "  self._mtx[1, 2] += ty\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x0 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_strip(\n",
    "    reg_res: dict,\n",
    "    jitter: float = 0.2,\n",
    "    random_state: int = 0,\n",
    "    plot_kwargs: dict = {},\n",
    "    scatter_kwargs: dict = {},\n",
    "    only_classification: bool = False,\n",
    "    only_continuous: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a strip plot based on the list of y-values.\n",
    "    \"\"\"\n",
    "    x_values = []\n",
    "    y_labels = []\n",
    "\n",
    "    def score(vv: dict, only_classification: bool):\n",
    "        # print(vv)\n",
    "        if only_classification:\n",
    "            return vv[\"metadata\"][\"test\"][\"zo_loss\"] / vv[\"dummy_metadata\"][\"zo_loss\"]\n",
    "        else:\n",
    "            return vv[\"test_score\"] / vv[\"dummy_loss\"]\n",
    "\n",
    "    rescaled_res = {\n",
    "        k: {\n",
    "            kk: score(vv, only_classification)\n",
    "            for kk, vv in v.items()\n",
    "            if kk in classification_dataset_names\n",
    "            and vv[\"contains_categorical\"] != only_continuous\n",
    "        }\n",
    "        for k, v in reg_res.items()\n",
    "    }\n",
    "\n",
    "    rescaled_res = {k: v for k, v in rescaled_res.items() if len(v) > 0}\n",
    "\n",
    "    mean_res = {k: np.mean(list(v.values())) for k, v in rescaled_res.items()}\n",
    "\n",
    "    sorted_dict = {k: rescaled_res[k] for k in sorted(mean_res, key=mean_res.get)[::-1]}\n",
    "\n",
    "    for k, v in sorted_dict.items():\n",
    "        x_values.append(v.values())\n",
    "        y_labels.append(k)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, len(rescaled_res) * 2 / 3), **plot_kwargs)\n",
    "\n",
    "    for x_idx, val in enumerate(x_values):\n",
    "        np.random.seed(random_state)\n",
    "        # Apply jitter to x-axis positions\n",
    "        y_values = [x_idx + np.random.uniform(-jitter, jitter) for _ in val]\n",
    "        ax.scatter(val, y_values, **scatter_kwargs)\n",
    "\n",
    "    x_lim = ax.get_xlim() if only_classification else (0, 1)\n",
    "\n",
    "    for x_idx, val in enumerate(x_values[:-1]):\n",
    "        ax.hlines(x_idx + 1 / 2, *x_lim, color=\"k\", alpha=0.25)\n",
    "\n",
    "    # plot vertical line for mean for each model\n",
    "    means = [np.mean(list(val)) for val in x_values]\n",
    "    for x_value, mean in zip(range(len(x_values)), means):\n",
    "        ax.vlines(mean, x_value - 1 / 2, x_value + 1 / 2, color=\"k\")\n",
    "\n",
    "    ax.set_xlim(x_lim)\n",
    "\n",
    "    print({k: v for k, v in zip(y_labels, means)})\n",
    "\n",
    "    ax.set_yticks(range(len(y_labels)))\n",
    "    ax.set_yticklabels(y_labels)\n",
    "\n",
    "    if not only_classification:\n",
    "        ax.vlines(1, -1, len(y_labels) + 1, color=\"k\", linestyle=\"--\")\n",
    "\n",
    "    ax.set_ylim(-1 / 2, len(y_labels) - 1 / 2)\n",
    "\n",
    "    plt.xlabel(\"Normalized Loss\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return ax, means\n",
    "\n",
    "\n",
    "ONLY_CLASSIFICATION = False\n",
    "ONLY_CONTINUOUS = False\n",
    "\n",
    "ax, means = plot_strip(\n",
    "    reg_res,\n",
    "    scatter_kwargs={\"alpha\": 0.5, \"s\": 20},\n",
    "    only_classification=ONLY_CLASSIFICATION,\n",
    "    only_continuous=ONLY_CONTINUOUS,\n",
    ")\n",
    "\n",
    "plt.xlim(0, 1 if ONLY_CLASSIFICATION else 1.5)\n",
    "\n",
    "plt.savefig(PLOT_PATH / \"reg_strip.png\", dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WilcoxonResult(statistic=1156.0, pvalue=7.593838960908157e-07)\n",
      "WilcoxonResult(statistic=1417.0, pvalue=4.344111235231945e-05)\n"
     ]
    }
   ],
   "source": [
    "# compute non-parametic distribution test\n",
    "from scipy.stats import mannwhitneyu\n",
    "from scipy.stats import wilcoxon\n",
    "\n",
    "joplen, gb, rf = [], [], []\n",
    "\n",
    "for k, v in reg_res[\"JOPLEn (constant, GB partitions, l2)\"].items():\n",
    "    if k in reg_res[\"Gradient Boosting\"]:\n",
    "        if (k in classification_dataset_names) != ONLY_CLASSIFICATION:\n",
    "            continue\n",
    "\n",
    "        dummy_loss = v[\"dummy_loss\"] if ONLY_CLASSIFICATION else 1\n",
    "        joplen.append(v[\"test_score\"] / dummy_loss)\n",
    "        gb.append(reg_res[\"Gradient Boosting\"][k][\"test_score\"] / dummy_loss)\n",
    "        rf.append(reg_res[\"Random forest\"][k][\"test_score\"] / dummy_loss)\n",
    "\n",
    "joplen = np.array(joplen)\n",
    "gb = np.array(gb)\n",
    "\n",
    "print(wilcoxon(joplen, gb, alternative=\"less\"))\n",
    "print(wilcoxon(gb, rf, alternative=\"less\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ours' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m plt\u001b[38;5;241m.\u001b[39mscatter(\u001b[43mours\u001b[49m, theirs)\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m], color\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mk\u001b[39m\u001b[38;5;124m\"\u001b[39m, linestyle\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJOPLEn\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ours' is not defined"
     ]
    }
   ],
   "source": [
    "plt.scatter(ours, theirs)\n",
    "plt.plot([0, 1], [0, 1], color=\"k\", linestyle=\"--\")\n",
    "plt.xlabel(\"JOPLEn\")\n",
    "plt.ylabel(\"Gradient Boosting\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rescaled_res' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m datasets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[43mrescaled_res\u001b[49m\u001b[38;5;241m.\u001b[39mvalues())[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m      3\u001b[0m ratio \u001b[38;5;241m=\u001b[39m ours \u001b[38;5;241m/\u001b[39m theirs\n\u001b[1;32m      4\u001b[0m args \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(ratio)[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rescaled_res' is not defined"
     ]
    }
   ],
   "source": [
    "datasets = list(list(rescaled_res.values())[0].keys())\n",
    "\n",
    "ratio = ours / theirs\n",
    "args = np.argsort(ratio)[::-1]\n",
    "\n",
    "plt.scatter(range(len(ratio)), ratio[args])\n",
    "plt.hlines(1, 0, len(ratio), color=\"k\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"Dataset\")\n",
    "plt.ylabel(\"JOPLEn / Gradient Boosting\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"We do much worse on these datasets\")\n",
    "print([datasets[i] for i in args[:2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [192_vineyard](https://github.com/EpistasisLab/pmlb/blob/master/datasets/192_vineyard/metadata.yaml): Small dataset, 52 samples, 2 features. May be that we just needed to use a better GB model for selecting splits\n",
    "- 645_fri_c3_500_50: Not sure what's wrong with this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = []\n",
    "\n",
    "for ds_path in reg_datasets:\n",
    "    if ds_path.name in EXCLUDE:\n",
    "        continue\n",
    "\n",
    "    x_train = np.loadtxt(ds_path / \"x_train.csv\", delimiter=\",\")\n",
    "    shapes.append(x_train.shape[1])\n",
    "\n",
    "plt.hist(shapes, bins=20)\n",
    "plt.ylabel(\"# features\")\n",
    "plt.xlabel(\"Frequency\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
