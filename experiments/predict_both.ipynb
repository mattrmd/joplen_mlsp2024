{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "from pmlb import classification_dataset_names, regression_dataset_names\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from JOPLEn.singletask import JOPLEn\n",
    "from JOPLEn.enums import *\n",
    "from sklearn.ensemble import (\n",
    "    ExtraTreesRegressor,\n",
    "    ExtraTreesClassifier,\n",
    "    RandomForestRegressor,\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingRegressor,\n",
    "    GradientBoostingClassifier,\n",
    "    AdaBoostRegressor,\n",
    "    AdaBoostClassifier,\n",
    ")\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from lineartree import (\n",
    "    LinearForestRegressor,\n",
    "    LinearForestClassifier,\n",
    "    LinearBoostRegressor,\n",
    "    LinearBoostClassifier,\n",
    ")\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from JOPLEn.ablation import Booster\n",
    "import lineartree as lt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.dummy import DummyRegressor, DummyClassifier\n",
    "from itertools import product\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from linear_operator.utils.warnings import NumericalWarning\n",
    "from sklearn.linear_model import Ridge, RidgeClassifier\n",
    "from sklearn.base import ClassifierMixin\n",
    "\n",
    "# Hide future warnings because ax uses deprecated functions from pandas\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "# Hide unfixable warning from ax (warns about default behavior but there isn't\n",
    "# a clear way to turn the warning off)\n",
    "warnings.simplefilter(action=\"ignore\", category=UserWarning)\n",
    "# Ax gives warning about non PSD matrix.\n",
    "# TODO: Should I fix this?\n",
    "warnings.simplefilter(action=\"ignore\", category=NumericalWarning)\n",
    "import logging\n",
    "import sys\n",
    "import time\n",
    "from copy import copy, deepcopy\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import lightgbm\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from ax import optimize\n",
    "from ax.service.ax_client import AxClient, ObjectiveProperties\n",
    "from ax.utils.common.logger import ROOT_STREAM_HANDLER\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor, Pool\n",
    "from JOPLEn.competing import FriedmanRefit\n",
    "from JOPLEn.enums import CellModel\n",
    "from JOPLEn.partitioner import (\n",
    "    CBPartition,\n",
    "    GBPartition,\n",
    "    LinearBoostPartition,\n",
    "    LinearForestPartition,\n",
    "    RFPartition,\n",
    "    VarMaxForestPartition,\n",
    "    VPartition,\n",
    ")\n",
    "from JOPLEn.singletask import LogisticLoss, SquaredError\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from nn import NN\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.metrics import log_loss, roc_auc_score, zero_one_loss\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "fastel_path = Path().resolve().parent\n",
    "sys.path.append(str(fastel_path))\n",
    "\n",
    "from FASTEL.src.engine import MultiTaskTrees\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "ROOT_STREAM_HANDLER.setLevel(logging.ERROR)\n",
    "\n",
    "CACHE_DIR = Path(\"ax_runs\") / \"prediction\"\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DS_PATH = (Path(\"..\") / \"datasets\" / \"pmlb\" / \"processed\").resolve()\n",
    "PARAM_PATH = (Path(\".\") / \"parameters\").resolve()\n",
    "PLOT_PATH = (Path(\".\") / \"plots\").resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# too many samples, causes JOPLEn to crash\n",
    "EXCLUDE = [\n",
    "    # regression\n",
    "    \"1191_BNG_pbc\",\n",
    "    \"215_2dplanes\",\n",
    "    \"1201_BNG_breastTumor\",\n",
    "    \"1196_BNG_pharynx\",\n",
    "    \"1595_poker\",\n",
    "    \"1203_BNG_pwLinear\",\n",
    "    \"594_fri_c2_100_5\",\n",
    "    \"218_house_8L\",\n",
    "    \"1193_BNG_lowbwt\",\n",
    "    \"537_houses\",\n",
    "    \"564_fried\",\n",
    "    \"344_mv\",\n",
    "    \"574_house_16H\",\n",
    "    \"573_cpu_act\",\n",
    "    \"562_cpu_small\",\n",
    "    \"1199_BNG_echoMonths\",\n",
    "    \"294_satellite_image\",\n",
    "    \"197_cpu_act\",\n",
    "    \"201_pol\",\n",
    "    \"227_cpu_small\",\n",
    "    \"503_wind\",\n",
    "    # classification\n",
    "    # \"Hill_Valley_with_noise\",\n",
    "    # \"Hill_Valley_without_noise\",\n",
    "    # \"breast_cancer_wisconsin\",\n",
    "    # \"appendicitis\",\n",
    "    # \"prnn_synth\",\n",
    "    # \"sonar\",\n",
    "    # \"phoneme\",\n",
    "    # \"twonorm\",\n",
    "    # \"magic\",\n",
    "    # \"wdbc\",\n",
    "    \"adult\",\n",
    "    # crashing for some reason, fix later\n",
    "    \"Hill_Valley_without_noise\",\n",
    "    # crashes for joplen with catboost partitioner with early stopping\n",
    "    \"magic\",\n",
    "    \"225_puma8NH\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info = {\n",
    "    \"reg\": {},\n",
    "    \"class\": {},\n",
    "}\n",
    "\n",
    "for t in [\"reg\", \"class\"]:\n",
    "    for model in (PARAM_PATH / t).glob(\"*.yaml\"):\n",
    "        model_info[t][model.stem] = yaml.safe_load(open(model, \"r\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred, squared=False)\n",
    "\n",
    "\n",
    "def auc(y_true, y_pred):\n",
    "    if len(set(y_true)) == 2:\n",
    "        return float(roc_auc_score(y_true, y_pred))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def convert_to_ordinals(feature_type, x_train, x_val, x_test):\n",
    "    cat_idxs = np.array(\n",
    "        [i for i, t in enumerate(feature_type) if t in [\"categorical\", \"binary\"]]\n",
    "    )\n",
    "\n",
    "    if len(cat_idxs) > 0:\n",
    "        # lgbm doesn't like negative values for categorical values. Technically\n",
    "        # negative indicates that the value is actually quantized scalar, but\n",
    "        # PMLB doesn't distinguish these from regular categorical values.\n",
    "        x_train = x_train.copy()\n",
    "        x_val = x_val.copy()\n",
    "\n",
    "        enc = OrdinalEncoder().fit(x_train[:, cat_idxs])\n",
    "\n",
    "        x_train[:, cat_idxs] = enc.transform(x_train[:, cat_idxs])\n",
    "        x_val[:, cat_idxs] = enc.transform(x_val[:, cat_idxs])\n",
    "\n",
    "        if x_test is not None:\n",
    "            x_test = x_test.copy()\n",
    "            x_test[:, cat_idxs] = enc.transform(x_test[:, cat_idxs])\n",
    "\n",
    "    return cat_idxs, x_train, x_val, x_test\n",
    "\n",
    "\n",
    "def loss(\n",
    "    y_true: np.ndarray, y_pred: np.ndarray, loss_str: str\n",
    ") -> tuple[float, dict[str, float]]:\n",
    "    if loss_str in [\"mse\", \"rmse\", \"regression\", False, \"reg:squarederror\"]:\n",
    "        return float(rmse(y_true, y_pred)), {}\n",
    "    elif loss_str in [\"log_loss\", \"binary\", True, \"reg:logistic\"]:\n",
    "        y_class_pred = (y_pred > 0.5).astype(int)\n",
    "\n",
    "        return float(log_loss(y_true, y_pred)), {\n",
    "            \"auc\": float(roc_auc_score(y_true, y_pred)),\n",
    "            \"zo_loss\": float(zero_one_loss(y_true, y_class_pred)),\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown loss function: {loss_str}\")\n",
    "\n",
    "\n",
    "def timer_decorator(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        return (result, start_time, end_time, elapsed_time)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "@timer_decorator\n",
    "def train_lgbm(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    params = deepcopy(params)\n",
    "\n",
    "    early_stopping_rounds = params.pop(\"early_stopping_rounds\")\n",
    "    model = ModelClass(**params)\n",
    "\n",
    "    is_classifier = issubclass(ModelClass, ClassifierMixin)\n",
    "\n",
    "    early_stop = lightgbm.early_stopping(\n",
    "        stopping_rounds=early_stopping_rounds,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    cat_idxs, x_train, x_val, x_test = convert_to_ordinals(\n",
    "        feature_type, x_train, x_val, x_test\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train.flatten(),\n",
    "        eval_set=[(x_val, y_val.flatten())],\n",
    "        # verbose=-1,\n",
    "        callbacks=[early_stop],\n",
    "        categorical_feature=cat_idxs,\n",
    "    )\n",
    "\n",
    "    val_pred = (\n",
    "        model.predict_proba(x_val)[:, 1] if is_classifier else model.predict(x_val)\n",
    "    )\n",
    "\n",
    "    val_error = loss(y_val, val_pred, params[\"objective\"])\n",
    "\n",
    "    if x_test is not None and y_test is not None:\n",
    "        y_pred = (\n",
    "            model.predict_proba(x_test)[:, 1]\n",
    "            if is_classifier\n",
    "            else model.predict(x_test)\n",
    "        )\n",
    "\n",
    "        test_error = loss(y_test, y_pred, params[\"objective\"])\n",
    "\n",
    "        return (\n",
    "            val_error[0],\n",
    "            test_error[0],\n",
    "            model,\n",
    "            {\"val\": val_error[1], \"test\": test_error[1]},\n",
    "        )\n",
    "    else:\n",
    "        return val_error[0], model\n",
    "\n",
    "\n",
    "@timer_decorator\n",
    "def train_xgboost(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    params = deepcopy(params)\n",
    "    early_stopping_rounds = params.pop(\"early_stopping_rounds\")\n",
    "\n",
    "    model = ModelClass(**params)\n",
    "\n",
    "    is_classifier = issubclass(ModelClass, ClassifierMixin)\n",
    "\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        eval_set=[(x_val, y_val.flatten())],\n",
    "        early_stopping_rounds=early_stopping_rounds,\n",
    "        verbose=False,\n",
    "        eval_metric=\"logloss\" if is_classifier else \"rmse\",\n",
    "    )\n",
    "\n",
    "    val_pred = (\n",
    "        model.predict_proba(x_val)[:, 1] if is_classifier else model.predict(x_val)\n",
    "    )\n",
    "\n",
    "    val_error = loss(y_val, val_pred, params[\"objective\"])\n",
    "\n",
    "    if x_test is not None and y_test is not None:\n",
    "        y_pred = (\n",
    "            model.predict_proba(x_test)[:, 1]\n",
    "            if is_classifier\n",
    "            else model.predict(x_test)\n",
    "        )\n",
    "\n",
    "        test_error = loss(y_test, y_pred, params[\"objective\"])\n",
    "\n",
    "        return (\n",
    "            val_error[0],\n",
    "            test_error[0],\n",
    "            model,\n",
    "            {\"val\": val_error[1], \"test\": test_error[1]},\n",
    "        )\n",
    "    else:\n",
    "        return val_error[0], model\n",
    "\n",
    "\n",
    "@timer_decorator\n",
    "def train_catboost(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    feature_type = [\n",
    "        i for i, t in enumerate(feature_type) if t in [\"categorical\", \"binary\"]\n",
    "    ]\n",
    "\n",
    "    train_pool = Pool(x_train, y_train, cat_features=feature_type)\n",
    "    val_pool = Pool(x_val, y_val, cat_features=feature_type)\n",
    "\n",
    "    # https://catboost.ai/en/docs/concepts/python-reference_catboostregressor\n",
    "    model = ModelClass(**params)\n",
    "\n",
    "    # They don't subclass ClassifierMixin, so we have to check manually\n",
    "    is_classifier = ModelClass == CatBoostClassifier\n",
    "\n",
    "    model.fit(\n",
    "        train_pool,\n",
    "        eval_set=val_pool,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    val_pred = (\n",
    "        model.predict_proba(val_pool)[:, 1]\n",
    "        if is_classifier\n",
    "        else model.predict(val_pool)\n",
    "    )\n",
    "\n",
    "    loss_fn = \"log_loss\" if is_classifier else \"rmse\"\n",
    "\n",
    "    val_error = loss(y_val, val_pred, loss_fn)\n",
    "\n",
    "    if x_test is not None and y_test is not None:\n",
    "        test_pool = Pool(x_test, cat_features=feature_type)\n",
    "\n",
    "        y_pred = (\n",
    "            model.predict_proba(test_pool)[:, 1]\n",
    "            if is_classifier\n",
    "            else model.predict(test_pool)\n",
    "        )\n",
    "\n",
    "        test_error = loss(y_test, y_pred, loss_fn)\n",
    "\n",
    "        return (\n",
    "            val_error[0],\n",
    "            test_error[0],\n",
    "            model,\n",
    "            {\"val\": val_error[1], \"test\": test_error[1]},\n",
    "        )\n",
    "    else:\n",
    "        return val_error[0], model\n",
    "\n",
    "\n",
    "@timer_decorator\n",
    "def train_sklearn(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    rescale=False,\n",
    "    feature_type=[],\n",
    "):\n",
    "    if rescale:\n",
    "        model = Pipeline(\n",
    "            [(\"scaler\", StandardScaler()), (\"model\", ModelClass(**params))]\n",
    "        )\n",
    "    else:\n",
    "        model = ModelClass(**params)\n",
    "\n",
    "    model.fit(x_train, y_train.flatten())\n",
    "\n",
    "    is_classification = issubclass(ModelClass, ClassifierMixin)\n",
    "\n",
    "    if is_classification:\n",
    "        val_error = loss(y_val, model.predict_proba(x_val)[:, 1], True)\n",
    "    else:\n",
    "        val_error = loss(y_val, model.predict(x_val), False)\n",
    "\n",
    "    if x_test is not None and y_test is not None:\n",
    "        if is_classification:\n",
    "            y_pred = model.predict_proba(x_test)[:, 1]\n",
    "        else:\n",
    "            y_pred = model.predict(x_test)\n",
    "\n",
    "        test_error = loss(y_test, y_pred, is_classification)\n",
    "\n",
    "        return (\n",
    "            val_error[0],\n",
    "            test_error[0],\n",
    "            model,\n",
    "            {\"val\": val_error[1], \"test\": test_error[1]},\n",
    "        )\n",
    "    else:\n",
    "        return val_error[0], model\n",
    "\n",
    "\n",
    "def train_gbr(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    return train_sklearn(\n",
    "        ModelClass,\n",
    "        params,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_val,\n",
    "        y_val,\n",
    "        x_test,\n",
    "        y_test,\n",
    "        feature_type=[],\n",
    "    )\n",
    "\n",
    "\n",
    "def train_rfr(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    return train_sklearn(\n",
    "        ModelClass,\n",
    "        params,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_val,\n",
    "        y_val,\n",
    "        x_test,\n",
    "        y_test,\n",
    "        feature_type=[],\n",
    "    )\n",
    "\n",
    "\n",
    "def train_etr(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    return train_sklearn(\n",
    "        ModelClass,\n",
    "        params,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_val,\n",
    "        y_val,\n",
    "        x_test,\n",
    "        y_test,\n",
    "        feature_type=[],\n",
    "    )\n",
    "\n",
    "\n",
    "# TODO: compare JOPLEn to AdaBoost\n",
    "def train_abr(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    return train_sklearn(\n",
    "        ModelClass,\n",
    "        params,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_val,\n",
    "        y_val,\n",
    "        x_test,\n",
    "        y_test,\n",
    "        feature_type=[],\n",
    "    )\n",
    "\n",
    "\n",
    "def train_lf(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    return train_sklearn(\n",
    "        ModelClass,\n",
    "        {**params, \"base_estimator\": LinearRegression()},\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_val,\n",
    "        y_val,\n",
    "        x_test,\n",
    "        y_test,\n",
    "        feature_type=[],\n",
    "    )\n",
    "\n",
    "\n",
    "def train_ridge(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    return train_sklearn(\n",
    "        ModelClass,\n",
    "        params,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_val,\n",
    "        y_val,\n",
    "        x_test,\n",
    "        y_test,\n",
    "        rescale=True,\n",
    "        feature_type=[],\n",
    "    )\n",
    "\n",
    "\n",
    "@timer_decorator\n",
    "def train_pen(\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    params = deepcopy(params)\n",
    "    is_classification = eval(params.get(\"loss_fn\", \"SquaredError\")) == LogisticLoss\n",
    "\n",
    "    initial_params = {\n",
    "        \"partitioner\": eval(params.pop(\"partitioner\")),\n",
    "        \"n_cells\": params.pop(\"n_cells\"),\n",
    "        \"n_partitions\": params.pop(\"n_partitions\"),\n",
    "        \"random_state\": params.pop(\"random_state\"),\n",
    "    }\n",
    "\n",
    "    if \"cell_model\" in params:\n",
    "        initial_params[\"cell_model\"] = eval(params.pop(\"cell_model\"))\n",
    "\n",
    "    model = JOPLEn(\n",
    "        loss_fn=eval(params.pop(\"loss_fn\", \"SquaredError\")),\n",
    "        **initial_params,\n",
    "    )\n",
    "\n",
    "    if \"norm_type\" in params:\n",
    "        params[\"norm_type\"] = eval(params[\"norm_type\"])\n",
    "\n",
    "    history = model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        val_x=x_val,\n",
    "        val_y=y_val,\n",
    "        **params,\n",
    "    )\n",
    "\n",
    "    val_error = loss(y_val, model.predict(x_val), is_classification)\n",
    "\n",
    "    if x_test is not None and y_test is not None:\n",
    "        y_pred = model.predict(x_test)\n",
    "        test_error = loss(y_test, y_pred, is_classification)\n",
    "        return (\n",
    "            val_error[0],\n",
    "            test_error[0],\n",
    "            model,\n",
    "            {\n",
    "                \"n_epochs\": (\n",
    "                    len(history[\"objective\"]) if \"objective\" in history else None\n",
    "                ),\n",
    "                \"val\": val_error[1],\n",
    "                \"test\": test_error[1],\n",
    "            },\n",
    "        )\n",
    "    else:\n",
    "        return val_error[0], model\n",
    "\n",
    "\n",
    "def train_joplen(\n",
    "    _,  # ModelClass is not used\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    fn = train_cb_joplen if params[\"partitioner\"] == \"CBPartition\" else train_pen\n",
    "\n",
    "    return fn(\n",
    "        params,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_val,\n",
    "        y_val,\n",
    "        x_test,\n",
    "        y_test,\n",
    "        feature_type=[],\n",
    "    )\n",
    "\n",
    "\n",
    "@timer_decorator\n",
    "def train_cb_joplen(\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    params = deepcopy(params)\n",
    "    is_classification = eval(params.get(\"loss_fn\", \"SquaredError\")) == LogisticLoss\n",
    "\n",
    "    initial_params = {\n",
    "        \"partitioner\": eval(params.pop(\"partitioner\")),\n",
    "        \"n_cells\": 2 ** params.pop(\"max_depth\"),\n",
    "        # \"n_cells\": 2 ** params.pop(\"max_depth\"),\n",
    "        \"n_partitions\": params.pop(\"iterations\"),\n",
    "        \"random_state\": params.pop(\"random_state\"),\n",
    "    }\n",
    "\n",
    "    if \"cell_model\" in params:\n",
    "        initial_params[\"cell_model\"] = eval(params.pop(\"cell_model\"))\n",
    "\n",
    "    partitioner_keys = [\n",
    "        \"od_wait\",\n",
    "        \"learning_rate\",\n",
    "        \"l2_leaf_reg\",\n",
    "        \"od_type\",\n",
    "        \"subsample\",\n",
    "        \"grow_policy\",\n",
    "        \"allow_writing_files\",\n",
    "    ]\n",
    "    initial_params[\"part_kwargs\"] = {\n",
    "        k: params.pop(k) for k in partitioner_keys if k in params\n",
    "    }\n",
    "    initial_params[\"part_kwargs\"][\"cat_features\"] = feature_type\n",
    "\n",
    "    model = JOPLEn(\n",
    "        loss_fn=eval(params.pop(\"loss_fn\", \"SquaredError\")),\n",
    "        **initial_params,\n",
    "    )\n",
    "\n",
    "    if \"norm_type\" in params:\n",
    "        params[\"norm_type\"] = eval(params[\"norm_type\"])\n",
    "\n",
    "    history = model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        val_x=x_val,\n",
    "        val_y=y_val,\n",
    "        rescale=False,\n",
    "        **params,\n",
    "    )\n",
    "\n",
    "    val_error = loss(y_val, model.predict(x_val), is_classification)\n",
    "\n",
    "    if x_test is not None and y_test is not None:\n",
    "        y_pred = model.predict(x_test)\n",
    "        test_error = loss(y_test, y_pred, is_classification)\n",
    "        return (\n",
    "            val_error[0],\n",
    "            test_error[0],\n",
    "            model,\n",
    "            {\n",
    "                \"n_epochs\": (\n",
    "                    len(history[\"objective\"]) if \"objective\" in history else None\n",
    "                ),\n",
    "                \"val\": val_error[1],\n",
    "                \"test\": test_error[1],\n",
    "            },\n",
    "        )\n",
    "    else:\n",
    "        return val_error[0], model\n",
    "\n",
    "\n",
    "@timer_decorator\n",
    "def train_friedman(\n",
    "    _,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    params = deepcopy(params)\n",
    "\n",
    "    base_model = eval(params.pop(\"base_model\"))\n",
    "    refit_model = eval(params.pop(\"refit_model\"))\n",
    "\n",
    "    is_classification = issubclass(refit_model, ClassifierMixin)\n",
    "\n",
    "    all_params = {\n",
    "        \"base_params\": {k: v for k, v in params.items() if \"base\" in k},\n",
    "        \"refit_params\": {k: v for k, v in params.items() if \"refit\" in k},\n",
    "        \"shared_params\": {k: v for k, v in params.items() if \"shared\" in k},\n",
    "    }\n",
    "\n",
    "    for k, v in all_params.items():\n",
    "        all_params[k] = {\"_\".join(k.split(\"_\")[1:]): v for k, v in v.items()}\n",
    "\n",
    "    model = FriedmanRefit(\n",
    "        base_model,\n",
    "        refit_model,\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        base_params={**all_params[\"base_params\"], **all_params[\"shared_params\"]},\n",
    "        refit_params={**all_params[\"refit_params\"], **all_params[\"shared_params\"]},\n",
    "    )\n",
    "\n",
    "    val_pred = model.predict_proba(x_val) if is_classification else model.predict(x_val)\n",
    "    val_error = loss(y_val, val_pred, is_classification)\n",
    "\n",
    "    if x_test is not None and y_test is not None:\n",
    "        test_pred = (\n",
    "            model.predict_proba(x_test) if is_classification else model.predict(x_test)\n",
    "        )\n",
    "        test_error = loss(y_test, test_pred, is_classification)\n",
    "        return (\n",
    "            val_error[0],\n",
    "            test_error[0],\n",
    "            model,\n",
    "            {\"val\": val_error[1], \"test\": test_error[1]},\n",
    "        )\n",
    "    else:\n",
    "        return val_error[0], model\n",
    "\n",
    "\n",
    "@timer_decorator\n",
    "def train_fastel(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    assert (\n",
    "        params.get(\"loss_criteria\", \"mse\") != \"log_loss\"\n",
    "    ), \"FASTEL does not support the logistic loss\"\n",
    "\n",
    "    xs = StandardScaler().fit(x_train)\n",
    "    x_train = xs.transform(x_train)\n",
    "    x_val = xs.transform(x_val)\n",
    "    x_test = xs.transform(x_test) if x_test is not None else None\n",
    "\n",
    "    ys = StandardScaler().fit(y_train.reshape(-1, 1))\n",
    "    y_train = ys.transform(y_train.reshape(-1, 1))\n",
    "    y_val = ys.transform(y_val.reshape(-1, 1))\n",
    "    y_test = ys.transform(y_test.reshape(-1, 1)) if y_test is not None else None\n",
    "\n",
    "    model = MultiTaskTrees(\n",
    "        input_shape=x_train.shape[1:],\n",
    "        **params,\n",
    "    )\n",
    "\n",
    "    model.train(\n",
    "        x_train,\n",
    "        y_train.reshape(-1, 1),\n",
    "        np.ones((y_train.shape[0], 1)),\n",
    "        x_val,\n",
    "        y_val.reshape(-1, 1),\n",
    "        np.ones((y_val.shape[0], 1)),\n",
    "    )\n",
    "\n",
    "    y_val_pred = ys.inverse_transform(model.predict(x_val)[:, None])\n",
    "    val_error = loss(y_val, y_val_pred.flatten(), params.get(\"loss_criteria\", \"mse\"))\n",
    "\n",
    "    if x_test is not None and y_test is not None:\n",
    "        y_test_pred = ys.inverse_transform(model.predict(x_test)[:, None])\n",
    "        test_error = loss(\n",
    "            y_test, y_test_pred.flatten(), params.get(\"loss_criteria\", \"mse\")\n",
    "        )\n",
    "        return (\n",
    "            val_error[0],\n",
    "            test_error[0],\n",
    "            model,\n",
    "            {\n",
    "                \"val\": val_error[1],\n",
    "                \"test\": test_error[1],\n",
    "            },\n",
    "        )\n",
    "    else:\n",
    "        return val_error[0], model\n",
    "\n",
    "\n",
    "@timer_decorator\n",
    "def train_nn(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    # TODO: should rescale the y values as well\n",
    "    xs = StandardScaler().fit(x_train)\n",
    "\n",
    "    loss_criteria = params.pop(\"loss_criteria\", \"mse\")\n",
    "\n",
    "    assert loss_criteria == \"mse\"\n",
    "\n",
    "    tmp_params = deepcopy(params)\n",
    "\n",
    "    model = NN(\n",
    "        hidden_layer_size=tmp_params.pop(\"hidden_layer_size\"),\n",
    "        n_hidden_layers=tmp_params.pop(\"n_hidden_layers\"),\n",
    "        activation=tmp_params.pop(\"activation\"),\n",
    "        sel_feat=False,\n",
    "    )\n",
    "    model.fit(\n",
    "        xs.transform(x_train),\n",
    "        y_train,\n",
    "        xs.transform(x_val),\n",
    "        y_val,\n",
    "        **tmp_params,\n",
    "    )\n",
    "\n",
    "    y_val_pred = model.predict(xs.transform(x_val))\n",
    "    val_error = loss(y_val, y_val_pred.flatten(), loss_criteria)\n",
    "\n",
    "    if x_test is not None and y_test is not None:\n",
    "        y_test_pred = model.predict(xs.transform(x_test))\n",
    "        test_error = loss(y_test, y_test_pred.flatten(), loss_criteria)\n",
    "        return (\n",
    "            val_error[0],\n",
    "            test_error[0],\n",
    "            model,\n",
    "            {\"val\": val_error[1], \"test\": test_error[1]},\n",
    "        )\n",
    "    else:\n",
    "        return val_error[0], model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_prediction(\n",
    "    x_train,\n",
    "    x_val,\n",
    "    x_test,\n",
    "    y_train,\n",
    "    y_val,\n",
    "    y_test,\n",
    "    is_classification,\n",
    "):\n",
    "    if is_classification:\n",
    "        dummy = DummyClassifier(strategy=\"most_frequent\")\n",
    "    else:\n",
    "        dummy = DummyRegressor(strategy=\"mean\")\n",
    "\n",
    "    dummy.fit(x_train, y_train)\n",
    "    y_pred = dummy.predict(x_test)\n",
    "\n",
    "    res = loss(y_test, y_pred, is_classification)\n",
    "\n",
    "    return {\n",
    "        \"model_name\": dummy.__class__.__name__,\n",
    "        \"loss\": res[0],\n",
    "        \"metadata\": res[1],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fn = {\n",
    "    FriedmanRefit.__name__: train_friedman,\n",
    "    LGBMRegressor.__name__: train_lgbm,\n",
    "    LGBMClassifier.__name__: train_lgbm,\n",
    "    CatBoostRegressor.__name__: train_catboost,\n",
    "    CatBoostClassifier.__name__: train_catboost,\n",
    "    XGBRegressor.__name__: train_xgboost,\n",
    "    XGBClassifier.__name__: train_xgboost,\n",
    "    GradientBoostingRegressor.__name__: train_gbr,\n",
    "    GradientBoostingClassifier.__name__: train_gbr,\n",
    "    RandomForestRegressor.__name__: train_rfr,\n",
    "    RandomForestClassifier.__name__: train_rfr,\n",
    "    ExtraTreesRegressor.__name__: train_etr,\n",
    "    ExtraTreesClassifier.__name__: train_etr,\n",
    "    JOPLEn.__name__: train_joplen,\n",
    "    LinearForestRegressor.__name__: train_lf,\n",
    "    LinearForestClassifier.__name__: train_lf,\n",
    "    Ridge.__name__: train_ridge,\n",
    "    MultiTaskTrees.__name__: train_fastel,\n",
    "    NN.__name__: train_nn,\n",
    "}\n",
    "\n",
    "\n",
    "def optimize_model(model_info, ds_path, n_trials, skip_categorical, ds_metadata):\n",
    "    ds_name = ds_path.name\n",
    "    params = model_info[\"parameters\"]\n",
    "\n",
    "    is_classification = ds_metadata[\"pmlb_metadata\"][\"target\"][\"type\"] == \"categorical\"\n",
    "\n",
    "    loss_type = \"log_loss\" if is_classification else \"rmse\"\n",
    "\n",
    "    dir_path = (\n",
    "        CACHE_DIR\n",
    "        / (\"class\" if is_classification else \"regr\")\n",
    "        / model_info[\"dir_name\"]\n",
    "        / ds_name\n",
    "    )\n",
    "    exp_path = dir_path / \"experiment.json\"\n",
    "    metadata_path = dir_path / \"metadata.yaml\"\n",
    "\n",
    "    if metadata_path.exists():\n",
    "        with open(metadata_path, \"r\") as f:\n",
    "            metadata = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "        return metadata\n",
    "\n",
    "    cont_mask = np.array([t == \"continuous\" for t in ds_metadata[\"feature_type\"]])\n",
    "    bl_categorical = np.any(~cont_mask)\n",
    "\n",
    "    if np.sum(cont_mask) == 0:\n",
    "        return None\n",
    "\n",
    "    x_train = np.loadtxt(ds_path / \"x_train.csv\", delimiter=\",\")[:, cont_mask]\n",
    "    x_val = np.loadtxt(ds_path / \"x_val.csv\", delimiter=\",\")[:, cont_mask]\n",
    "    x_test = np.loadtxt(ds_path / \"x_test.csv\", delimiter=\",\")[:, cont_mask]\n",
    "    y_train = np.loadtxt(ds_path / \"y_train.csv\", delimiter=\",\")\n",
    "    y_val = np.loadtxt(ds_path / \"y_val.csv\", delimiter=\",\")\n",
    "    y_test = np.loadtxt(ds_path / \"y_test.csv\", delimiter=\",\")\n",
    "\n",
    "    if is_classification:\n",
    "        enc = LabelEncoder()\n",
    "        y_train = enc.fit_transform(y_train)\n",
    "        y_val = enc.transform(y_val)\n",
    "        y_test = enc.transform(y_test)\n",
    "\n",
    "    dummy_info = dummy_prediction(\n",
    "        x_train,\n",
    "        x_val,\n",
    "        x_test,\n",
    "        y_train,\n",
    "        y_val,\n",
    "        y_test,\n",
    "        is_classification=is_classification,\n",
    "    )\n",
    "\n",
    "    if not exp_path.exists():\n",
    "        ax_client = AxClient(\n",
    "            random_seed=0,\n",
    "            verbose_logging=False,\n",
    "        )\n",
    "\n",
    "        ax_client.create_experiment(\n",
    "            name=f\"{model_info['model']}_{ds_name}\",\n",
    "            parameters=params,\n",
    "            objectives={loss_type: ObjectiveProperties(minimize=True)},\n",
    "            overwrite_existing_experiment=True,\n",
    "        )\n",
    "\n",
    "        for _ in trange(n_trials, leave=False, position=1):\n",
    "            round_params, trial_index = ax_client.get_next_trial()\n",
    "\n",
    "            try:\n",
    "                val_error, _ = train_fn[model_info[\"model\"]](\n",
    "                    eval(model_info[\"model\"]),\n",
    "                    round_params,\n",
    "                    x_train=x_train,\n",
    "                    y_train=y_train,\n",
    "                    x_val=x_val,\n",
    "                    y_val=y_val,\n",
    "                    feature_type=[],\n",
    "                )[0]\n",
    "                ax_client.complete_trial(\n",
    "                    trial_index=trial_index, raw_data=float(val_error)\n",
    "                )\n",
    "            except ValueError as e:\n",
    "                print(e)\n",
    "                ax_client.abandon_trial(\n",
    "                    trial_index=trial_index,\n",
    "                    reason=str(e),\n",
    "                )\n",
    "\n",
    "        exp_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        ax_client.save_to_json_file(\n",
    "            filepath=exp_path,\n",
    "        )\n",
    "    else:\n",
    "        ax_client = AxClient.load_from_json_file(filepath=exp_path)\n",
    "\n",
    "    best_parameters, values = ax_client.get_best_parameters()\n",
    "\n",
    "    (val_error, test_error, model, metadata), _, _, train_time = train_fn[\n",
    "        model_info[\"model\"]\n",
    "    ](\n",
    "        eval(model_info[\"model\"]),\n",
    "        best_parameters,\n",
    "        x_train=x_train,\n",
    "        y_train=y_train,\n",
    "        x_val=x_val,\n",
    "        y_val=y_val,\n",
    "        x_test=x_test,\n",
    "        y_test=y_test,\n",
    "        feature_type=[],\n",
    "    )\n",
    "\n",
    "    metadata = {\n",
    "        \"model_name\": model_info[\"model\"],\n",
    "        \"val_score\": float(val_error),\n",
    "        \"test_score\": float(test_error),\n",
    "        \"train_time\": float(train_time),\n",
    "        \"params\": best_parameters,\n",
    "        \"dummy_loss\": float(dummy_info[\"loss\"]),\n",
    "        \"contains_categorical\": \"postprocessed\" if bl_categorical else False,\n",
    "        \"metadata\": metadata,\n",
    "        \"dummy_metadata\": dummy_info[\"metadata\"],\n",
    "    }\n",
    "\n",
    "    with open(metadata_path, \"w\") as f:\n",
    "        yaml.dump(metadata, f)\n",
    "\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reg datasets\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d4a265c056b4d8cb45c3560500ab452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running class datasets\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "663b43f2266545e5ae4a8765b3cac8e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb6a923a70994738a0cc84031d41fc4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b064ecc5f0f64d669a2a3499c99c1552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "474bd1af03c04cadaf6022ccf63a93f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79616a58a5bf4af5b370f4d6b2adf82d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38b0e489f11842f09c45e18a92b57380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aac1842ed6a40b7bd6e5b1c74515832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37024b883e21473cb34991134d81cb2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ccabfe7e14e4910adc5bea76cf3d0d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1741482375b84174a2b506160f51ceea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f6f28089f9f4cef8491bcce8af490c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86350c2d650f410d8da420d0a8d4de08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce4a81bb8f37443cb92eb1b84660dd39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7613187567c4dedb836594b02e0bf77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "752dee3e1723490c82c31917e06e72ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3e84f28c3d4442da45d1f79c2747168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8dd06aa22dd483581e5fa39717e2d86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e81cf05a43141ee9222ed55931de282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92be087b0fa94da0a4ac4933cadbe748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e676da9d5a4ae782f6cff5a51e3ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de6a74cef87b4f8bb6ac0d5944a3de89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bca7688751d24e0b988aef2d2766ff0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7a88a0055b545ff8df9cef50a520d13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51893d5dcc0b4d7b9a282ac3bcd06484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deb3c66a666246829b9e4ce0fc795acc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41a4e7b92ebd4b27a20d0899be9e7c3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94eb23ab11c2404980d981fbc0614302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "408d260c45034e0a917fd38abdddbb26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faba3153e030489ca590985c6d496997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dac0aefb507b4336a6ef82abc61c9e96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aba7ce862c7477b8d5f32c1e0aa5d34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f551c714cbe743aeb6f38f127d522e6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b91c4a9c344c4cada31e2cfff74ec644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "934b40c9eeb641bda03ee7dd9a9abe29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4af1fce99b3492f8e61340ddb56955f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3053e25345f84b5ca1d7bc7665a57e9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad900fb5ccd84cffba4f9e77f4503d93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca7a8637f64e4b3d963bec26ba415880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4230e6ac297e423b8bd0b65ce28fc66b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3126bbe8a2245ad892a2451c57924c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab672dc0e6b40eca15c96d50d486f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "640bab1f83974ecfbe642068a274f30c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c5196996f0d47fab4e30e77e06d9c72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab3cb72e2f914583a53d194884b7e372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a2ddfb86db4a74b34ad1231a1cf23a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57eec603703a424fb25cc69681ddfc24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77f11b879371461e931b49160441cc0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "620b03d4dc75412abdf96462e28caadb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c2c8050b0f3471e99903544d53e08aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb53c8eea2ec4ee4bf72e3ea5167ac55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f210ba954a6426d85aef22b67c708b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16f093d712b5447fae7d1ebfa4bd5672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21abd11d4b8e4c4d8124e1d770c3bd3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6a3104dfe0641cf9f5e7aec02a84cee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e80605374d99438fa84219d97db1dfb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ee9b06c81d4c13aaa068b312fa65a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2ee23c548014ffa8314c850cb2bfbde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cf839caab8d4d49b4d1e37dedf65eb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60660ec7a1c64aca92f322e352a90b60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ignored_models = [\n",
    "    \"adaboost\",\n",
    "    \"joplen_const_linforest_part\",\n",
    "    \"lf\",\n",
    "    \"et\",\n",
    "    # \"joplen_const_rf_part\",\n",
    "    \"lgbm\",\n",
    "    \"fastel\",\n",
    "    \"joplen_const\",\n",
    "    \"nn\",\n",
    "    # \"gb\",  # normal GB\n",
    "    \"joplen_linear_gb_part\",\n",
    "    # \"rf\",\n",
    "    \"joplen_const_gb_part_l1\",\n",
    "    \"joplen_linear_inf\",\n",
    "    \"ridge\",\n",
    "    # \"joplen_const_gb_part_l2\",  # fast joplen loss\n",
    "    \"joplen_linear_linforest_part\",\n",
    "    # \"xgboost\",  # gradient boosting with penalty term\n",
    "    \"joplen_const_gb_part_sl2\",\n",
    "    \"joplen_linear_rf_part\",\n",
    "    \"joplen_const_gb_part\",\n",
    "    \"joplen_linear\",\n",
    "    # \"jp_cb\",\n",
    "]\n",
    "\n",
    "reg_datasets = [d for d in (DS_PATH / \"reg\").iterdir() if d.is_dir()]\n",
    "class_datasets = [d for d in (DS_PATH / \"class\").iterdir() if d.is_dir()]\n",
    "\n",
    "metadata = {}\n",
    "\n",
    "for name in [\"reg\", \"class\"]:\n",
    "    metadata[name] = yaml.safe_load(open(DS_PATH.parent / f\"{name}_metadata.yaml\", \"r\"))\n",
    "\n",
    "\n",
    "reg_res = defaultdict(dict)\n",
    "\n",
    "for name, lst in zip([\"reg\", \"class\"], [reg_datasets, class_datasets]):\n",
    "    print(f\"Running {name} datasets\")\n",
    "\n",
    "    itr = tqdm(lst, position=0)\n",
    "\n",
    "    for ds_path in itr:\n",
    "        if ds_path.name in EXCLUDE:\n",
    "            continue\n",
    "\n",
    "        # print(ignored_models)\n",
    "        for file_name, info in model_info[name].items():\n",
    "            if file_name in ignored_models:\n",
    "                continue\n",
    "\n",
    "            # print(f\"Running {file_name} on {ds_path.name}\")\n",
    "\n",
    "            model_str = f\"{file_name} on {ds_path.name}\"\n",
    "            itr.set_description(f\"Running {model_str : <50}\")\n",
    "            res = optimize_model(\n",
    "                info,\n",
    "                ds_path,\n",
    "                50,\n",
    "                False,\n",
    "                metadata[name][ds_path.name],\n",
    "            )\n",
    "\n",
    "            if res is not None:\n",
    "                reg_res[info[\"name\"]][ds_path.name] = res\n",
    "\n",
    "reg_res = dict(reg_res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
