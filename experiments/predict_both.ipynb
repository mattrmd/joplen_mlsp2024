{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "\n",
    "from pmlb import classification_dataset_names, regression_dataset_names\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from JOPLEn.singletask import JOPLEn\n",
    "from JOPLEn.enums import *\n",
    "from sklearn.ensemble import (\n",
    "    ExtraTreesRegressor,\n",
    "    ExtraTreesClassifier,\n",
    "    RandomForestRegressor,\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingRegressor,\n",
    "    GradientBoostingClassifier,\n",
    "    AdaBoostRegressor,\n",
    "    AdaBoostClassifier,\n",
    ")\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from lineartree import (\n",
    "    LinearForestRegressor,\n",
    "    LinearForestClassifier,\n",
    "    LinearBoostRegressor,\n",
    "    LinearBoostClassifier,\n",
    ")\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from JOPLEn.ablation import Booster\n",
    "import lineartree as lt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.dummy import DummyRegressor, DummyClassifier\n",
    "from itertools import product\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from linear_operator.utils.warnings import NumericalWarning\n",
    "from sklearn.linear_model import Ridge, RidgeClassifier\n",
    "from sklearn.base import ClassifierMixin\n",
    "\n",
    "# Hide future warnings because ax uses deprecated functions from pandas\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "# Hide unfixable warning from ax (warns about default behavior but there isn't\n",
    "# a clear way to turn the warning off)\n",
    "warnings.simplefilter(action=\"ignore\", category=UserWarning)\n",
    "# Ax gives warning about non PSD matrix.\n",
    "# TODO: Should I fix this?\n",
    "warnings.simplefilter(action=\"ignore\", category=NumericalWarning)\n",
    "import logging\n",
    "import sys\n",
    "import time\n",
    "from copy import copy, deepcopy\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import lightgbm\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from ax import optimize\n",
    "from ax.service.ax_client import AxClient, ObjectiveProperties\n",
    "from ax.utils.common.logger import ROOT_STREAM_HANDLER\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor, Pool\n",
    "from JOPLEn.competing import FriedmanRefit\n",
    "from JOPLEn.enums import CellModel\n",
    "from JOPLEn.partitioner import (\n",
    "    CBPartition,\n",
    "    GBPartition,\n",
    "    LinearBoostPartition,\n",
    "    LinearForestPartition,\n",
    "    RFPartition,\n",
    "    VarMaxForestPartition,\n",
    "    VPartition,\n",
    ")\n",
    "from JOPLEn.singletask import LogisticLoss, SquaredError\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from nn import NN\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.metrics import log_loss, roc_auc_score, zero_one_loss\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "fastel_path = Path().resolve().parent\n",
    "sys.path.append(str(fastel_path))\n",
    "\n",
    "from FASTEL.src.engine import MultiTaskTrees\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "ROOT_STREAM_HANDLER.setLevel(logging.ERROR)\n",
    "\n",
    "CACHE_DIR = Path(\"ax_runs\") / \"prediction\"\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DS_PATH = (Path(\"..\") / \"datasets\" / \"pmlb\" / \"processed\").resolve()\n",
    "PARAM_PATH = (Path(\".\") / \"parameters\").resolve()\n",
    "PLOT_PATH = (Path(\".\") / \"plots\").resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# too many samples, causes JOPLEn to crash\n",
    "EXCLUDE = [\n",
    "    # regression\n",
    "    \"1191_BNG_pbc\",\n",
    "    \"215_2dplanes\",\n",
    "    \"1201_BNG_breastTumor\",\n",
    "    \"1196_BNG_pharynx\",\n",
    "    \"1595_poker\",\n",
    "    \"1203_BNG_pwLinear\",\n",
    "    \"594_fri_c2_100_5\",\n",
    "    \"218_house_8L\",\n",
    "    \"1193_BNG_lowbwt\",\n",
    "    \"537_houses\",\n",
    "    \"564_fried\",\n",
    "    \"344_mv\",\n",
    "    \"574_house_16H\",\n",
    "    \"573_cpu_act\",\n",
    "    \"562_cpu_small\",\n",
    "    \"1199_BNG_echoMonths\",\n",
    "    \"294_satellite_image\",\n",
    "    \"197_cpu_act\",\n",
    "    \"201_pol\",\n",
    "    \"227_cpu_small\",\n",
    "    \"503_wind\",\n",
    "    # classification\n",
    "    # \"Hill_Valley_with_noise\",\n",
    "    # \"Hill_Valley_without_noise\",\n",
    "    # \"breast_cancer_wisconsin\",\n",
    "    # \"appendicitis\",\n",
    "    # \"prnn_synth\",\n",
    "    # \"sonar\",\n",
    "    # \"phoneme\",\n",
    "    # \"twonorm\",\n",
    "    # \"magic\",\n",
    "    # \"wdbc\",\n",
    "    \"adult\",\n",
    "    # crashing for some reason, fix later\n",
    "    \"Hill_Valley_without_noise\",\n",
    "    # crashes for joplen with catboost partitioner with early stopping\n",
    "    \"magic\",\n",
    "    \"225_puma8NH\",\n",
    "    # Crashes for joplen with l1 regularization\n",
    "    \"624_fri_c0_100_5\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info = {\n",
    "    \"reg\": {},\n",
    "    \"class\": {},\n",
    "}\n",
    "\n",
    "for t in [\"reg\", \"class\"]:\n",
    "    for model in (PARAM_PATH / t).glob(\"*.yaml\"):\n",
    "        model_info[t][model.stem] = yaml.safe_load(open(model, \"r\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred, squared=False)\n",
    "\n",
    "\n",
    "def auc(y_true, y_pred):\n",
    "    if len(set(y_true)) == 2:\n",
    "        return float(roc_auc_score(y_true, y_pred))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def convert_to_ordinals(feature_type, x_train, x_val, x_test):\n",
    "    cat_idxs = np.array(\n",
    "        [i for i, t in enumerate(feature_type) if t in [\"categorical\", \"binary\"]]\n",
    "    )\n",
    "\n",
    "    if len(cat_idxs) > 0:\n",
    "        # lgbm doesn't like negative values for categorical values. Technically\n",
    "        # negative indicates that the value is actually quantized scalar, but\n",
    "        # PMLB doesn't distinguish these from regular categorical values.\n",
    "        x_train = x_train.copy()\n",
    "        x_val = x_val.copy()\n",
    "\n",
    "        enc = OrdinalEncoder().fit(x_train[:, cat_idxs])\n",
    "\n",
    "        x_train[:, cat_idxs] = enc.transform(x_train[:, cat_idxs])\n",
    "        x_val[:, cat_idxs] = enc.transform(x_val[:, cat_idxs])\n",
    "\n",
    "        if x_test is not None:\n",
    "            x_test = x_test.copy()\n",
    "            x_test[:, cat_idxs] = enc.transform(x_test[:, cat_idxs])\n",
    "\n",
    "    return cat_idxs, x_train, x_val, x_test\n",
    "\n",
    "\n",
    "def loss(\n",
    "    y_true: np.ndarray, y_pred: np.ndarray, loss_str: str\n",
    ") -> tuple[float, dict[str, float]]:\n",
    "    if loss_str in [\"mse\", \"rmse\", \"regression\", False, \"reg:squarederror\"]:\n",
    "        return float(rmse(y_true, y_pred)), {}\n",
    "    elif loss_str in [\"log_loss\", \"binary\", True, \"reg:logistic\"]:\n",
    "        y_class_pred = (y_pred > 0.5).astype(int)\n",
    "\n",
    "        return float(log_loss(y_true, y_pred)), {\n",
    "            \"auc\": float(roc_auc_score(y_true, y_pred)),\n",
    "            \"zo_loss\": float(zero_one_loss(y_true, y_class_pred)),\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown loss function: {loss_str}\")\n",
    "\n",
    "\n",
    "def timer_decorator(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        return (result, start_time, end_time, elapsed_time)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "@timer_decorator\n",
    "def train_lgbm(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    params = deepcopy(params)\n",
    "\n",
    "    early_stopping_rounds = params.pop(\"early_stopping_rounds\")\n",
    "    model = ModelClass(**params)\n",
    "\n",
    "    is_classifier = issubclass(ModelClass, ClassifierMixin)\n",
    "\n",
    "    early_stop = lightgbm.early_stopping(\n",
    "        stopping_rounds=early_stopping_rounds,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    cat_idxs, x_train, x_val, x_test = convert_to_ordinals(\n",
    "        feature_type, x_train, x_val, x_test\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train.flatten(),\n",
    "        eval_set=[(x_val, y_val.flatten())],\n",
    "        # verbose=-1,\n",
    "        callbacks=[early_stop],\n",
    "        categorical_feature=cat_idxs,\n",
    "    )\n",
    "\n",
    "    val_pred = (\n",
    "        model.predict_proba(x_val)[:, 1] if is_classifier else model.predict(x_val)\n",
    "    )\n",
    "\n",
    "    val_error = loss(y_val, val_pred, params[\"objective\"])\n",
    "\n",
    "    if x_test is not None and y_test is not None:\n",
    "        y_pred = (\n",
    "            model.predict_proba(x_test)[:, 1]\n",
    "            if is_classifier\n",
    "            else model.predict(x_test)\n",
    "        )\n",
    "\n",
    "        test_error = loss(y_test, y_pred, params[\"objective\"])\n",
    "\n",
    "        return (\n",
    "            val_error[0],\n",
    "            test_error[0],\n",
    "            model,\n",
    "            {\"val\": val_error[1], \"test\": test_error[1]},\n",
    "        )\n",
    "    else:\n",
    "        return val_error[0], model\n",
    "\n",
    "\n",
    "@timer_decorator\n",
    "def train_xgboost(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    params = deepcopy(params)\n",
    "    early_stopping_rounds = params.pop(\"early_stopping_rounds\")\n",
    "\n",
    "    model = ModelClass(**params)\n",
    "\n",
    "    is_classifier = issubclass(ModelClass, ClassifierMixin)\n",
    "\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        eval_set=[(x_val, y_val.flatten())],\n",
    "        early_stopping_rounds=early_stopping_rounds,\n",
    "        verbose=False,\n",
    "        eval_metric=\"logloss\" if is_classifier else \"rmse\",\n",
    "    )\n",
    "\n",
    "    val_pred = (\n",
    "        model.predict_proba(x_val)[:, 1] if is_classifier else model.predict(x_val)\n",
    "    )\n",
    "\n",
    "    val_error = loss(y_val, val_pred, params[\"objective\"])\n",
    "\n",
    "    if x_test is not None and y_test is not None:\n",
    "        y_pred = (\n",
    "            model.predict_proba(x_test)[:, 1]\n",
    "            if is_classifier\n",
    "            else model.predict(x_test)\n",
    "        )\n",
    "\n",
    "        test_error = loss(y_test, y_pred, params[\"objective\"])\n",
    "\n",
    "        return (\n",
    "            val_error[0],\n",
    "            test_error[0],\n",
    "            model,\n",
    "            {\"val\": val_error[1], \"test\": test_error[1]},\n",
    "        )\n",
    "    else:\n",
    "        return val_error[0], model\n",
    "\n",
    "\n",
    "@timer_decorator\n",
    "def train_catboost(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    feature_type = [\n",
    "        i for i, t in enumerate(feature_type) if t in [\"categorical\", \"binary\"]\n",
    "    ]\n",
    "\n",
    "    train_pool = Pool(x_train, y_train, cat_features=feature_type)\n",
    "    val_pool = Pool(x_val, y_val, cat_features=feature_type)\n",
    "\n",
    "    # https://catboost.ai/en/docs/concepts/python-reference_catboostregressor\n",
    "    model = ModelClass(**params)\n",
    "\n",
    "    # They don't subclass ClassifierMixin, so we have to check manually\n",
    "    is_classifier = ModelClass == CatBoostClassifier\n",
    "\n",
    "    model.fit(\n",
    "        train_pool,\n",
    "        eval_set=val_pool,\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    val_pred = (\n",
    "        model.predict_proba(val_pool)[:, 1]\n",
    "        if is_classifier\n",
    "        else model.predict(val_pool)\n",
    "    )\n",
    "\n",
    "    loss_fn = \"log_loss\" if is_classifier else \"rmse\"\n",
    "\n",
    "    val_error = loss(y_val, val_pred, loss_fn)\n",
    "\n",
    "    if x_test is not None and y_test is not None:\n",
    "        test_pool = Pool(x_test, cat_features=feature_type)\n",
    "\n",
    "        y_pred = (\n",
    "            model.predict_proba(test_pool)[:, 1]\n",
    "            if is_classifier\n",
    "            else model.predict(test_pool)\n",
    "        )\n",
    "\n",
    "        test_error = loss(y_test, y_pred, loss_fn)\n",
    "\n",
    "        return (\n",
    "            val_error[0],\n",
    "            test_error[0],\n",
    "            model,\n",
    "            {\"val\": val_error[1], \"test\": test_error[1]},\n",
    "        )\n",
    "    else:\n",
    "        return val_error[0], model\n",
    "\n",
    "\n",
    "@timer_decorator\n",
    "def train_sklearn(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    rescale=False,\n",
    "    feature_type=[],\n",
    "):\n",
    "    if rescale:\n",
    "        model = Pipeline(\n",
    "            [(\"scaler\", StandardScaler()), (\"model\", ModelClass(**params))]\n",
    "        )\n",
    "    else:\n",
    "        model = ModelClass(**params)\n",
    "\n",
    "    model.fit(x_train, y_train.flatten())\n",
    "\n",
    "    is_classification = issubclass(ModelClass, ClassifierMixin)\n",
    "\n",
    "    if is_classification:\n",
    "        val_error = loss(y_val, model.predict_proba(x_val)[:, 1], True)\n",
    "    else:\n",
    "        val_error = loss(y_val, model.predict(x_val), False)\n",
    "\n",
    "    if x_test is not None and y_test is not None:\n",
    "        if is_classification:\n",
    "            y_pred = model.predict_proba(x_test)[:, 1]\n",
    "        else:\n",
    "            y_pred = model.predict(x_test)\n",
    "\n",
    "        test_error = loss(y_test, y_pred, is_classification)\n",
    "\n",
    "        return (\n",
    "            val_error[0],\n",
    "            test_error[0],\n",
    "            model,\n",
    "            {\"val\": val_error[1], \"test\": test_error[1]},\n",
    "        )\n",
    "    else:\n",
    "        return val_error[0], model\n",
    "\n",
    "\n",
    "def train_gbr(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    return train_sklearn(\n",
    "        ModelClass,\n",
    "        params,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_val,\n",
    "        y_val,\n",
    "        x_test,\n",
    "        y_test,\n",
    "        feature_type=[],\n",
    "    )\n",
    "\n",
    "\n",
    "def train_rfr(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    return train_sklearn(\n",
    "        ModelClass,\n",
    "        params,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_val,\n",
    "        y_val,\n",
    "        x_test,\n",
    "        y_test,\n",
    "        feature_type=[],\n",
    "    )\n",
    "\n",
    "\n",
    "def train_etr(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    return train_sklearn(\n",
    "        ModelClass,\n",
    "        params,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_val,\n",
    "        y_val,\n",
    "        x_test,\n",
    "        y_test,\n",
    "        feature_type=[],\n",
    "    )\n",
    "\n",
    "\n",
    "# TODO: compare JOPLEn to AdaBoost\n",
    "def train_abr(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    return train_sklearn(\n",
    "        ModelClass,\n",
    "        params,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_val,\n",
    "        y_val,\n",
    "        x_test,\n",
    "        y_test,\n",
    "        feature_type=[],\n",
    "    )\n",
    "\n",
    "\n",
    "def train_lf(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    return train_sklearn(\n",
    "        ModelClass,\n",
    "        {**params, \"base_estimator\": LinearRegression()},\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_val,\n",
    "        y_val,\n",
    "        x_test,\n",
    "        y_test,\n",
    "        feature_type=[],\n",
    "    )\n",
    "\n",
    "\n",
    "def train_ridge(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    return train_sklearn(\n",
    "        ModelClass,\n",
    "        params,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_val,\n",
    "        y_val,\n",
    "        x_test,\n",
    "        y_test,\n",
    "        rescale=True,\n",
    "        feature_type=[],\n",
    "    )\n",
    "\n",
    "\n",
    "@timer_decorator\n",
    "def train_pen(\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    params = deepcopy(params)\n",
    "    is_classification = eval(params.get(\"loss_fn\", \"SquaredError\")) == LogisticLoss\n",
    "\n",
    "    initial_params = {\n",
    "        \"partitioner\": eval(params.pop(\"partitioner\")),\n",
    "        \"n_cells\": params.pop(\"n_cells\"),\n",
    "        \"n_partitions\": params.pop(\"n_partitions\"),\n",
    "        \"random_state\": params.pop(\"random_state\"),\n",
    "    }\n",
    "\n",
    "    if \"cell_model\" in params:\n",
    "        initial_params[\"cell_model\"] = eval(params.pop(\"cell_model\"))\n",
    "\n",
    "    model = JOPLEn(\n",
    "        loss_fn=eval(params.pop(\"loss_fn\", \"SquaredError\")),\n",
    "        **initial_params,\n",
    "    )\n",
    "\n",
    "    if \"norm_type\" in params:\n",
    "        params[\"norm_type\"] = eval(params[\"norm_type\"])\n",
    "\n",
    "    history = model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        val_x=x_val,\n",
    "        val_y=y_val,\n",
    "        **params,\n",
    "    )\n",
    "\n",
    "    val_error = loss(y_val, model.predict(x_val), is_classification)\n",
    "\n",
    "    if x_test is not None and y_test is not None:\n",
    "        y_pred = model.predict(x_test)\n",
    "        test_error = loss(y_test, y_pred, is_classification)\n",
    "        return (\n",
    "            val_error[0],\n",
    "            test_error[0],\n",
    "            model,\n",
    "            {\n",
    "                \"n_epochs\": (\n",
    "                    len(history[\"objective\"]) if \"objective\" in history else None\n",
    "                ),\n",
    "                \"val\": val_error[1],\n",
    "                \"test\": test_error[1],\n",
    "            },\n",
    "        )\n",
    "    else:\n",
    "        return val_error[0], model\n",
    "\n",
    "\n",
    "def train_joplen(\n",
    "    _,  # ModelClass is not used\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    fn = train_cb_joplen if params[\"partitioner\"] == \"CBPartition\" else train_pen\n",
    "\n",
    "    return fn(\n",
    "        params,\n",
    "        x_train,\n",
    "        y_train,\n",
    "        x_val,\n",
    "        y_val,\n",
    "        x_test,\n",
    "        y_test,\n",
    "        feature_type=[],\n",
    "    )\n",
    "\n",
    "\n",
    "@timer_decorator\n",
    "def train_cb_joplen(\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    params = deepcopy(params)\n",
    "    is_classification = eval(params.get(\"loss_fn\", \"SquaredError\")) == LogisticLoss\n",
    "\n",
    "    initial_params = {\n",
    "        \"partitioner\": eval(params.pop(\"partitioner\")),\n",
    "        \"n_cells\": 2 ** params.pop(\"max_depth\"),\n",
    "        # \"n_cells\": 2 ** params.pop(\"max_depth\"),\n",
    "        \"n_partitions\": params.pop(\"iterations\"),\n",
    "        \"random_state\": params.pop(\"random_state\"),\n",
    "    }\n",
    "\n",
    "    if \"cell_model\" in params:\n",
    "        initial_params[\"cell_model\"] = eval(params.pop(\"cell_model\"))\n",
    "\n",
    "    partitioner_keys = [\n",
    "        \"od_wait\",\n",
    "        \"learning_rate\",\n",
    "        \"l2_leaf_reg\",\n",
    "        \"od_type\",\n",
    "        \"subsample\",\n",
    "        \"grow_policy\",\n",
    "        \"allow_writing_files\",\n",
    "    ]\n",
    "    initial_params[\"part_kwargs\"] = {\n",
    "        k: params.pop(k) for k in partitioner_keys if k in params\n",
    "    }\n",
    "    initial_params[\"part_kwargs\"][\"cat_features\"] = feature_type\n",
    "\n",
    "    model = JOPLEn(\n",
    "        loss_fn=eval(params.pop(\"loss_fn\", \"SquaredError\")),\n",
    "        **initial_params,\n",
    "    )\n",
    "\n",
    "    if \"norm_type\" in params:\n",
    "        params[\"norm_type\"] = eval(params[\"norm_type\"])\n",
    "\n",
    "    history = model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        val_x=x_val,\n",
    "        val_y=y_val,\n",
    "        rescale=False,\n",
    "        **params,\n",
    "    )\n",
    "\n",
    "    val_error = loss(y_val, model.predict(x_val), is_classification)\n",
    "\n",
    "    if x_test is not None and y_test is not None:\n",
    "        y_pred = model.predict(x_test)\n",
    "        test_error = loss(y_test, y_pred, is_classification)\n",
    "        return (\n",
    "            val_error[0],\n",
    "            test_error[0],\n",
    "            model,\n",
    "            {\n",
    "                \"n_epochs\": (\n",
    "                    len(history[\"objective\"]) if \"objective\" in history else None\n",
    "                ),\n",
    "                \"val\": val_error[1],\n",
    "                \"test\": test_error[1],\n",
    "            },\n",
    "        )\n",
    "    else:\n",
    "        return val_error[0], model\n",
    "\n",
    "\n",
    "@timer_decorator\n",
    "def train_friedman(\n",
    "    _,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    params = deepcopy(params)\n",
    "\n",
    "    base_model = eval(params.pop(\"base_model\"))\n",
    "    refit_model = eval(params.pop(\"refit_model\"))\n",
    "\n",
    "    is_classification = issubclass(refit_model, ClassifierMixin)\n",
    "\n",
    "    all_params = {\n",
    "        \"base_params\": {k: v for k, v in params.items() if \"base\" in k},\n",
    "        \"refit_params\": {k: v for k, v in params.items() if \"refit\" in k},\n",
    "        \"shared_params\": {k: v for k, v in params.items() if \"shared\" in k},\n",
    "    }\n",
    "\n",
    "    for k, v in all_params.items():\n",
    "        all_params[k] = {\"_\".join(k.split(\"_\")[1:]): v for k, v in v.items()}\n",
    "\n",
    "    model = FriedmanRefit(\n",
    "        base_model,\n",
    "        refit_model,\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        base_params={**all_params[\"base_params\"], **all_params[\"shared_params\"]},\n",
    "        refit_params={**all_params[\"refit_params\"], **all_params[\"shared_params\"]},\n",
    "    )\n",
    "\n",
    "    val_pred = model.predict_proba(x_val) if is_classification else model.predict(x_val)\n",
    "    val_error = loss(y_val, val_pred, is_classification)\n",
    "\n",
    "    if x_test is not None and y_test is not None:\n",
    "        test_pred = (\n",
    "            model.predict_proba(x_test) if is_classification else model.predict(x_test)\n",
    "        )\n",
    "        test_error = loss(y_test, test_pred, is_classification)\n",
    "        return (\n",
    "            val_error[0],\n",
    "            test_error[0],\n",
    "            model,\n",
    "            {\"val\": val_error[1], \"test\": test_error[1]},\n",
    "        )\n",
    "    else:\n",
    "        return val_error[0], model\n",
    "\n",
    "\n",
    "@timer_decorator\n",
    "def train_fastel(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    assert (\n",
    "        params.get(\"loss_criteria\", \"mse\") != \"log_loss\"\n",
    "    ), \"FASTEL does not support the logistic loss\"\n",
    "\n",
    "    xs = StandardScaler().fit(x_train)\n",
    "    x_train = xs.transform(x_train)\n",
    "    x_val = xs.transform(x_val)\n",
    "    x_test = xs.transform(x_test) if x_test is not None else None\n",
    "\n",
    "    ys = StandardScaler().fit(y_train.reshape(-1, 1))\n",
    "    y_train = ys.transform(y_train.reshape(-1, 1))\n",
    "    y_val = ys.transform(y_val.reshape(-1, 1))\n",
    "    y_test = ys.transform(y_test.reshape(-1, 1)) if y_test is not None else None\n",
    "\n",
    "    model = MultiTaskTrees(\n",
    "        input_shape=x_train.shape[1:],\n",
    "        **params,\n",
    "    )\n",
    "\n",
    "    model.train(\n",
    "        x_train,\n",
    "        y_train.reshape(-1, 1),\n",
    "        np.ones((y_train.shape[0], 1)),\n",
    "        x_val,\n",
    "        y_val.reshape(-1, 1),\n",
    "        np.ones((y_val.shape[0], 1)),\n",
    "    )\n",
    "\n",
    "    y_val_pred = ys.inverse_transform(model.predict(x_val)[:, None])\n",
    "    val_error = loss(y_val, y_val_pred.flatten(), params.get(\"loss_criteria\", \"mse\"))\n",
    "\n",
    "    if x_test is not None and y_test is not None:\n",
    "        y_test_pred = ys.inverse_transform(model.predict(x_test)[:, None])\n",
    "        test_error = loss(\n",
    "            y_test, y_test_pred.flatten(), params.get(\"loss_criteria\", \"mse\")\n",
    "        )\n",
    "        return (\n",
    "            val_error[0],\n",
    "            test_error[0],\n",
    "            model,\n",
    "            {\n",
    "                \"val\": val_error[1],\n",
    "                \"test\": test_error[1],\n",
    "            },\n",
    "        )\n",
    "    else:\n",
    "        return val_error[0], model\n",
    "\n",
    "\n",
    "@timer_decorator\n",
    "def train_nn(\n",
    "    ModelClass,\n",
    "    params,\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_val,\n",
    "    y_val,\n",
    "    x_test=None,\n",
    "    y_test=None,\n",
    "    feature_type=[],\n",
    "):\n",
    "    # TODO: should rescale the y values as well\n",
    "    xs = StandardScaler().fit(x_train)\n",
    "\n",
    "    loss_criteria = params.pop(\"loss_criteria\", \"mse\")\n",
    "\n",
    "    assert loss_criteria == \"mse\"\n",
    "\n",
    "    tmp_params = deepcopy(params)\n",
    "\n",
    "    model = NN(\n",
    "        hidden_layer_size=tmp_params.pop(\"hidden_layer_size\"),\n",
    "        n_hidden_layers=tmp_params.pop(\"n_hidden_layers\"),\n",
    "        activation=tmp_params.pop(\"activation\"),\n",
    "        sel_feat=False,\n",
    "    )\n",
    "    model.fit(\n",
    "        xs.transform(x_train),\n",
    "        y_train,\n",
    "        xs.transform(x_val),\n",
    "        y_val,\n",
    "        **tmp_params,\n",
    "    )\n",
    "\n",
    "    y_val_pred = model.predict(xs.transform(x_val))\n",
    "    val_error = loss(y_val, y_val_pred.flatten(), loss_criteria)\n",
    "\n",
    "    if x_test is not None and y_test is not None:\n",
    "        y_test_pred = model.predict(xs.transform(x_test))\n",
    "        test_error = loss(y_test, y_test_pred.flatten(), loss_criteria)\n",
    "        return (\n",
    "            val_error[0],\n",
    "            test_error[0],\n",
    "            model,\n",
    "            {\"val\": val_error[1], \"test\": test_error[1]},\n",
    "        )\n",
    "    else:\n",
    "        return val_error[0], model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_prediction(\n",
    "    x_train,\n",
    "    x_val,\n",
    "    x_test,\n",
    "    y_train,\n",
    "    y_val,\n",
    "    y_test,\n",
    "    is_classification,\n",
    "):\n",
    "    if is_classification:\n",
    "        dummy = DummyClassifier(strategy=\"most_frequent\")\n",
    "    else:\n",
    "        dummy = DummyRegressor(strategy=\"mean\")\n",
    "\n",
    "    dummy.fit(x_train, y_train)\n",
    "    y_pred = dummy.predict(x_test)\n",
    "\n",
    "    res = loss(y_test, y_pred, is_classification)\n",
    "\n",
    "    return {\n",
    "        \"model_name\": dummy.__class__.__name__,\n",
    "        \"loss\": res[0],\n",
    "        \"metadata\": res[1],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fn = {\n",
    "    FriedmanRefit.__name__: train_friedman,\n",
    "    LGBMRegressor.__name__: train_lgbm,\n",
    "    LGBMClassifier.__name__: train_lgbm,\n",
    "    CatBoostRegressor.__name__: train_catboost,\n",
    "    CatBoostClassifier.__name__: train_catboost,\n",
    "    XGBRegressor.__name__: train_xgboost,\n",
    "    XGBClassifier.__name__: train_xgboost,\n",
    "    GradientBoostingRegressor.__name__: train_gbr,\n",
    "    GradientBoostingClassifier.__name__: train_gbr,\n",
    "    RandomForestRegressor.__name__: train_rfr,\n",
    "    RandomForestClassifier.__name__: train_rfr,\n",
    "    ExtraTreesRegressor.__name__: train_etr,\n",
    "    ExtraTreesClassifier.__name__: train_etr,\n",
    "    JOPLEn.__name__: train_joplen,\n",
    "    LinearForestRegressor.__name__: train_lf,\n",
    "    LinearForestClassifier.__name__: train_lf,\n",
    "    Ridge.__name__: train_ridge,\n",
    "    MultiTaskTrees.__name__: train_fastel,\n",
    "    NN.__name__: train_nn,\n",
    "}\n",
    "\n",
    "\n",
    "def optimize_model(model_info, ds_path, n_trials, skip_categorical, ds_metadata):\n",
    "    ds_name = ds_path.name\n",
    "    params = model_info[\"parameters\"]\n",
    "\n",
    "    is_classification = ds_metadata[\"pmlb_metadata\"][\"target\"][\"type\"] == \"categorical\"\n",
    "\n",
    "    loss_type = \"log_loss\" if is_classification else \"rmse\"\n",
    "\n",
    "    dir_path = (\n",
    "        CACHE_DIR\n",
    "        / (\"class\" if is_classification else \"regr\")\n",
    "        / model_info[\"dir_name\"]\n",
    "        / ds_name\n",
    "    )\n",
    "    exp_path = dir_path / \"experiment.json\"\n",
    "    metadata_path = dir_path / \"metadata.yaml\"\n",
    "\n",
    "    if metadata_path.exists():\n",
    "        with open(metadata_path, \"r\") as f:\n",
    "            metadata = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "        return metadata\n",
    "\n",
    "    cont_mask = np.array([t == \"continuous\" for t in ds_metadata[\"feature_type\"]])\n",
    "    bl_categorical = np.any(~cont_mask)\n",
    "\n",
    "    if np.sum(cont_mask) == 0:\n",
    "        return None\n",
    "\n",
    "    x_train = np.loadtxt(ds_path / \"x_train.csv\", delimiter=\",\")[:, cont_mask]\n",
    "    x_val = np.loadtxt(ds_path / \"x_val.csv\", delimiter=\",\")[:, cont_mask]\n",
    "    x_test = np.loadtxt(ds_path / \"x_test.csv\", delimiter=\",\")[:, cont_mask]\n",
    "    y_train = np.loadtxt(ds_path / \"y_train.csv\", delimiter=\",\")\n",
    "    y_val = np.loadtxt(ds_path / \"y_val.csv\", delimiter=\",\")\n",
    "    y_test = np.loadtxt(ds_path / \"y_test.csv\", delimiter=\",\")\n",
    "\n",
    "    if is_classification:\n",
    "        enc = LabelEncoder()\n",
    "        y_train = enc.fit_transform(y_train)\n",
    "        y_val = enc.transform(y_val)\n",
    "        y_test = enc.transform(y_test)\n",
    "\n",
    "    dummy_info = dummy_prediction(\n",
    "        x_train,\n",
    "        x_val,\n",
    "        x_test,\n",
    "        y_train,\n",
    "        y_val,\n",
    "        y_test,\n",
    "        is_classification=is_classification,\n",
    "    )\n",
    "\n",
    "    if not exp_path.exists():\n",
    "        ax_client = AxClient(\n",
    "            random_seed=0,\n",
    "            verbose_logging=False,\n",
    "        )\n",
    "\n",
    "        ax_client.create_experiment(\n",
    "            name=f\"{model_info['model']}_{ds_name}\",\n",
    "            parameters=params,\n",
    "            objectives={loss_type: ObjectiveProperties(minimize=True)},\n",
    "            overwrite_existing_experiment=True,\n",
    "        )\n",
    "\n",
    "        for _ in trange(n_trials, leave=False, position=1):\n",
    "            round_params, trial_index = ax_client.get_next_trial()\n",
    "\n",
    "            try:\n",
    "                val_error, _ = train_fn[model_info[\"model\"]](\n",
    "                    eval(model_info[\"model\"]),\n",
    "                    round_params,\n",
    "                    x_train=x_train,\n",
    "                    y_train=y_train,\n",
    "                    x_val=x_val,\n",
    "                    y_val=y_val,\n",
    "                    feature_type=[],\n",
    "                )[0]\n",
    "                ax_client.complete_trial(\n",
    "                    trial_index=trial_index, raw_data=float(val_error)\n",
    "                )\n",
    "            except ValueError as e:\n",
    "                print(e)\n",
    "                ax_client.abandon_trial(\n",
    "                    trial_index=trial_index,\n",
    "                    reason=str(e),\n",
    "                )\n",
    "\n",
    "        exp_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        ax_client.save_to_json_file(\n",
    "            filepath=exp_path,\n",
    "        )\n",
    "    else:\n",
    "        ax_client = AxClient.load_from_json_file(filepath=exp_path)\n",
    "\n",
    "    best_parameters, values = ax_client.get_best_parameters()\n",
    "\n",
    "    (val_error, test_error, model, metadata), _, _, train_time = train_fn[\n",
    "        model_info[\"model\"]\n",
    "    ](\n",
    "        eval(model_info[\"model\"]),\n",
    "        best_parameters,\n",
    "        x_train=x_train,\n",
    "        y_train=y_train,\n",
    "        x_val=x_val,\n",
    "        y_val=y_val,\n",
    "        x_test=x_test,\n",
    "        y_test=y_test,\n",
    "        feature_type=[],\n",
    "    )\n",
    "\n",
    "    metadata = {\n",
    "        \"model_name\": model_info[\"model\"],\n",
    "        \"val_score\": float(val_error),\n",
    "        \"test_score\": float(test_error),\n",
    "        \"train_time\": float(train_time),\n",
    "        \"params\": best_parameters,\n",
    "        \"dummy_loss\": float(dummy_info[\"loss\"]),\n",
    "        \"contains_categorical\": \"postprocessed\" if bl_categorical else False,\n",
    "        \"metadata\": metadata,\n",
    "        \"dummy_metadata\": dummy_info[\"metadata\"],\n",
    "    }\n",
    "\n",
    "    with open(metadata_path, \"w\") as f:\n",
    "        yaml.dump(metadata, f)\n",
    "\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running reg datasets\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "592f908213424e458c5336e1f9d5706b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/122 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c30d4aebae944aabd6bda55156fbe71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0da29456dc9746a586ca9dd13247a4fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db3c258033ee42fe9b86323e2ff612b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "909fc11a3d4d47aebd83fbdb67d32b96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c664d912d7a149dbb96c77949d8d8bca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fd55d364a46466c8d6ab3e43b9d99d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2577473f634744489d6a13e31d973511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46e9b755cbbd4a6abebcf79956192e76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m model_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mds_path\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     55\u001b[0m itr\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_str\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m <50\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mds_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mds_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     reg_res[info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]][ds_path\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m res\n",
      "Cell \u001b[0;32mIn[48], line 93\u001b[0m, in \u001b[0;36moptimize_model\u001b[0;34m(model_info, ds_path, n_trials, skip_categorical, ds_metadata)\u001b[0m\n\u001b[1;32m     90\u001b[0m round_params, trial_index \u001b[38;5;241m=\u001b[39m ax_client\u001b[38;5;241m.\u001b[39mget_next_trial()\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 93\u001b[0m     val_error, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_fn\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mround_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    102\u001b[0m     ax_client\u001b[38;5;241m.\u001b[39mcomplete_trial(\n\u001b[1;32m    103\u001b[0m         trial_index\u001b[38;5;241m=\u001b[39mtrial_index, raw_data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m(val_error)\n\u001b[1;32m    104\u001b[0m     )\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[46], line 507\u001b[0m, in \u001b[0;36mtrain_joplen\u001b[0;34m(_, params, x_train, y_train, x_val, y_val, x_test, y_test, feature_type)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_joplen\u001b[39m(\n\u001b[1;32m    495\u001b[0m     _,  \u001b[38;5;66;03m# ModelClass is not used\u001b[39;00m\n\u001b[1;32m    496\u001b[0m     params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    503\u001b[0m     feature_type\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m    504\u001b[0m ):\n\u001b[1;32m    505\u001b[0m     fn \u001b[38;5;241m=\u001b[39m train_cb_joplen \u001b[38;5;28;01mif\u001b[39;00m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartitioner\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCBPartition\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m train_pen\n\u001b[0;32m--> 507\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[46], line 55\u001b[0m, in \u001b[0;36mtimer_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     54\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 55\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     57\u001b[0m     elapsed_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "Cell \u001b[0;32mIn[46], line 465\u001b[0m, in \u001b[0;36mtrain_pen\u001b[0;34m(params, x_train, y_train, x_val, y_val, x_test, y_test, feature_type)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnorm_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m    463\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnorm_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnorm_type\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m--> 465\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_x\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_y\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m val_error \u001b[38;5;241m=\u001b[39m loss(y_val, model\u001b[38;5;241m.\u001b[39mpredict(x_val), is_classification)\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x_test \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m y_test \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/code/school/joplen/code/joplen-tests/joplen/src/JOPLEn/singletask.py:597\u001b[0m, in \u001b[0;36mJOPLEn.fit\u001b[0;34m(self, x, y, lam, mu, max_iters, print_epochs, verbose, val_x, val_y, feat_thresh, alpha, gamma, delta, sigma, norm_type, patience, stop_thresh, early_stop, rescale, mu_decr, use_nesterov)\u001b[0m\n\u001b[1;32m    594\u001b[0m     m_next \u001b[38;5;241m=\u001b[39m w_next \u001b[38;5;241m+\u001b[39m beta \u001b[38;5;241m*\u001b[39m (w_next \u001b[38;5;241m-\u001b[39m w_prev)\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# loss gradient\u001b[39;00m\n\u001b[0;32m--> 597\u001b[0m     w_next \u001b[38;5;241m=\u001b[39m m_next \u001b[38;5;241m-\u001b[39m mu \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm_next\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    599\u001b[0m     w_next \u001b[38;5;241m=\u001b[39m w_prev \u001b[38;5;241m-\u001b[39m mu \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn\u001b[38;5;241m.\u001b[39mgrad(w_prev, x, y, s)\n",
      "File \u001b[0;32m~/code/school/joplen/code/joplen-tests/joplen/src/JOPLEn/singletask.py:140\u001b[0m, in \u001b[0;36mSquaredError.grad\u001b[0;34m(self, w, x, y, s)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrad\u001b[39m(\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28mself\u001b[39m: SquaredError,\n\u001b[1;32m    135\u001b[0m     w: cupy\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     s: cupy\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[1;32m    139\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m cupy\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m--> 140\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m ((y_pred \u001b[38;5;241m-\u001b[39m y) \u001b[38;5;241m*\u001b[39m s) \u001b[38;5;241m/\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/code/school/joplen/code/joplen-tests/joplen/src/JOPLEn/singletask.py:106\u001b[0m, in \u001b[0;36mLoss._raw_output\u001b[0;34m(self, w, x, s)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raw_output\u001b[39m(\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28mself\u001b[39m: Loss,\n\u001b[1;32m    102\u001b[0m     w: cupy\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[1;32m    103\u001b[0m     x: cupy\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[1;32m    104\u001b[0m     s: cupy\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[1;32m    105\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m cupy\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcupy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_partitions\u001b[49m\n",
      "File \u001b[0;32mcupy/_core/core.pyx:1309\u001b[0m, in \u001b[0;36mcupy._core.core._ndarray_base.__truediv__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy/_core/core.pyx:1697\u001b[0m, in \u001b[0;36mcupy._core.core._ndarray_base.__array_ufunc__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy/_core/_kernel.pyx:1331\u001b[0m, in \u001b[0;36mcupy._core._kernel.ufunc.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy/_core/_kernel.pyx:1555\u001b[0m, in \u001b[0;36mcupy._core._kernel._Ops.guess_routine\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy/_core/_kernel.pyx:1116\u001b[0m, in \u001b[0;36mcupy._core._kernel._min_scalar_type\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m<__array_function__ internals>:177\u001b[0m, in \u001b[0;36mmin_scalar_type\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ignored_models = [\n",
    "    \"adaboost\",\n",
    "    \"joplen_const_linforest_part\",\n",
    "    \"lf\",\n",
    "    \"et\",\n",
    "    # \"joplen_const_rf_part\",\n",
    "    \"lgbm\",\n",
    "    \"fastel\",\n",
    "    \"joplen_const\",\n",
    "    \"nn\",\n",
    "    # \"gb\",  # normal GB\n",
    "    \"joplen_linear_gb_part\",\n",
    "    # \"rf\",\n",
    "    # \"joplen_const_gb_part_l1\",\n",
    "    \"joplen_linear_inf\",\n",
    "    \"ridge\",\n",
    "    # \"joplen_const_gb_part_l2\",  # fast joplen loss\n",
    "    \"joplen_linear_linforest_part\",\n",
    "    # \"xgboost\",  # gradient boosting with penalty term\n",
    "    \"joplen_const_gb_part_sl2\",\n",
    "    \"joplen_linear_rf_part\",\n",
    "    \"joplen_const_gb_part\",\n",
    "    \"joplen_linear\",\n",
    "    # \"jp_cb\",\n",
    "]\n",
    "\n",
    "reg_datasets = [d for d in (DS_PATH / \"reg\").iterdir() if d.is_dir()]\n",
    "class_datasets = [d for d in (DS_PATH / \"class\").iterdir() if d.is_dir()]\n",
    "\n",
    "metadata = {}\n",
    "\n",
    "for name in [\"reg\", \"class\"]:\n",
    "    metadata[name] = yaml.safe_load(open(DS_PATH.parent / f\"{name}_metadata.yaml\", \"r\"))\n",
    "\n",
    "\n",
    "reg_res = defaultdict(dict)\n",
    "\n",
    "for name, lst in zip([\"reg\", \"class\"], [reg_datasets, class_datasets]):\n",
    "    print(f\"Running {name} datasets\")\n",
    "\n",
    "    itr = tqdm(lst, position=0)\n",
    "\n",
    "    for ds_path in itr:\n",
    "        if ds_path.name in EXCLUDE:\n",
    "            continue\n",
    "\n",
    "        # print(ignored_models)\n",
    "        for file_name, info in model_info[name].items():\n",
    "            if file_name in ignored_models:\n",
    "                continue\n",
    "\n",
    "            # print(f\"Running {file_name} on {ds_path.name}\")\n",
    "\n",
    "            model_str = f\"{file_name} on {ds_path.name}\"\n",
    "            itr.set_description(f\"Running {model_str : <50}\")\n",
    "            res = optimize_model(\n",
    "                info,\n",
    "                ds_path,\n",
    "                50,\n",
    "                False,\n",
    "                metadata[name][ds_path.name],\n",
    "            )\n",
    "\n",
    "            if res is not None:\n",
    "                reg_res[info[\"name\"]][ds_path.name] = res\n",
    "\n",
    "reg_res = dict(reg_res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
