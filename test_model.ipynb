{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from JOPLEn.singletask import JOPLEn, LogisticLoss\n",
    "from JOPLEn.partitioner import GBPartition\n",
    "from JOPLEn.enums import CellModel\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic dataset\n",
    "x, y = make_classification(n_samples=1000, n_features=20, random_state=0)\n",
    "\n",
    "# Split the dataset into a training and a test set\n",
    "x_train, x_test = x[:800], x[800:]\n",
    "y_train, y_test = y[:800], y[800:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 0.24\n",
      "AUC: 0.97\n",
      "Log loss: 1.62\n",
      "AUC: 0.96\n"
     ]
    }
   ],
   "source": [
    "# Train a gradient boosting model\n",
    "clf = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=1.0,\n",
    "    max_depth=1,\n",
    "    random_state=0,\n",
    "    loss=\"log_loss\",\n",
    ")\n",
    "\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = clf.predict(x_test)\n",
    "y_pred_proba = clf.predict_proba(x_test)[:, 1]\n",
    "\n",
    "# using prebabilities\n",
    "print(f\"Log loss: {log_loss(y_test, y_pred_proba):.2f}\")\n",
    "print(f\"AUC: {roc_auc_score(y_test, y_pred_proba):.2f}\")\n",
    "\n",
    "# using predictions\n",
    "print(f\"Log loss: {log_loss(y_test, y_pred):.2f}\")\n",
    "print(f\"AUC: {roc_auc_score(y_test, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[04:04:07]: Epoch    100 | TrL: 0.506250 | FNorm: 0.136372 | PNorm: 1.167785 | WNz:    1 | Obj: 0.585158\n",
      "[04:04:07]: Epoch    200 | TrL: 0.506250 | FNorm: 1.514515 | PNorm: 3.891678 | WNz:    1 | Obj: 0.389995\n",
      "[04:04:07]: Epoch    300 | TrL: 0.506250 | FNorm: 4.896377 | PNorm: 6.997412 | WNz:    1 | Obj: 0.250075\n",
      "[04:04:07]: Epoch    400 | TrL: 0.506250 | FNorm: 9.348858 | PNorm: 9.668949 | WNz:    1 | Obj: 0.180537\n",
      "Log loss: 0.20\n",
      "AUC: 0.98\n",
      "Log loss: 2.52\n",
      "AUC: 0.93\n"
     ]
    }
   ],
   "source": [
    "jp = JOPLEn(\n",
    "    loss_fn=LogisticLoss,\n",
    "    partitioner=GBPartition,\n",
    "    random_state=0,\n",
    "    n_cells=10,\n",
    "    n_partitions=10,\n",
    "    cell_model=CellModel.constant,\n",
    ")\n",
    "\n",
    "jp.fit(x_train, y_train, verbose=True, max_iters=400, alpha=0, lam=0.01, mu=0.01)\n",
    "\n",
    "y_pred_proba = jp.predict(x_test)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# using probabilities\n",
    "print(f\"Log loss: {log_loss(y_test, y_pred_proba):.2f}\")\n",
    "print(f\"AUC: {roc_auc_score(y_test, y_pred_proba):.2f}\")\n",
    "\n",
    "# using predictions\n",
    "print(f\"Log loss: {log_loss(y_test, y_pred):.2f}\")\n",
    "print(f\"AUC: {roc_auc_score(y_test, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.8988485336303711, 1),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (1, 0.7403137683868408, 1),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (1, 0.8016042113304138, 1),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.15806594491004944, 0),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (0, 0.2555866539478302, 0),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (0, 0.15806594491004944, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (0, 0.15806594491004944, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (0, 0.20146484673023224, 0),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (0, 0.15806594491004944, 0),\n",
       " (1, 0.7877060770988464, 1),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (1, 0.7877060770988464, 1),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.15806594491004944, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (0, 0.7972596287727356, 1),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (1, 0.8016042113304138, 1),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (0, 0.3742901086807251, 0),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (1, 0.1066424548625946, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.50819993019104, 1),\n",
       " (0, 0.5803762078285217, 1),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.8988485336303711, 1),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (1, 0.4913457930088043, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (0, 0.7352212071418762, 1),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (1, 0.7922020554542542, 1),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.20146484673023224, 0),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (0, 0.21829955279827118, 0),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.5575863122940063, 1),\n",
       " (0, 0.50819993019104, 1),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (1, 0.8016042113304138, 1),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (0, 0.49721407890319824, 0),\n",
       " (0, 0.563805878162384, 1),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (0, 0.5556631088256836, 1),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.20146484673023224, 0),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (1, 0.4913457930088043, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (0, 0.1066424548625946, 0),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (0, 0.8988485336303711, 1),\n",
       " (1, 0.8988485336303711, 1),\n",
       " (0, 0.5441932082176208, 1),\n",
       " (1, 0.8988485336303711, 1)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(int(x), float(y), int(z)) for x, y, z in zip(y_test, y_pred_proba, y_pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.base import ClassifierMixin\n",
    "\n",
    "issubclass(LGBMClassifier, ClassifierMixin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "data = np.array([0.0, 2.0, 0.0, 2.0])\n",
    "encoder.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.87499022e-03, -5.06571835e-04, -3.40375662e-04,\n",
       "         4.95325612e-04,  0.00000000e+00, -3.57401928e-04,\n",
       "         3.41540129e-04, -3.98488790e-03,  0.00000000e+00,\n",
       "         0.00000000e+00,  1.52062896e-03, -4.26606912e-04,\n",
       "        -4.61134471e-04,  8.73241529e-04,  2.78838194e-04,\n",
       "        -5.98732379e-04, -1.50767912e-04, -3.51284838e-03,\n",
       "         0.00000000e+00,  0.00000000e+00, -6.61710263e-04,\n",
       "         1.75573231e-03, -3.40375662e-04,  7.89990464e-04,\n",
       "         0.00000000e+00, -3.57401928e-04, -1.50767912e-04,\n",
       "        -3.51284838e-03,  0.00000000e+00,  0.00000000e+00,\n",
       "         1.87499022e-03, -5.06571835e-04, -9.83118411e-05,\n",
       "        -1.92789495e-04, -5.94221574e-04,  6.82870931e-04,\n",
       "         3.41540129e-04, -3.98488790e-03,  0.00000000e+00,\n",
       "         0.00000000e+00, -8.14196052e-04,  2.11153064e-03,\n",
       "        -9.83118411e-05, -1.92789495e-04,  0.00000000e+00,\n",
       "         1.59733150e-04,  3.41540129e-04, -3.98488790e-03,\n",
       "         0.00000000e+00,  0.00000000e+00, -1.80502168e-04,\n",
       "         1.67555074e-03, -4.61134471e-04,  8.73241529e-04,\n",
       "         2.78838194e-04, -5.98732379e-04, -4.21295039e-04,\n",
       "        -3.64334777e-03,  0.00000000e+00,  0.00000000e+00,\n",
       "         1.63665756e-03, -3.12898032e-04, -3.40375662e-04,\n",
       "         7.89990464e-04,  0.00000000e+00, -3.57401928e-04,\n",
       "        -2.29737472e-04, -3.66361629e-03,  0.00000000e+00,\n",
       "         0.00000000e+00, -9.10135293e-05,  1.41477305e-03,\n",
       "        -5.23557612e-04,  7.84896758e-04, -6.69270329e-04,\n",
       "         3.49376144e-04, -2.29737472e-04, -3.51284838e-03,\n",
       "         0.00000000e+00,  0.00000000e+00,  1.63665756e-03,\n",
       "        -3.12898032e-04, -5.45336958e-04,  8.44183848e-04,\n",
       "         0.00000000e+00, -3.57401928e-04, -1.54715077e-05,\n",
       "        -3.72711435e-03,  0.00000000e+00,  0.00000000e+00,\n",
       "         1.36726259e-03, -2.73240540e-04, -5.23557612e-04,\n",
       "         7.84896758e-04, -6.69270329e-04,  3.49376144e-04,\n",
       "         3.41540129e-04, -3.85438851e-03,  0.00000000e+00,\n",
       "         0.00000000e+00]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from JOPLEn.singletask import LogisticLoss\n",
    "import cupy\n",
    "\n",
    "w = jp.w\n",
    "s = jp._get_cells(x_train)\n",
    "\n",
    "x = cupy.array(np.ones((y_train.shape[0], 1)))\n",
    "\n",
    "# x.shape, w.shape\n",
    "\n",
    "loss = LogisticLoss(10)\n",
    "\n",
    "loss.grad(w, x, cupy.array(2 * y_train - 1)[:, None], s)\n",
    "# loss.predict(w, x, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "       0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
       "       1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0,\n",
       "       1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1,\n",
       "       1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,\n",
       "       0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0,\n",
       "       1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1,\n",
       "       0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,\n",
       "       1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
       "       1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "       1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0,\n",
       "       1, 1, 1, 0, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "y_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
